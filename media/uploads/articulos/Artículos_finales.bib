@Article{Amalfitano2024,
  author            = {Amalfitano, Domenico and Faralli, Stefano and Hauck, Jean Carlo Rossa and Matalonga, Santiago and Distante, Damiano},
  journal           = {ACM Computing Surveys},
  title             = {Artificial Intelligence Applied to Software Testing: A Tertiary Study},
  year              = {2024},
  note              = {Cited by: 12; All Open Access, Green Open Access, Hybrid Gold Open Access},
  number            = {3},
  volume            = {56},
  abstract          = {Context: Artificial intelligence (AI) methods and models have extensively been applied to support different phases of the software development lifecycle, including software testing (ST). Several secondary studies investigated the interplay between AI and ST but restricted the scope of the research to specific domains or sub-domains within either area.Objective: This research aims to explore the overall contribution of AI to ST, while identifying the most popular applications and potential paths for future research directions.Method: We executed a tertiary study following well-established guidelines for conducting systematic literature mappings in software engineering and for answering nine research questions.Results: We identified and analyzed 20 relevant secondary studies. The analysis was performed by drawing from well-recognized AI and ST taxonomies and mapping the selected studies according to them. The resulting mapping and discussions provide extensive and detailed information on the interplay between AI and ST.Conclusion: The application of AI to support ST is a well-consolidated and growing interest research topic. The mapping resulting from our study can be used by researchers to identify opportunities for future research, and by practitioners looking for evidence-based information on which AI-supported technology to possibly adopt in their testing processes. © 2023 Copyright held by the owner/author(s).},
  author_keywords   = {Artificial intelligence; Software testing; Systematic literature review; Systematic mapping study; Taxonomy; Tertiary study},
  doi               = {10.1145/3616372},
  keywords          = {Artificial intelligence; Life cycle; Mapping; Software design; Software testing; Artificial intelligence methods; Future research directions; Research questions; Research topics; Software development life-cycle; Software testings; Subdomain; Systematic literature review; Systematic mapping studies; Tertiary study; Taxonomies},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176768820&doi=10.1145%2f3616372&partnerID=40&md5=c9dfb5d6c6d2d99c777d457e6d4d7c54},
}

@Article{Esposito2024,
  author            = {Esposito, Mark and Sarbazvatan, Saman and Tse, Terence and Silva-Atencio, Gabriel},
  journal           = {Frontiers in Artificial Intelligence},
  title             = {The use of artificial intelligence for automatic analysis and reporting of software defects},
  year              = {2024},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  volume            = {7},
  abstract          = {The COVID-19 pandemic marked a before and after in the business world, causing a growing demand for applications that streamline operations, reduce delivery times and costs, and improve the quality of products. In this context, artificial intelligence (AI) has taken a relevant role in improving these processes, since it incorporates mathematical models that allow analyzing the logical structure of the systems to detect and reduce errors or failures in real-time. This study aimed to determine the most relevant aspects to be considered for detecting software defects using AI. The methodology used was qualitative, with an exploratory, descriptive, and non-experimental approach. The technique involved a documentary review of 79 bibliometric references. The most relevant finding was the use of regression testing techniques and automated log files, in machine learning (ML) and robotic process automation (RPA) environments. These techniques help reduce the time required to identify failures, thereby enhancing efficiency and effectiveness in the lifecycle of applications. In conclusion, companies that incorporate AI algorithms will be able to include an agile model in their lifecycle, as they will reduce the rate of failures, errors, and breakdowns allowing cost savings, and ensuring quality. Copyright © 2024 Esposito, Sarbazvatan, Tse and Silva-Atencio.},
  author_keywords   = {artificial intelligence; automation testing; software defect; software development; software failure},
  doi               = {10.3389/frai.2024.1443956},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212822758&doi=10.3389%2ffrai.2024.1443956&partnerID=40&md5=b91ccf0600bec4378facfe74ab42d3a7},
}

@Article{Sonkin2025,
  author            = {Sonkin, Vladimir and Tudose, Cătălin},
  journal           = {Computers},
  title             = {Beyond Snippet Assistance: A Workflow-Centric Framework for End-to-End AI-Driven Code Generation},
  year              = {2025},
  note              = {Cited by: 0; All Open Access, Gold Open Access},
  number            = {3},
  volume            = {14},
  abstract          = {Recent AI-assisted coding tools, such as GitHub Copilot and Cursor, have enhanced developer productivity through real-time snippet suggestions. However, these tools primarily assist with isolated coding tasks and lack a structured approach to automating complex, multi-step software development workflows. This paper introduces a workflow-centric AI framework for end-to-end automation, from requirements gathering to code generation, validation, and integration, while maintaining developer oversight. Key innovations include automatic context discovery, which selects relevant codebase elements to improve LLM accuracy; a structured execution pipeline using Prompt Pipeline Language (PPL) for iterative code refinement; self-healing mechanisms that generate tests, detect errors, trigger rollbacks, and regenerate faulty code; and AI-assisted code merging, which preserves manual modifications while integrating AI-generated updates. These capabilities enable efficient automation of repetitive tasks, enforcement of coding standards, and streamlined development workflows. This approach lays the groundwork for AI-driven development that remains adaptable as LLM models advance, progressively reducing the need for human intervention while ensuring code reliability. © 2025 by the authors.},
  author_keywords   = {AI code review; artificial intelligence; developer productivity improvement; JAIG; LLM; prompt engineering; routine tasks; software development automation},
  doi               = {10.3390/computers14030094},
  keywords          = {Coding errors; Multiprocessing programs; Pipeline codes; Systems analysis; AI code review; Code review; Developer productivity improvement; JAIG; LLM; Productivity improvements; Prompt engineering; Routine task; Software development automation; Work-flows; Software design},
  priority          = {prio2},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001419180&doi=10.3390%2fcomputers14030094&partnerID=40&md5=94849e69f1b2c3aad6dc2a2d257c7681},
}

@Article{Alenezi2025,
  author            = {Alenezi, Mamdouh and Akour, Mohammed},
  journal           = {Applied Sciences (Switzerland)},
  title             = {AI-Driven Innovations in Software Engineering: A Review of Current Practices and Future Directions},
  year              = {2025},
  note              = {Cited by: 4; All Open Access, Gold Open Access},
  number            = {3},
  volume            = {15},
  abstract          = {The software engineering landscape is undergoing a significant transformation with the advent of artificial intelligence (AI). AI technologies are poised to redefine traditional software development practices, offering innovative solutions to long-standing challenges. This paper explores the integration of AI into software engineering processes, aiming to identify its impacts, benefits, and the challenges that accompany this paradigm shift. A comprehensive analysis of current AI applications in software engineering is conducted, supported by case studies and theoretical models. The study examines various phases of software development to assess where AI contributes most effectively. The integration of AI enhances productivity, improves code quality, and accelerates development cycles. Key areas of impact include automated code generation, intelligent debugging, predictive maintenance, and enhanced decision-making processes. AI is revolutionizing software engineering by introducing automation and intelligence into the development lifecycle. Embracing AI-driven tools and methodologies is essential for staying competitive in the evolving technological landscape. © 2025 by the authors.},
  author_keywords   = {artificial intelligence (AI); automation; software development lifecycle; software engineering},
  doi               = {10.3390/app15031344},
  keywords          = {Application programs; Decision making; Life cycle; Program debugging; Software design; Software quality; 'current; Artificial intelligence; Artificial intelligence technologies; Comprehensive analysis; Current practices; Innovative solutions; Paradigm shifts; Software development life-cycle; Software development practices; Software engineering process; Predictive maintenance},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217549714&doi=10.3390%2fapp15031344&partnerID=40&md5=496085eaddfeaddffef6c5383232a4e9},
}

@Article{Hadzhikoleva2024,
  author            = {Hadzhikoleva, Stanka and Rachovski, Todor and Ivanov, Ivan and Hadzhikolev, Emil and Dimitrov, Georgi},
  journal           = {Applied Sciences (Switzerland)},
  title             = {Automated Test Creation Using Large Language Models: A Practical Application},
  year              = {2024},
  note              = {Cited by: 4; All Open Access, Gold Open Access},
  number            = {19},
  volume            = {14},
  abstract          = {The article presents work on developing a software application for test creation using artificial intelligence and large language models. Its main goal is to optimize the educators’ work by automating the process of test generation and evaluation, with the tests being stored for subsequent analysis and use. The application can generate test questions based on specified criteria such as difficulty level, Bloom’s taxonomy level, question type, style and format, feedback inclusion, and more, thereby providing opportunities to enhance the adaptability and efficiency of the learning process. It is developed on the Google Firebase platform, utilizing the ChatGPT API, and also incorporates cloud computing to ensure scalability and data reliability. © 2024 by the authors.},
  author_keywords   = {AI in education; artificial intelligence; automated assessment; ChatGPT API; LLM; prompt engineering; student evaluation; test generation},
  doi               = {10.3390/app14199125},
  keywords          = {Automatic test pattern generation; Computer software selection and evaluation; Software testing; Students; AI in education; Automated assessment; Automated test; ChatGPT API; Language model; LLM; Prompt engineering; Software applications; Students' evaluations; Test generations; Application programs},
  priority          = {prio2},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206644172&doi=10.3390%2fapp14199125&partnerID=40&md5=92898ae973d389a008c8c09966da689d},
}

@Article{Zhang202414437,
  author            = {Zhang, Ao and Zhang, Yiying and Xu, Yao and Wang, Cong and Li, Siwei},
  journal           = {IEEE Access},
  title             = {Machine Learning-Based Fuzz Testing Techniques: A Survey},
  year              = {2024},
  note              = {Cited by: 2; All Open Access, Gold Open Access},
  pages             = {14437 – 14454},
  volume            = {12},
  abstract          = {Fuzz testing is a vulnerability discovery technique that tests the robustness of target programs by providing them with unconventional data. With the rapid increase in software quantity, scale and complexity, traditional fuzzing has revealed issues such as incomplete logic coverage, low automation level and insufficient test cases. Machine learning, with its exceptional capabilities in data analysis and classification prediction, presents a promising approach for improve fuzzing. This paper investigates the latest research results in fuzzing and provides a systematic review of machine learning-based fuzzing techniques. Firstly, by outlining the workflow of fuzzing, it summarizes the optimization of different stages of fuzzing using machine learning. Specifically, it focuses on the application of machine learning in the preprocessing phase, test case generation phase, input selection phase and result analysis phase. Secondly, it mentally focuses on the optimization methods of machine learning in the process of mutation, generation and filtering of test cases and compares and analyzes its technical principles. Furthermore, it analyzes the performance gains brought by applying machine learning techniques to fuzzing, mainly including coverage, vulnerability detection capability, efficiency and effectiveness of test cases. Lastly, it concludes by summarizing the challenges and difficulties in combining machine learning with fuzzing and presents prospects for future trends in this field.  © 2013 IEEE.},
  author_keywords   = {fuzzing; machine learning; Vulnerability discovery},
  doi               = {10.1109/ACCESS.2023.3347652},
  keywords          = {Artificial intelligence; Learning systems; Software testing; Closed box; Fuzzing; Glass box; Machine learning algorithms; Machine-learning; Performance Gain; Test case; Vulnerability discovery; Learning algorithms},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181559106&doi=10.1109%2fACCESS.2023.3347652&partnerID=40&md5=22ae009d6ba869c5ad5ca936a5d5cdae},
}

@Article{Bahi202454,
  author            = {Bahi, Anas and Gharib, Jihane and Gahi, Youssef},
  journal           = {International Journal of Advanced Computer Science and Applications},
  title             = {Integrating Generative AI for Advancing Agile Software Development and Mitigating Project Management Challenges},
  year              = {2024},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  number            = {3},
  pages             = {54 – 61},
  volume            = {15},
  abstract          = {Agile software development emphasizes iterative progress, adaptability, and stakeholder collaboration. It champions flexible planning, continuous improvement, and rapid delivery, aiming to respond swiftly to change and deliver value efficiently. Integrating Generative Artificial Intelligence (AI) into Agile software development processes presents a promising avenue for overcoming project management challenges and enhancing the efficiency and effectiveness of software development endeavors. This paper explores the potential benefits of leveraging Generative AI in Agile methodologies, aiming to streamline development workflows, foster innovation, and mitigate common project management challenges. By harnessing the capabilities of Generative AI for tasks such as code generation, automated testing, and predictive analytics, Agile teams can augment their productivity, accelerate delivery cycles, and improve the quality of software products. Additionally, Generative AI offers opportunities for enhancing collaboration, facilitating decision-making, and addressing uncertainties inherent in Agile project management. Through an in-depth analysis of the integration of Generative AI within Agile frameworks, this paper provides insights into how organizations can harness the transformative potential of AI to advance Agile software development practices and navigate the complexities of modern software projects more effectively. © (2024), (Science and Information Organization). All Rights Reserved.},
  author_keywords   = {Agile software development; Artificial intelligence; software engineering},
  doi               = {10.14569/IJACSA.2024.0150306},
  keywords          = {Decision making; Iterative methods; Predictive analytics; Project management; Software design; Software testing; Agile Methodologies; Agile software development; Agile software development process; Codegeneration; Common projects; Continuous improvements; Development workflow; Flexible planning; Management challenges; Potential benefits; Artificial intelligence},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189940766&doi=10.14569%2fIJACSA.2024.0150306&partnerID=40&md5=7c9a6d1d09b1dc98d3d23c6146f4f519},
}

@Article{Ghani2024114974,
  author            = {Ghani, Israr and Wan Kadir, Wan Mohd Nasir and Arbain, Adila Firdaus and Ghani, Imran},
  journal           = {IEEE Access},
  title             = {A Detection-Based Multi-Objective Test Case Selection Algorithm to Improve Time and Efficiency in Regression Testing},
  year              = {2024},
  note              = {Cited by: 1; All Open Access, Gold Open Access},
  pages             = {114974 – 114994},
  volume            = {12},
  abstract          = {Regression testing is carried out to ensure that changes or enhancements are not impacting previous working software. Deciding how much retesting is required after modifications, bug fixes or before product deployments are difficult. Therefore, Test Case Selection (TCS) select the satisfactory subset of modified test cases from already executed test suites. The testing primary concerns in TCS for regression testing are efficiency (i.e., coverage, fault detection ability, redundancy) and time. The first challenge in TCS concerns the efficiency of multi-objective test case selection. The second challenge is to improve the execution time to detect the changes in a test suite, which makes it impractical to use these efficiency measures as a single goal for TCS. To overcome these challenges, there is a need to introduce an efficient detection-based multi-objective framework to improve the Time and efficiency of TCS. A multi-objective advanced and efficient regression test case selection (ARTeCS) framework is devised to improve the time performance and efficiency of a given TCS objective relative to the other TCS approaches. An algorithm to detect the changes in test cases using multiple TCS objectives. This comparison found that the enhanced ARTeCS algorithm improves redundancy efficiency by 44.02%. The selection technique showed ARTeCS improved the modified change detection by 43.00%, whereas the Hybrid Whale Optimization Algorithm (HWOA) stated 23% and ACO showed 33% only for selected test cases. Regarding average for fault detection, ACO scores 21%, HWOA scores 11%, and ARTeCS scores 31.08% with total execution times of 12, 21 and 09 seconds, respectively. In conclusion, the multiple-objective ARTeCS framework with four test suite selection parameters is more efficient than the existing multi-objective selection framework. © 2013 IEEE.},
  author_keywords   = {multi-objective approach in TCS; regression testing; Software testing; TCS algorithm; TCS framework; test case selection},
  doi               = {10.1109/ACCESS.2024.3435678},
  keywords          = {Ability testing; Artificial intelligence; Fault detection; Redundancy; Regression analysis; Software testing; Code; Faults detection; Multi objective; Multi-objective approach in test case selection; Objective approaches; Regression testing; Selection algorithm; Selection framework; Software; Software algorithms; Software testings; Test case selection; Test case selection algorithm; Test case selection framework; Efficiency},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200203676&doi=10.1109%2fACCESS.2024.3435678&partnerID=40&md5=d2f7bb5df78bc6ee18afeb8590128afd},
}

@Article{Layman202427,
  author            = {Layman, Lucas and Vetter, Ron},
  journal           = {Computer},
  title             = {Generative Artificial Intelligence and the Future of Software Testing},
  year              = {2024},
  note              = {Cited by: 9; All Open Access, Bronze Open Access},
  number            = {1},
  pages             = {27 – 32},
  volume            = {57},
  abstract          = {This virtual roundtable focuses on applications of generative artificial intelligence (GenAI) to software testing with four leading experts from the field. Our experts reflect on transforming the work of software testing with GenAI, its impact on quality assurance engineers, and privacy concerns. COMPUTER0018-9162/24 ©2024 IEEE.},
  doi               = {10.1109/MC.2023.3306998},
  keywords          = {Application programs; Artificial intelligence; Quality assurance; Leading experts; Privacy concerns; Software testings; Software testing},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183077943&doi=10.1109%2fMC.2023.3306998&partnerID=40&md5=b75bacfb63d6710200a262db6e818d28},
}

@Article{González-Manzano2025,
  author            = {González-Manzano, Lorena and Garcia-Alfaro, Joaquin},
  journal           = {International Journal of Information Security},
  title             = {Software vulnerability detection under poisoning attacks using CNN-based image processing},
  year              = {2025},
  note              = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
  number            = {2},
  volume            = {24},
  abstract          = {Design flows, code errors, or inadequate countermeasures may occur in software development. Some of them lead to vulnerabilities in the code, opening the door to attacks. Assorted techniques are developed to detect vulnerable code samples, making artificial intelligence techniques, such as Machine Learning (ML), a common practice. Nonetheless, the security of ML is a major concern. This includes the the case of ML-based detection whose training process is affected by data poisoning. More generally, vulnerability detection can be evaded unless poisoning attacks are properly handled. This paper tackles this problem. A novel vulnerability detection system based on ML-based image processing, using Convolutional Neural Network (CNN), is proposed. The system, hereinafter called IVul, is evaluated under the presence of backdoor attacks, a precise type of poisoning in which a pattern is introduced in the training data to alter the expected behavior of the learned models. IVul is evaluated with more than three thousand code samples associated with two representative programming languages (C# and PHP). IVul outperforms other comparable state-of-the-art vulnerability detectors in the literature, reaching 82% to 99% detection accuracy. Besides, results show that the type of attack may affect a particular language more than another, though, in general, PHP is more resilient to proposed attacks than C#. © The Author(s) 2025.},
  author_keywords   = {Artificial Intelligence; Convolutional neural networks; Machine learning; Poisoning attack; Software vulnerability detection},
  doi               = {10.1007/s10207-025-00989-2},
  keywords          = {Adversarial machine learning; C (programming language); Image coding; Network security; Problem oriented languages; Artificial intelligence techniques; Convolutional neural network; Design flows; Images processing; Machine-learning; Network-based; Poisoning attacks; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Convolutional neural networks},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219748986&doi=10.1007%2fs10207-025-00989-2&partnerID=40&md5=a9e3834d3b3d12baf419356a84202afb},
}

@Article{Mustaqeem2024,
  author            = {Mustaqeem, Mohd and Mustajab, Suhel and Alam, Mahfooz and Jeribi, Fathe and Alam, Shadab and Shuaib, Mohammed},
  journal           = {PLoS ONE},
  title             = {A trustworthy hybrid model for transparent software defect prediction: SPAM-XAI},
  year              = {2024},
  note              = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access},
  number            = {7 July},
  volume            = {19},
  abstract          = {Maintaining quality in software development projects is becoming very difficult because the complexity of modules in the software is growing exponentially. Software defects are the primary concern, and software defect prediction (SDP) plays a crucial role in detecting faulty modules early and planning effective testing to reduce maintenance costs. However, SDP faces challenges like imbalanced data, high-dimensional features, model overfitting, and outliers. Moreover, traditional SDP models lack transparency and interpretability, which impacts stakeholder confidence in the Software Development Life Cycle (SDLC). We propose SPAM-XAI, a hybrid model integrating novel sampling, feature selection, and eXplainable-AI (XAI) algorithms to address these challenges. The SPAM-XAI model reduces features, optimizes the model, and reduces time and space complexity, enhancing its robustness. The SPAM-XAI model exhibited improved performance after experimenting with the NASA PROMISE repository’s datasets. It achieved an accuracy of 98.13% on CM1, 96.00% on PC1, and 98.65% on PC2, surpassing previous state-of-the-art and baseline models with other evaluation matrices enhancement compared to existing methods. The SPAM-XAI model increases transparency and facilitates understanding of the interaction between features and error status, enabling coherent and comprehensible predictions. This enhancement optimizes the decision-making process and enhances the model’s trustworthiness in the SDLC. © 2024 Mustaqeem et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  doi               = {10.1371/journal.pone.0307112},
  keywords          = {Algorithms; Artificial Intelligence; Humans; Models, Theoretical; Software; adult; algorithm; article; decision making; explainable artificial intelligence; feature selection; human; life cycle; male; prediction; software; trustworthiness; algorithm; artificial intelligence; theoretical model},
  priority          = {prio1},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198363847&doi=10.1371%2fjournal.pone.0307112&partnerID=40&md5=1c51064eaa98ec865c2fde043df74115},
}

@Article{Garrad2023,
  author            = {Garrad, Phillip and Unnikrishnan, Saritha},
  journal           = {Results in Engineering},
  title             = {Reinforcement learning in VANET penetration testing},
  year              = {2023},
  note              = {Cited by: 7; All Open Access, Gold Open Access},
  volume            = {17},
  abstract          = {The recent popularity of Connected and Autonomous Vehicles (CAV) corresponds with an increase in the risk of cyber-attacks. These cyber-attacks are instigated by white-coat hackers, and cyber-criminals. As Connected Vehicles move towards full autonomy the impact of these cyber-attacks also grows. The current research highlights challenges faced in cybersecurity testing of CAV, including access, the cost of representative test setup and the lack of experts in the field. Possible solutions of how these challenges can be overcome are reviewed and discussed. From these findings a software simulated Vehicular Ad Hoc NETwork (VANET) is established as a cost-effective representative testbed. Penetration tests are then performed on this simulation, demonstrating a cyber-attack in CAV. Studies have shown Artificial Intelligence (AI) to improve runtime, increase efficiency and comprehensively cover all the typical test aspects, in penetration testing in other industries. In this research a Reinforcement Learning model, called Q-Learning, is applied to automate the software simulation. The expectation from this implementation is to see improvements in runtime and efficiency for the VANET model. The results show this approach to be promising and using AI in penetration testing for VANET to improve efficiency in most cases. Each case is reviewed in detail before discussing possible ways to improve the implementation and get a truer reflection of the real-world application. © The Authors},
  author_keywords   = {Artificial intelligence; Connected vehicles; Cybersecurity; Penetration testing; Software simulation},
  doi               = {10.1016/j.rineng.2023.100970},
  keywords          = {Computer crime; Cost effectiveness; Crime; Cyber attacks; Efficiency; Learning algorithms; Network security; Personal computing; Reinforcement learning; Software testing; Vehicular ad hoc networks; Autonomous Vehicles; Connected vehicle; Cyber security; Cyber-attacks; Network penetration testing; Penetration testing; Reinforcement learnings; Runtimes; Software simulation; Vehicular Adhoc Networks (VANETs); Vehicles},
  priority          = {prio3},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149917954&doi=10.1016%2fj.rineng.2023.100970&partnerID=40&md5=388f4c523d9f12c974b02972aa52f9b3},
}

@Article{Geethal20234526,
  author            = {Geethal, Charaka and Bohme, Marcel and Pham, Van-Thuan},
  journal           = {IEEE Transactions on Software Engineering},
  title             = {Human-in-the-Loop Automatic Program Repair},
  year              = {2023},
  note              = {Cited by: 2; All Open Access, Green Open Access, Hybrid Gold Open Access},
  number            = {10},
  pages             = {4526 – 4549},
  volume            = {49},
  abstract          = {learn2fix is a human-in-the-loop interactive program repair technique, which can be applied when no bug oracle - except the user who is reporting the bug - is available. This approach incrementally learns the condition under which the bug is observed by systematic negotiation with the user. In this process, learn2fix generates alternative test inputs and sends some of those to the user for obtaining their labels. A limited query budget is assigned to the user for this task. A query is a Yes/No question: 'When executing this alternative test input, the program under test produces the following output; is the bug observed?'. Using the labelled test inputs, learn2fix incrementally learns an automatic bug oracle to predict the user's response. A classification algorithm in machine learning is used for this task. Our key challenge is to maximise the oracle's accuracy in predicting the tests that expose the bug given a practical, small budget of queries. After learning the automatic oracle, an existing program repair tool attempts to repair the bug using the alternative tests that the user has labelled. Our experiments demonstrate that learn2fix trains a sufficiently accurate automatic oracle with a reasonably low labelling effort (lt. 20 queries), and the oracles represented by interpolation-based classifiers produce more accurate predictions than those represented by approximation-based classifiers. Given the user-labelled test inputs, generated using the interpolation-based approach, the GenProg and Angelix automatic program repair tools produce patches that pass a much larger proportion of validation tests than the manually constructed test suites provided by the repair benchmark.  © 1976-2012 IEEE.},
  author_keywords   = {active machine learning; Automated test oracles; classification algorithms; semi-automatic program repair},
  doi               = {10.1109/TSE.2023.3305052},
  keywords          = {Artificial intelligence; Budget control; Costs; Forecasting; Interpolation; Learning algorithms; Learning systems; Program debugging; Query processing; Software testing; Active machine learning; Automated test; Automated test oracle; Automatic programs; Classification algorithm; Computer bugs; Fuzzing; Human-in-the-loop; Labelings; Semi-automatic program repair; Semi-automatics; Test oracles; Repair},
  priority          = {prio2},
  publication_stage = {Final},
  source            = {Scopus},
  type              = {Article},
  url               = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168713494&doi=10.1109%2fTSE.2023.3305052&partnerID=40&md5=54179bf4493f0d4fd86d368b593963ec},
}

@Comment{jabref-meta: databaseType:bibtex;}
