Scopus
EXPORT DATE: 10 June 2025

@ARTICLE{Lu2024,
	author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Zowghi, Didar and Jacquet, Aurelie},
	title = {Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering},
	year = {2024},
	journal = {ACM Computing Surveys},
	volume = {56},
	number = {7},
	doi = {10.1145/3626234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191069804&doi=10.1145%2f3626234&partnerID=40&md5=a0725376767e5345f50a4fd084dbf5f0},
	abstract = {Responsible Artificial Intelligence (RAI) is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of Artificial Intelligence (AI). Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. In addition, significant efforts have been placed at algorithm level rather than system level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize RAI from a system perspective, in this article, we present an RAI Pattern Catalogue based on the results of a multivocal literature review. Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The RAI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and RAI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement RAI.  © 2024 Copyright held by the owner/author(s).},
	author_keywords = {AI engineering; AI governance; best practice; ethical AI; MLOps; pattern; Responsible AI; software architecture; software engineering; trustworthy AI},
	keywords = {Life cycle; Philosophical aspects; Product design; Software architecture; Algorithm-level; Artificial intelligence engineering; Artificial intelligence governance; Artificial intelligence systems; Best practices; Ethical artificial intelligence; MLOp; Pattern; Responsible artificial intelligence; Trustworthy artificial intelligence; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 33; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Chinnaiah2024531,
	author = {Chinnaiah, Veluru and Veerabhadram, Vadlamani and Aavula, Ravi and Aluvala, Srinivas},
	title = {PMiner: Process Mining using Deep Autoencoder for Anomaly Detection and Reconstruction of Business Processes},
	year = {2024},
	journal = {International Journal of Electrical and Computer Engineering Systems},
	volume = {15},
	number = {6},
	pages = {531 – 542},
	doi = {10.32985/ijeces.15.6.7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196426044&doi=10.32985%2fijeces.15.6.7&partnerID=40&md5=439b9c76dce961dc72e632d7186726b4},
	abstract = {We proposed a deep learning-based process mining framework known as PMiner for automatic detection of anomalies in business processes. Since there are thousands of business processes in real-time applications such as e-commerce, in the presence of concurrency, they are prone to exhibit anomalies. Such anomalies if not detected and rectified, cause severe damage to businesses in the long run. Our Artificial Intelligence (AI) enabled framework PMiner takes business process event longs as input and detects anomalies using a deep autoencoder. The framework exploits a deep autoencoder technique which is well-known for Its ability to discriminate anomalies. We proposed an algorithm known as Intelligent Business Process Anomaly Detector (IBPAD) to realize the framework. This algorithm learns from historical data and performs encoding and decoding procedures to detect business process anomalies automatically. Our empirical results using the BPI Challenge dataset, released by the IEEE Task Force on Process Mining, revealed that PMiner outperforms state-of-the-art methods in detecting business process anomalies. This framework helps businesses to identify process anomalies and rectify them in time to leverage business continuity prospects. © 2024, J.J. Strossmayer University of Osijek, Faculty of Electrical Engineering, Computer Science and Information Technology. All rights reserved.},
	author_keywords = {Artificial Intelligence; Deep Autoencoder; Deep Learning; Long Short Term Memory; Process Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Jia2024,
	author = {Jia, Xiaodong and Tan, Gang},
	title = {V-Star: Learning Visibly Pushdown Grammars from Program Inputs},
	year = {2024},
	journal = {Proceedings of the ACM on Programming Languages},
	volume = {8},
	doi = {10.1145/3656458},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196771834&doi=10.1145%2f3656458&partnerID=40&md5=3ba77f86cee437c649a36382da61a53e},
	abstract = {Accurate description of program inputs remains a critical challenge in the field of programming languages. Active learning, as a well-established field, achieves exact learning for regular languages. We offer an innovative grammar inference tool, V-Star, based on the active learning of visibly pushdown automata. V-Star deduces nesting structures of program input languages from sample inputs, employing a novel inference mechanism based on nested patterns. This mechanism identifies token boundaries and converts languages such as XML documents into VPLs. We then adapted Angluin's L-Star, an exact learning algorithm, for VPA learning, which improves the precision of our tool. Our evaluation demonstrates that V-Star effectively and efficiently learns a variety of practical grammars, including S-Expressions, JSON, and XML, and outperforms other state-of-The-Art tools.  © 2024 Owner/Author.},
	author_keywords = {grammar inference; visibly pushdown grammars},
	keywords = {Artificial intelligence; Learning algorithms; XML; Active Learning; Critical challenges; Exact learning; Grammar inference; Inference mechanism; Inference tools; Mechanism-based; Pushdown; Visibly pushdown automaton; Visibly pushdown grammar; Stars},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Grau-Luque2024,
	author = {Grau-Luque, Enric and Becerril-Romero, Ignacio and Atlan, Fabien and Huber, Daniel and Harnisch, Martina and Zimmermann, Andreas and Pérez-Rodríguez, Alejandro and Guc, Maxim and Izquierdo-Roca, Victor},
	title = {Accelerating the Development of Thin Film Photovoltaic Technologies: An Artificial Intelligence Assisted Methodology Using Spectroscopic and Optoelectronic Techniques},
	year = {2024},
	journal = {Small Methods},
	volume = {8},
	number = {12},
	doi = {10.1002/smtd.202301573},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188949149&doi=10.1002%2fsmtd.202301573&partnerID=40&md5=898c075f262e0e64e354aaeccff34b56},
	abstract = {Thin film photovoltaic (TFPV) materials and devices present a high complexity with multiscale, multilayer, and multielement structures and with complex fabrication procedures. To deal with this complexity, the evaluation of their physicochemical properties is critical for generating a model that proposes strategies for their development and optimization. However, this process is time-consuming and requires high expertise. In this context, the adoption of combinatorial analysis (CA) and artificial intelligence (AI) strategies represents a powerful asset for accelerating the development of these complex materials and devices. This work introduces a methodology to facilitate the adoption of AI and CA for the development of TFPV technologies. The methodology covers all the necessary steps from the synthesis of samples for CA to data acquisition, AI-assisted data analysis, and the extraction of relevant information for research acceleration. Each step provides details on the necessary concepts, requirements, and procedures and are illustrated with examples from the literature. Then, the application of the methodology to a complex set of samples from a TFPV production line highlights its ability to rapidly glean significant insights even in intricate scenarios. The proposed methodology can be applied to other types of materials and devices beyond PV and using different characterization techniques. © 2024 The Authors. Small Methods published by Wiley-VCH GmbH.},
	author_keywords = {accelerated research; artificial intelligence; combinatorial analysis; methodology; thin film photovoltaics},
	keywords = {Artificial intelligence; Data acquisition; Film preparation; Thin films; Accelerated research; Combinatorial analysis; Fabrication procedure; High complexity; Methodology; Multielements; Photovoltaic devices; Photovoltaic materials; Photovoltaic technology; Thin film photovoltaics; acceleration; article; artificial intelligence; controlled study; data analysis; drug development; methodology; nonhuman; physical chemistry; production line; spectroscopy; synthesis; Physicochemical properties},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Amalfitano2024,
	author = {Amalfitano, Domenico and Faralli, Stefano and Hauck, Jean Carlo Rossa and Matalonga, Santiago and Distante, Damiano},
	title = {Artificial Intelligence Applied to Software Testing: A Tertiary Study},
	year = {2024},
	journal = {ACM Computing Surveys},
	volume = {56},
	number = {3},
	doi = {10.1145/3616372},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176768820&doi=10.1145%2f3616372&partnerID=40&md5=c9dfb5d6c6d2d99c777d457e6d4d7c54},
	abstract = {Context: Artificial intelligence (AI) methods and models have extensively been applied to support different phases of the software development lifecycle, including software testing (ST). Several secondary studies investigated the interplay between AI and ST but restricted the scope of the research to specific domains or sub-domains within either area.Objective: This research aims to explore the overall contribution of AI to ST, while identifying the most popular applications and potential paths for future research directions.Method: We executed a tertiary study following well-established guidelines for conducting systematic literature mappings in software engineering and for answering nine research questions.Results: We identified and analyzed 20 relevant secondary studies. The analysis was performed by drawing from well-recognized AI and ST taxonomies and mapping the selected studies according to them. The resulting mapping and discussions provide extensive and detailed information on the interplay between AI and ST.Conclusion: The application of AI to support ST is a well-consolidated and growing interest research topic. The mapping resulting from our study can be used by researchers to identify opportunities for future research, and by practitioners looking for evidence-based information on which AI-supported technology to possibly adopt in their testing processes. © 2023 Copyright held by the owner/author(s).},
	author_keywords = {Artificial intelligence; Software testing; Systematic literature review; Systematic mapping study; Taxonomy; Tertiary study},
	keywords = {Artificial intelligence; Life cycle; Mapping; Software design; Software testing; Artificial intelligence methods; Future research directions; Research questions; Research topics; Software development life-cycle; Software testings; Subdomain; Systematic literature review; Systematic mapping studies; Tertiary study; Taxonomies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Farshidi2024137,
	author = {Farshidi, Siamak and Kwantes, Izaak Beer and Jansen, Slinger},
	title = {Business process modeling language selection for research modelers},
	year = {2024},
	journal = {Software and Systems Modeling},
	volume = {23},
	number = {1},
	pages = {137 – 162},
	doi = {10.1007/s10270-023-01110-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160449334&doi=10.1007%2fs10270-023-01110-8&partnerID=40&md5=23b50531b66804d45a4801427a22bd47},
	abstract = {Business process modeling is a crucial aspect of domains such as Business Process Management and Software Engineering. The availability of various BPM languages in the market makes it challenging for process modelers to select the best-fit BPM language for a specific process modeling task. A decision model is necessary to systematically capture and make scattered knowledge on BPM languages available for reuse by process modelers and academics. This paper presents a decision model for the BPM language selection problem in research projects. The model contains mappings of 72 BPM features to 23 BPM languages. We validated and refined the decision model through 10 expert interviews with domain experts from various organizations. We evaluated the efficiency, validity, and generality of the decision model by conducting four case studies of academic research projects with their original researchers. The results confirmed that the decision model supports process modelers in the selection process by providing more insights into the decision process. Based on the empirical evidence from the case studies and domain expert feedback, we conclude that having the knowledge readily available in the decision model supports academics in making more informed decisions that align with their preferences and prioritized requirements. Furthermore, the captured knowledge provides a comprehensive overview of BPM languages, features, and quality characteristics that other researchers can employ to tackle future research challenges. Our observations indicate that BPMN is a commonly used modeling language for process modeling. Therefore, it is more sensible for academics to explain why they did not select BPMN than to discuss why they chose it for their research project(s). © The Author(s) 2023.},
	author_keywords = {Business process modeling language selection; Case study research; Decision model; Decision support system; Multi-criteria decision-making},
	keywords = {Artificial intelligence; Decision making; Modeling languages; Process engineering; Software engineering; Business process modeling language selection; Business process modeling languages; Case study research; Case-studies; Decision modeling; Domain experts; Multi criteria decision-making; Multicriteria decision-making; Multicriterion decision makings; Process-models; Decision support systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Li2024,
	author = {Li, Zhixin and Luo, Gang and Ji, Zhixian and Wang, Sibao and Pan, Silin},
	title = {Explanatory deep learning to predict elevated pulmonary artery pressure in children with ventricular septal defects using standard chest x-rays: a novel approach},
	year = {2024},
	journal = {Frontiers in Cardiovascular Medicine},
	volume = {11},
	doi = {10.3389/fcvm.2024.1330685},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183002849&doi=10.3389%2ffcvm.2024.1330685&partnerID=40&md5=b80353d16370636ef022ee86f1d373ab},
	abstract = {Objective: Early risk assessment of pulmonary arterial hypertension (PAH) in patients with congenital heart disease (CHD) is crucial to ensure timely treatment. We hypothesize that applying artificial intelligence (AI) to chest x-rays (CXRs) could identify the future risk of PAH in patients with ventricular septal defect (VSD). Methods: A total of 831 VSD patients (161 PAH-VSD, 670 nonPAH-VSD) was retrospectively included. A residual neural networks (ResNet) was trained for classify VSD patients with different outcomes based on chest radiographs. The endpoint of this study was the occurrence of PAH in VSD children before or after surgery. Results: In the validation set, the AI algorithm achieved an area under the curve (AUC) of 0.82. In an independent test set, the AI algorithm significantly outperformed human observers in terms of AUC (0.81 vs. 0.65). Class Activation Mapping (CAM) images demonstrated the model's attention focused on the pulmonary artery segment. Conclusion: The preliminary findings of this study suggest that the application of artificial intelligence to chest x-rays in VSD patients can effectively identify the risk of PAH. 2024 Li, Luo, Ji, Wang and Pan.},
	author_keywords = {artificial intelligence; chest x-ray; deep learning—artificial intelligence; pulmonary arterial hypertension; ventricular septal defect},
	keywords = {area under the curve; Article; artificial intelligence; child; cohort analysis; controlled study; deep learning; diagnostic test accuracy study; echocardiography; heart ventricle septum defect; hospitalization; human; lung artery pressure; male; outcome assessment; pulmonary hypertension; receiver operating characteristic; residual neural network; retrospective study; school child; thorax radiography; training},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Esposito2024,
	author = {Esposito, Mark and Sarbazvatan, Saman and Tse, Terence and Silva-Atencio, Gabriel},
	title = {The use of artificial intelligence for automatic analysis and reporting of software defects},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence},
	volume = {7},
	doi = {10.3389/frai.2024.1443956},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212822758&doi=10.3389%2ffrai.2024.1443956&partnerID=40&md5=b91ccf0600bec4378facfe74ab42d3a7},
	abstract = {The COVID-19 pandemic marked a before and after in the business world, causing a growing demand for applications that streamline operations, reduce delivery times and costs, and improve the quality of products. In this context, artificial intelligence (AI) has taken a relevant role in improving these processes, since it incorporates mathematical models that allow analyzing the logical structure of the systems to detect and reduce errors or failures in real-time. This study aimed to determine the most relevant aspects to be considered for detecting software defects using AI. The methodology used was qualitative, with an exploratory, descriptive, and non-experimental approach. The technique involved a documentary review of 79 bibliometric references. The most relevant finding was the use of regression testing techniques and automated log files, in machine learning (ML) and robotic process automation (RPA) environments. These techniques help reduce the time required to identify failures, thereby enhancing efficiency and effectiveness in the lifecycle of applications. In conclusion, companies that incorporate AI algorithms will be able to include an agile model in their lifecycle, as they will reduce the rate of failures, errors, and breakdowns allowing cost savings, and ensuring quality. Copyright © 2024 Esposito, Sarbazvatan, Tse and Silva-Atencio.},
	author_keywords = {artificial intelligence; automation testing; software defect; software development; software failure},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Bello202429,
	author = {Bello, K.A. and Kanakana-Katumba, M.G. and Maladzhi, R.W. and Omoyi, C.O.},
	title = {Recent Advances in Smart Manufacturing: A Case Study of Small, Medium, and Micro Enterprises (SMME)},
	year = {2024},
	journal = {Nigerian Journal of Technological Development},
	volume = {21},
	number = {1},
	pages = {29 – 41},
	doi = {10.4314/njtd.v21i1.1905},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188046720&doi=10.4314%2fnjtd.v21i1.1905&partnerID=40&md5=c11dc3a4e8afc5adb52c8413c798d1a1},
	abstract = {The digital revolution is the future pathway to experiencing business evolution. It will be difficult for any organization to remain in business without deploying fourth industry revolution (4IR) techniques. This review guides prospective business owners on the need to embrace Smart Manufacturing (SM) timely and appropriate to enhance their business performance indicators. Many manufacturing companies are facing challenges in adopting SM tools in their organization due to a lack of essential resources despite the benefits associated with SM. Therefore, this study systematically reviewed the criteria evaluation techniques in implementing digital factories. This work has analysed small, medium, and micro enterprises (SMME), with the view to enumerate the appropriate criteria to determine the level of digital technology tools adoption framework. It also highlights how to compensate for the inadequate technical and financial resources in SM. The guidelines for SM implementation adoption in SMME is a research gap that was missing in previous studies. SM benefits, challenges, applications, significance, impact, and future perspectives, are discussed. Evaluation Criteria for SM Adoption practices were also expounded. The framework used in this study will help SMME owners to adopt SM. © 2024, University of Ilorin, Faculty of Engineering and Technology. All rights reserved.},
	author_keywords = {Artificial intelligence; Automation; Digitalization; Fourth Industrial Revolution; Smart Manufacturing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Sonkin2025,
	author = {Sonkin, Vladimir and Tudose, Cătălin},
	title = {Beyond Snippet Assistance: A Workflow-Centric Framework for End-to-End AI-Driven Code Generation},
	year = {2025},
	journal = {Computers},
	volume = {14},
	number = {3},
	doi = {10.3390/computers14030094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001419180&doi=10.3390%2fcomputers14030094&partnerID=40&md5=94849e69f1b2c3aad6dc2a2d257c7681},
	abstract = {Recent AI-assisted coding tools, such as GitHub Copilot and Cursor, have enhanced developer productivity through real-time snippet suggestions. However, these tools primarily assist with isolated coding tasks and lack a structured approach to automating complex, multi-step software development workflows. This paper introduces a workflow-centric AI framework for end-to-end automation, from requirements gathering to code generation, validation, and integration, while maintaining developer oversight. Key innovations include automatic context discovery, which selects relevant codebase elements to improve LLM accuracy; a structured execution pipeline using Prompt Pipeline Language (PPL) for iterative code refinement; self-healing mechanisms that generate tests, detect errors, trigger rollbacks, and regenerate faulty code; and AI-assisted code merging, which preserves manual modifications while integrating AI-generated updates. These capabilities enable efficient automation of repetitive tasks, enforcement of coding standards, and streamlined development workflows. This approach lays the groundwork for AI-driven development that remains adaptable as LLM models advance, progressively reducing the need for human intervention while ensuring code reliability. © 2025 by the authors.},
	author_keywords = {AI code review; artificial intelligence; developer productivity improvement; JAIG; LLM; prompt engineering; routine tasks; software development automation},
	keywords = {Coding errors; Multiprocessing programs; Pipeline codes; Systems analysis; AI code review; Code review; Developer productivity improvement; JAIG; LLM; Productivity improvements; Prompt engineering; Routine task; Software development automation; Work-flows; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Alenezi2025,
	author = {Alenezi, Mamdouh and Akour, Mohammed},
	title = {AI-Driven Innovations in Software Engineering: A Review of Current Practices and Future Directions},
	year = {2025},
	journal = {Applied Sciences (Switzerland)},
	volume = {15},
	number = {3},
	doi = {10.3390/app15031344},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217549714&doi=10.3390%2fapp15031344&partnerID=40&md5=496085eaddfeaddffef6c5383232a4e9},
	abstract = {The software engineering landscape is undergoing a significant transformation with the advent of artificial intelligence (AI). AI technologies are poised to redefine traditional software development practices, offering innovative solutions to long-standing challenges. This paper explores the integration of AI into software engineering processes, aiming to identify its impacts, benefits, and the challenges that accompany this paradigm shift. A comprehensive analysis of current AI applications in software engineering is conducted, supported by case studies and theoretical models. The study examines various phases of software development to assess where AI contributes most effectively. The integration of AI enhances productivity, improves code quality, and accelerates development cycles. Key areas of impact include automated code generation, intelligent debugging, predictive maintenance, and enhanced decision-making processes. AI is revolutionizing software engineering by introducing automation and intelligence into the development lifecycle. Embracing AI-driven tools and methodologies is essential for staying competitive in the evolving technological landscape. © 2025 by the authors.},
	author_keywords = {artificial intelligence (AI); automation; software development lifecycle; software engineering},
	keywords = {Application programs; Decision making; Life cycle; Program debugging; Software design; Software quality; 'current; Artificial intelligence; Artificial intelligence technologies; Comprehensive analysis; Current practices; Innovative solutions; Paradigm shifts; Software development life-cycle; Software development practices; Software engineering process; Predictive maintenance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Hadzhikoleva2024,
	author = {Hadzhikoleva, Stanka and Rachovski, Todor and Ivanov, Ivan and Hadzhikolev, Emil and Dimitrov, Georgi},
	title = {Automated Test Creation Using Large Language Models: A Practical Application},
	year = {2024},
	journal = {Applied Sciences (Switzerland)},
	volume = {14},
	number = {19},
	doi = {10.3390/app14199125},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206644172&doi=10.3390%2fapp14199125&partnerID=40&md5=92898ae973d389a008c8c09966da689d},
	abstract = {The article presents work on developing a software application for test creation using artificial intelligence and large language models. Its main goal is to optimize the educators’ work by automating the process of test generation and evaluation, with the tests being stored for subsequent analysis and use. The application can generate test questions based on specified criteria such as difficulty level, Bloom’s taxonomy level, question type, style and format, feedback inclusion, and more, thereby providing opportunities to enhance the adaptability and efficiency of the learning process. It is developed on the Google Firebase platform, utilizing the ChatGPT API, and also incorporates cloud computing to ensure scalability and data reliability. © 2024 by the authors.},
	author_keywords = {AI in education; artificial intelligence; automated assessment; ChatGPT API; LLM; prompt engineering; student evaluation; test generation},
	keywords = {Automatic test pattern generation; Computer software selection and evaluation; Software testing; Students; AI in education; Automated assessment; Automated test; ChatGPT API; Language model; LLM; Prompt engineering; Software applications; Students' evaluations; Test generations; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Zhang202414437,
	author = {Zhang, Ao and Zhang, Yiying and Xu, Yao and Wang, Cong and Li, Siwei},
	title = {Machine Learning-Based Fuzz Testing Techniques: A Survey},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {14437 – 14454},
	doi = {10.1109/ACCESS.2023.3347652},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181559106&doi=10.1109%2fACCESS.2023.3347652&partnerID=40&md5=22ae009d6ba869c5ad5ca936a5d5cdae},
	abstract = {Fuzz testing is a vulnerability discovery technique that tests the robustness of target programs by providing them with unconventional data. With the rapid increase in software quantity, scale and complexity, traditional fuzzing has revealed issues such as incomplete logic coverage, low automation level and insufficient test cases. Machine learning, with its exceptional capabilities in data analysis and classification prediction, presents a promising approach for improve fuzzing. This paper investigates the latest research results in fuzzing and provides a systematic review of machine learning-based fuzzing techniques. Firstly, by outlining the workflow of fuzzing, it summarizes the optimization of different stages of fuzzing using machine learning. Specifically, it focuses on the application of machine learning in the preprocessing phase, test case generation phase, input selection phase and result analysis phase. Secondly, it mentally focuses on the optimization methods of machine learning in the process of mutation, generation and filtering of test cases and compares and analyzes its technical principles. Furthermore, it analyzes the performance gains brought by applying machine learning techniques to fuzzing, mainly including coverage, vulnerability detection capability, efficiency and effectiveness of test cases. Lastly, it concludes by summarizing the challenges and difficulties in combining machine learning with fuzzing and presents prospects for future trends in this field.  © 2013 IEEE.},
	author_keywords = {fuzzing; machine learning; Vulnerability discovery},
	keywords = {Artificial intelligence; Learning systems; Software testing; Closed box; Fuzzing; Glass box; Machine learning algorithms; Machine-learning; Performance Gain; Test case; Vulnerability discovery; Learning algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Malik202499382,
	author = {Malik, Jasmita and Muthalagu, Raja and Pawar, Pranav M.},
	title = {A Systematic Review of Adversarial Machine Learning Attacks, Defensive Controls, and Technologies},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {99382 – 99421},
	doi = {10.1109/ACCESS.2024.3423323},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197531321&doi=10.1109%2fACCESS.2024.3423323&partnerID=40&md5=1169fd4d69e2276a7de697a073f25bba},
	abstract = {Adversarial machine learning (AML) attacks have become a major concern for organizations in recent years, as AI has become the industry’s focal point and GenAI applications have grown in popularity around the world. Organizations are eager to invest in GenAI applications and develop their own large language models, but they face numerous security and data privacy issues, particularly AML attacks. AML attacks have jeopardized numerous large-scale machine learning models. If carried out successfully, AML attacks can significantly reduce the efficiency and precision of machine learning models. They have far-reaching negative consequences in the context of critical healthcare and autonomous transportation systems. In this paper, AML attacks are identified, analyzed, and classified using adversarial tactics and techniques. This research also recommends open-source tools for testing AI and ML models against AML attacks. Furthermore, this research suggests specific mitigating measures against each attack. It aims to serve as a guidance for organizations to defend against AML attacks and gain assurance in the security of ML models. © 2024 The Authors.},
	author_keywords = {Adversarial machine learning; AI assurance; cybersecurity; data privacy; secure software development lifecycle},
	keywords = {Application programs; Artificial intelligence; Cybersecurity; Learning systems; Life cycle; Open source software; Software design; Software testing; Adversarial machine learning; AI assurance; Computational modelling; Cyber security; Life cycle assessment; Machine-learning; Secure software development; Secure software development lifecycle; Security; Software development life-cycle; Software development management; Data privacy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Gold Open Access}
}

@ARTICLE{Alaswad20232097,
	author = {Alaswad, S. and Kalganova, T. and Awad, W.S.},
	title = {Using ChatGPT and other LLMs in Professional Environments},
	year = {2023},
	journal = {Information Sciences Letters},
	volume = {12},
	number = {9},
	pages = {2097 – 2108},
	doi = {10.18576/isl/120916},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172336382&doi=10.18576%2fisl%2f120916&partnerID=40&md5=ff10faa3ab119006bb20f1cc511ed37a},
	abstract = {Large language models like ChatGPT, Google’s Bard, and Microsoft’s new Bing, to name a few, are developing rapidly in recent years, becoming very popular in different environments, and supporting a wide range of tasks. A deep look into their outcomes reveals several limitations and challenges that can be further improved. The main challenge of these models is the possibility of generating biased or inaccurate results, since these models rely on large amounts of data with no access to unpublic information. Moreover, these language models need to be properly monitored and trained to prevent generating inappropriate or offensive content and to ensure that they are used ethically and safely. This study investigates the use of ChatGPT and other large language models such as Blender, and BERT in professional environments. It has been found that none of the large language models, including ChatGPT, have been used in unstructured dialogues. Moreover, involving the models in professional environments requires extensive training and monitoring by domain professionals or fine-tuning through API. © 2023 NSP Natural Sciences Publishing Cor.},
	author_keywords = {Artificial Intelligence; Chatbot; ChatGPT; LLM},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Meske2023743,
	author = {Meske, Christian and Bunde, Enrico},
	title = {Design Principles for User Interfaces in AI-Based Decision Support Systems: The Case of Explainable Hate Speech Detection},
	year = {2023},
	journal = {Information Systems Frontiers},
	volume = {25},
	number = {2},
	pages = {743 – 773},
	doi = {10.1007/s10796-021-10234-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125542214&doi=10.1007%2fs10796-021-10234-5&partnerID=40&md5=4dc9da137e577e709de78dd30c17c5a0},
	abstract = {Hate speech in social media is an increasing problem that can negatively affect individuals and society as a whole. Moderators on social media platforms need to be technologically supported to detect problematic content and react accordingly. In this article, we develop and discuss the design principles that are best suited for creating efficient user interfaces for decision support systems that use artificial intelligence (AI) to assist human moderators. We qualitatively and quantitatively evaluated various design options over three design cycles with a total of 641 participants. Besides measuring perceived ease of use, perceived usefulness, and intention to use, we also conducted an experiment to prove the significant influence of AI explainability on end users’ perceived cognitive efforts, perceived informativeness, mental model, and trustworthiness in AI. Finally, we tested the acquired design knowledge with software developers, who rated the reusability of the proposed design principles as high. © 2022, The Author(s).},
	author_keywords = {Design principles; Design science research; Explainable artificial intelligence; Hate speech detection; Local explanations},
	keywords = {Computer software reusability; Decision support systems; Moderators; Reusability; Social networking (online); Software testing; Speech recognition; User interfaces; Design cycle; Design option; Design Principles; Design-science researches; Explainable artificial intelligence; Hate speech detection; Local explanation; Social media; Social media platforms; Speech detection; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 46; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Uğraş2024,
	author = {Uğraş, Hilal and Uğraş, Mustafa and Papadakis, Stamatios and Kalogiannakis, Michail},
	title = {ChatGPT-Supported Education in Primary Schools: The Potential of ChatGPT for Sustainable Practices},
	year = {2024},
	journal = {Sustainability (Switzerland)},
	volume = {16},
	number = {22},
	doi = {10.3390/su16229855},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210254261&doi=10.3390%2fsu16229855&partnerID=40&md5=70186ffc649868d7ddb6939c85fd5c12},
	abstract = {This study aims to evaluate the potential of using ChatGPT at the primary school level from the teachers’ perspective within a sustainability framework. The research was conducted as a qualitative case study involving 40 primary school teachers in Turkey during the 2023–2024 academic year, all of whom had no prior experience with ChatGPT. Data collection tools included semi-structured interview forms and researcher diaries developed by the researchers. The data obtained were analysed using content analysis. The findings indicate that most primary school teachers believe ChatGPT is suitable for primary education and can contribute to Sustainable Development Goal (SDG) 4. Additionally, teachers noted that ChatGPT enriches the teaching process and is user-friendly. These findings suggest potential contributions to SDG 4.1 and SDG 4.2. However, concerns were raised regarding ChatGPT’s potential to provide false information, which may negatively impact SDG 4.7. The study also identified that ChatGPT is particularly suitable for mathematics, Turkish, and English courses. This study’s main contribution is that it shows how ChatGPT can help sustainable practices in primary education by getting teachers more involved and meeting specific curriculum needs. This gives us useful information for incorporating AI tools into education that is in line with SDG 4. It is recommended that training programs about ChatGPT and similar AI-supported tools be organised for teachers and parents. © 2024 by the authors.},
	author_keywords = {artificial intelligence (AI); ChatGPT; primary school level; primary school teachers; SDG4; sustainable},
	keywords = {Turkey; artificial intelligence; curriculum; primary education; software; sustainability; Sustainable Development Goal; teacher training; teaching},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Martins2023,
	author = {Martins, José and Branco, Frederico and Mamede, Henrique},
	title = {Combining low-code development with ChatGPT to novel no-code approaches: A focus-group study},
	year = {2023},
	journal = {Intelligent Systems with Applications},
	volume = {20},
	doi = {10.1016/j.iswa.2023.200289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174408846&doi=10.1016%2fj.iswa.2023.200289&partnerID=40&md5=eb07857c54dba2d9f1b448f90cf7dc21},
	abstract = {Low-code tools are a trend in software development for business solutions due to their agility and ease of use. There are a certain number of vendors with such solutions. Still, in most Western countries, there is a clear need for the existence of greater quantities of certified and experienced professionals to work with those tools. This means that companies with more resources can attract and maintain those professionals, whilst other smaller organizations must rely on an endless search for this scarce resource. We will present and validate a model designed to transform ChatGPT into a low-code developer, addressing the demand for a more skilled human resource solution. This innovative tool underwent rigorous validation via a focus group study, engaging a panel of highly experienced experts. Their invaluable insights and feedback on the proposed model were systematically gathered and meticulously analysed. © 2023 The Author(s)},
	author_keywords = {Artificial intelligence; ChatGPT; LLM; Low-code; No-code; Software models},
	keywords = {Business solutions; ChatGPT; Code development; Ease-of-use; Focus group studies; LLM; Low-code; No-code; Software modeling; Western countries; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Sarkar2023,
	author = {Sarkar, Sumon and Squire, Abigail and Diab, Hanin and Rahman, Md. Kaisar and Perdomo, Angela and Awosile, Babafela and Calle, Alexandra and Thompson, Jonathan},
	title = {Effect of Tryptic Digestion on Sensitivity and Specificity in MALDI-TOF-Based Molecular Diagnostics through Machine Learning},
	year = {2023},
	journal = {Sensors},
	volume = {23},
	number = {19},
	doi = {10.3390/s23198042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174072764&doi=10.3390%2fs23198042&partnerID=40&md5=28e738d8193476045e978a76e71a5b4e},
	abstract = {The digestion of protein into peptide fragments reduces the size and complexity of protein molecules. Peptide fragments can be analyzed with higher sensitivity (often > 102 fold) and resolution using MALDI-TOF mass spectrometers, leading to improved pattern recognition by common machine learning algorithms. In turn, enhanced sensitivity and specificity for bacterial sorting and/or disease diagnosis may be obtained. To test this hypothesis, four exemplar case studies have been pursued in which samples are sorted into dichotomous groups by machine learning (ML) software based on MALDI-TOF spectra. Samples were analyzed in ‘intact’ mode in which the proteins present in the sample were not digested with protease prior to MALDI-TOF analysis and separately after the standard overnight tryptic digestion of the same samples. For each case, sensitivity (sens), specificity (spc), and the Youdin index (J) were used to assess the ML model performance. The proteolytic digestion of samples prior to MALDI-TOF analysis substantially enhanced the sensitivity and specificity of dichotomous sorting. Two exceptions were when substantial differences in chemical composition between the samples were present and, in such cases, both ‘intact’ and ‘digested’ protocols performed similarly. The results suggest proteolytic digestion prior to analysis can improve sorting in MALDI/ML-based workflows and may enable improved biomarker discovery. However, when samples are easily distinguishable protein digestion is not necessary to obtain useful diagnostic results. © 2023 by the authors.},
	author_keywords = {artificial intelligence (AI); biomarker; diagnostic; machine learning; MALDI-TOF; molecular diagnostics; precision medicine; protein digest},
	keywords = {Digestion; Pathology, Molecular; Peptide Fragments; Peptide Hydrolases; Proteins; Sensitivity and Specificity; Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization; Diagnosis; Learning algorithms; Machine learning; Pattern recognition; Peptides; Software testing; peptide fragment; peptide hydrolase; protein; Artificial intelligence; Diagnostic; Machine-learning; MALDI-TOF; Molecular diagnostics; Peptide fragments; Protein digests; Proteolytic digestion; Sensitivity and specificity; Tryptic digestion; chemistry; digestion; matrix-assisted laser desorption-ionization mass spectrometry; molecular pathology; procedures; sensitivity and specificity; Biomarkers},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Hung2024534,
	author = {Hung, Chun-Chun and Wang, Meng-Hui and Lu, Shiue-Der and Kuo, Cheng-Chien},
	title = {Detection of failures in HV surge arrester using chaos pattern with deep learning neural network},
	year = {2024},
	journal = {IET Science, Measurement and Technology},
	volume = {18},
	number = {9},
	pages = {534 – 546},
	doi = {10.1049/smt2.12214},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201804333&doi=10.1049%2fsmt2.12214&partnerID=40&md5=55165b9753737e7968215713072ed5f3},
	abstract = {As a protective component of HV equipment, the primary function of a surge arrester is to mitigate the impact of surge voltages. When a surge arrester fails, the equipment it protects becomes vulnerable to damage. This study integrates chaotic systems with Convolutional Neural Networks (CNN) to diagnose faults in HV surge arresters. The Partial Discharge (PD) test was initially performed on six HV surge arrester fault models. The Discrete Wavelet Transform (DWT) was performed for filtering the PD signals. Subsequently, the Chen-Lee chaotic system converted the filtered PD signals into a dynamic error scatter diagram, creating a feature map of various fault states. This feature map was then used as the input layer to train the CNN model. The results demonstrate that the proposed CNN achieved an accuracy of 97.0%, outperforming AlexNet and traditional methods using Histograms of Oriented Gradients (HOG) combined with Support Vector Machine (SVM), Decision Tree (DT), Backpropagation Neural Network (BPNN), and K-Nearest Neighbor (KNN). This study also incorporates the LabVIEW graphic control software with a fault diagnosis system for HV surge arresters. The PD data can identify the fault type in real-time, enhancing power equipment maintenance efficiency. © 2024 The Author(s). IET Science, Measurement & Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author_keywords = {artificial intelligence; fault diagnosis; partial discharges},
	keywords = {Chaos theory; Convolutional neural networks; Deep neural networks; Discrete wavelet transforms; Fault tree analysis; Flow visualization; Multilayer neural networks; Photomapping; Software testing; Support vector machines; Chaotics; Convolutional neural network; Faults diagnosis; Feature map; HV equipment; Learning neural networks; Partial discharge signal; Primary functions; Surge arresters; Surge voltage; Decision trees},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Mahusin2024121035,
	author = {Mahusin, Noorbaiti and Sallehudin, Hasimi and Satar, Nurhizam Safie Mohd},
	title = {Malaysia Public Sector Challenges of Implementation of Artificial Intelligence (AI)},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {121035 – 121051},
	doi = {10.1109/ACCESS.2024.3448311},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201789643&doi=10.1109%2fACCESS.2024.3448311&partnerID=40&md5=8b697ba1a66be79afc02686fc1a4a1c4},
	abstract = {Artificial Intelligence (AI) has witnessed exponential growth in recent years fueled by advancements in software, hardware, and application development across diverse domains. This research utilizes a thorough analytical methodology to investigate the multifaceted concept of AI implementation in Malaysia's public sector. It underscores the transformative potential of AI to enhance operational efficiencies yet highlights critical challenges hindering its adoption. Through a detailed examination of the Malaysian public sector's AI landscape and previous research, our investigation identifies three key obstacles: a lack of qualified expertise, concerns over data privacy, and entrenched resistance to changes in organizational culture. The main findings suggest that despite these hurdles, the benefits of AI implementation, such as increased operational efficiency, streamlined processes, and improved public service delivery, are significant. Ultimately, this research concludes that Malaysia can navigate these challenges effectively with persistent efforts and strategic commitments. Embracing AI promises correctness advancements in public service quality, benefiting citizens and the nation's clarity. © 2013 IEEE.},
	author_keywords = {Artificial intelligence; challenge; framework; public sector},
	keywords = {Efficiency; Software testing; Challenge; Collaboration; Exponential growth; Framework; Malaysia; Operational efficiencies; Public sector; Software applications; Software/hardware; Task analysis; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Bahi202454,
	author = {Bahi, Anas and Gharib, Jihane and Gahi, Youssef},
	title = {Integrating Generative AI for Advancing Agile Software Development and Mitigating Project Management Challenges},
	year = {2024},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {15},
	number = {3},
	pages = {54 – 61},
	doi = {10.14569/IJACSA.2024.0150306},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189940766&doi=10.14569%2fIJACSA.2024.0150306&partnerID=40&md5=7c9a6d1d09b1dc98d3d23c6146f4f519},
	abstract = {Agile software development emphasizes iterative progress, adaptability, and stakeholder collaboration. It champions flexible planning, continuous improvement, and rapid delivery, aiming to respond swiftly to change and deliver value efficiently. Integrating Generative Artificial Intelligence (AI) into Agile software development processes presents a promising avenue for overcoming project management challenges and enhancing the efficiency and effectiveness of software development endeavors. This paper explores the potential benefits of leveraging Generative AI in Agile methodologies, aiming to streamline development workflows, foster innovation, and mitigate common project management challenges. By harnessing the capabilities of Generative AI for tasks such as code generation, automated testing, and predictive analytics, Agile teams can augment their productivity, accelerate delivery cycles, and improve the quality of software products. Additionally, Generative AI offers opportunities for enhancing collaboration, facilitating decision-making, and addressing uncertainties inherent in Agile project management. Through an in-depth analysis of the integration of Generative AI within Agile frameworks, this paper provides insights into how organizations can harness the transformative potential of AI to advance Agile software development practices and navigate the complexities of modern software projects more effectively. © (2024), (Science and Information Organization). All Rights Reserved.},
	author_keywords = {Agile software development; Artificial intelligence; software engineering},
	keywords = {Decision making; Iterative methods; Predictive analytics; Project management; Software design; Software testing; Agile Methodologies; Agile software development; Agile software development process; Codegeneration; Common projects; Continuous improvements; Development workflow; Flexible planning; Management challenges; Potential benefits; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Wagner2024,
	author = {Wagner, Michael M. and Hogan, William R. and Levander, John D. and Diller, Matthew},
	title = {Towards Machine-FAIR: Representing software and datasets to facilitate reuse and scientific discovery by machines},
	year = {2024},
	journal = {Journal of Biomedical Informatics},
	volume = {154},
	doi = {10.1016/j.jbi.2024.104647},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192218312&doi=10.1016%2fj.jbi.2024.104647&partnerID=40&md5=6c762e1f568705344e0c1f49496a8441},
	abstract = {Objective: To use software, datasets, and data formats in the domain of Infectious Disease Epidemiology as a test collection to evaluate a novel M1 use case, which we introduce in this paper. M1 is a machine that upon receipt of a new digital object of research exhaustively finds all valid compositions of it with existing objects. Method: We implemented a data-format-matching-only M1 using exhaustive search, which we refer to as M1DFM. We then ran M1DFM on the test collection and used error analysis to identify needed semantic constraints. Results: Precision of M1DFM search was 61.7%. Error analysis identified needed semantic constraints and needed changes in handling of data services. Most semantic constraints were simple, but one data format was sufficiently complex to be practically impossible to represent semantic constraints over, from which we conclude limitatively that software developers will have to meet the machines halfway by engineering software whose inputs are sufficiently simple that their semantic constraints can be represented, akin to the simple APIs of services. We summarize these insights as M1-FAIR guiding principles for composability and suggest a roadmap for progressively capable devices in the service of reuse and accelerated scientific discovery. Conclusion: Algorithmic search of digital repositories for valid workflow compositions has potential to accelerate scientific discovery but requires a scalable solution to the problem of knowledge acquisition about semantic constraints on software inputs. Additionally, practical limitations on the logical complexity of semantic constraints must be respected, which has implications for the design of software. © 2024 The Author(s)},
	author_keywords = {Automatic workflow composition; Machine-FAIR principles; Reusing digital research objects; Scientific discovery; Scientific workflows},
	keywords = {Algorithms; Databases, Factual; Humans; Machine Learning; Semantics; Software; Computer software reusability; Data handling; Parallel processing systems; Semantics; Software testing; Automatic workflow composition; Digital researches; Machine-FAIR principle; Research object; Reusing digital research object; Scientific discovery; Scientific workflows; Semantic constraints; Simple++; Workflow composition; algorithm; Article; artificial intelligence; controlled study; disease control; disease transmission; endemic disease; epidemic; FAIR principles; human; infection; machine; machine learning; metagenomics; nonhuman; phylogenetic tree construction method; population dynamics; science; scientific discovery; semantics; software; factual database; Error analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Dagli2024,
	author = {Dagli, Mert Marcel and Ghenbot, Yohannes and Ahmad, Hasan S. and Chauhan, Daksh and Turlip, Ryan and Wang, Patrick and Welch, William C. and Ozturk, Ali K. and Yoon, Jang W},
	title = {Development and validation of a novel AI framework using NLP with LLM integration for relevant clinical data extraction through automated chart review},
	year = {2024},
	journal = {Scientific Reports},
	volume = {14},
	number = {1},
	doi = {10.1038/s41598-024-77535-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208603126&doi=10.1038%2fs41598-024-77535-y&partnerID=40&md5=9ebe7483d59f1720f8bb598da0d75c44},
	abstract = {The accurate extraction of surgical data from electronic health records (EHRs), particularly operative notes through manual chart review (MCR), is complex, crucial, and time-intensive, limited by human error due to fatigue and the level of training. This study aimed to develop and validate a novel Natural Language Processing (NLP) algorithm integrated with a Large Language Model (LLM; GPT4-Turbo) to automate the extraction of spinal surgery data from EHRs. The algorithm employed a two-stage approach. Initially, a rule-based NLP framework reviewed and classified candidate segments from the text, preserving their reference segments. These segments were then verified in the second stage through the LLM. The primary outcomes of this study were the accurate extraction of surgical data, including the type of surgery, levels operated, number of disks removed, and presence of intraoperative incidental durotomies. Secondary objectives explored time efficiency, tokenization lengths, and costs. The performance of the algorithm was assessed across two validation databases, analyzing metrics such as accuracy, sensitivity, discrimination, F1-score, and precision, with 95% confidence intervals calculated using percentile-based bootstrapping. The NLP + LLM algorithm markedly outperformed all performance metrics, demonstrating significant improvements in time and cost efficiency. These results suggest the potential for widespread adoption of this technology. © The Author(s) 2024.},
	author_keywords = {Artificial intelligence; Data science; Electronic health records; Humans; Natural language processing; Quality improvement},
	keywords = {Algorithms; Artificial Intelligence; Electronic Health Records; Humans; Natural Language Processing; algorithm; artificial intelligence; electronic health record; human; natural language processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Gong2023213,
	author = {Gong, KeXin and Song, Xiangmei and Wang, Na and Wang, Chunyang and Zhu, Huijuan},
	title = {SCGformer: Smart contract vulnerability detection based on control flow graph and transformer},
	year = {2023},
	journal = {IET Blockchain},
	volume = {3},
	number = {4},
	pages = {213 – 221},
	doi = {10.1049/blc2.12046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189502078&doi=10.1049%2fblc2.12046&partnerID=40&md5=a741a6830a4e241b802d98e388d0f060},
	abstract = {The security of smart contract has always been one of the significant problems in blockchain. As shown in previous studies, vulnerabilities in smart contracts can lead to unpredictable losses. With the rapid growth of the number of smart contracts, more and more data driven detection technologies based on machine learning have been proposed. However, some state-of-the-art approaches mainly rely on the source code of smart contract. These methods are limited by the openness of the source code and the version of the programming language. To address this problem, we propose a novel vulnerability detection method based on transformer by constructing the control flow graph (CFG) of smart contracts operation codes (opcodes), which shields the difference of various versions of program language. Extensive experiments are conducted to evaluate the effectiveness of the proposed method on the authors' own collected dataset. The experimental results show that the proposed method achieves 94.36% accuracy in vulnerability detection, which performs better than other state-of-the-art methods. © 2023 The Authors. IET Blockchain published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author_keywords = {artificial intelligence; blockchain applications and digital technology; computer network security; contracts; DAPP; Ethereum},
	keywords = {Blockchain; Codes (symbols); Ethereum; Flow graphs; Graphic methods; Network security; Application technologies; Block-chain; Blockchain application and digital technology; Computer network security; Control-flow graphs; DAPP; Digital technologies; Rapid growth; Source codes; Vulnerability detection; Smart contract},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Guo20243311,
	author = {Guo, Yuejun and Bettaieb, Seifeddine and Casino, Fran},
	title = {A comprehensive analysis on software vulnerability detection datasets: trends, challenges, and road ahead},
	year = {2024},
	journal = {International Journal of Information Security},
	volume = {23},
	number = {5},
	pages = {3311 – 3327},
	doi = {10.1007/s10207-024-00888-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199262187&doi=10.1007%2fs10207-024-00888-y&partnerID=40&md5=d083339eb0db7f1d1e7869acebdd072e},
	abstract = {As society’s dependence on information and communication systems (ICTs) grows, so does the necessity of guaranteeing the proper functioning and use of such systems. In this context, it is critical to enhance the security and robustness of the DevSecOps pipeline through timely vulnerability detection. Usually, AI-based models enable desirable features such as automation, performance, and efficacy. However, the quality of such models highly depends on the datasets used during the training stage. The latter encompasses a series of challenges yet to be solved, such as access to extensive labelled datasets with specific properties, such as well-represented and balanced samples. This article explores the current state of practice of software vulnerability datasets and provides a classification of the main challenges and issues. After an extensive analysis, it describes a set of guidelines and desirable features that datasets should guarantee. The latter is applied to create a new dataset, which fulfils these properties, along with a descriptive comparison with the state of the art. Finally, a discussion on how to foster good practices among researchers and practitioners sets the ground for further research and continued improvement within this critical domain. © The Author(s) 2024.},
	author_keywords = {Benchmarking; Datasets; DevSecOps; Software vulnerability detection},
	keywords = {Artificial intelligence; Comprehensive analysis; Dataset; Desirable features; Devsecop; Information and communication systems; Labeled dataset; Performance; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Classification (of information)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Li2024,
	author = {Li, Xiang and Gao, Zhaoyang and Liao, Hong},
	title = {An empirical investigation of college students’ acceptance of translation technologies},
	year = {2024},
	journal = {PLoS ONE},
	volume = {19},
	number = {2 February},
	doi = {10.1371/journal.pone.0297297},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186100790&doi=10.1371%2fjournal.pone.0297297&partnerID=40&md5=5f035f65d8a5e75e65dd2cfed73ed100},
	abstract = {With the advancement of information technology and artificial intelligence, translation technologies have seen rapid development in language services and increasing integration in higher education. However, research on factors affecting students’ acceptance of these technologies remains limited. This study intends to formulate and test an extended Technology Acceptance Model (TAM) incorporating computer self-efficacy and perceived enjoyment to investigate students’ adoption of translation technologies. A questionnaire survey was conducted among 370 college students in China experienced with using translation technologies. The results from the structural equation modeling demonstrated a positive prediction on perceived ease of use and enjoyment from computer self-efficacy. Perceived enjoyment increased perceived ease of use and attitudes. Perceived ease of use positively influenced perceived usefulness and attitudes. Finally, attitudes positively predicted greater behavioral intentions to use translation technologies. However, computer self-efficacy was identified to have no significant effect on perceived usefulness. The study makes significant theoretical contributions by expanding TAM and offering practical guidance to improve students’ acceptance of translation technologies in tertiary education. © 2024 Public Library of Science. All rights reserved.},
	keywords = {Artificial Intelligence; Attitude; Humans; Intention; Students; Technology; Article; artificial intelligence; college student; e-learning; human; prediction; questionnaire; self concept; structural equation modeling; adult; article; controlled study; digital technology; female; information technology; male; open access publishing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Kaur2023,
	author = {Kaur, Ramanpreet and Gabrijelčič, Dušan and Klobučar, Tomaž},
	title = {Artificial intelligence for cybersecurity: Literature review and future research directions},
	year = {2023},
	journal = {Information Fusion},
	volume = {97},
	doi = {10.1016/j.inffus.2023.101804},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152146620&doi=10.1016%2fj.inffus.2023.101804&partnerID=40&md5=336dfa4faa8db777fb98d007f8be8db7},
	abstract = {Artificial intelligence (AI) is a powerful technology that helps cybersecurity teams automate repetitive tasks, accelerate threat detection and response, and improve the accuracy of their actions to strengthen the security posture against various security issues and cyberattacks. This article presents a systematic literature review and a detailed analysis of AI use cases for cybersecurity provisioning. The review resulted in 2395 studies, of which 236 were identified as primary. This article classifies the identified AI use cases based on a NIST cybersecurity framework using a thematic analysis approach. This classification framework will provide readers with a comprehensive overview of the potential of AI to improve cybersecurity in different contexts. The review also identifies future research opportunities in emerging cybersecurity application areas, advanced AI methods, data representation, and the development of new infrastructures for the successful adoption of AI-based cybersecurity in today's era of digital transformation and polycrisis. © 2023 The Author(s)},
	author_keywords = {Cyberattacks; Detection; Identify; Learning; Protection; Recovery; Response; Taxonomy},
	keywords = {Artificial intelligence; Metadata; Cyber security; Cyber-attacks; Detection; Future research directions; Identify; Learning; Literature reviews; Protection; Repetitive task; Response; Cybersecurity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 221; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Gomes2023509,
	author = {Gomes, Juliana Carneiro and de Freitas Barbosa, Valter Augusto and de Santana, Maíra Araújo and de Lima, Clarisse Lins and Calado, Raquel Bezerra and Júnior, Cláudio Roberto Bertoldo and de Almeida Albuquerque, Jeniffer Emidio and de Souza, Rodrigo Gomes and de Araújo, Ricardo Juarez Escorel and Moreno, Giselle Machado Magalhães and Soares, Luiz Alberto Lira and Júnior, Luiz Alberto Reis Mattos and de Souza, Ricardo Emmanuel and dos Santos, Wellington Pinheiro},
	title = {Rapid protocols to support COVID-19 clinical diagnosis based on hematological parameters},
	year = {2023},
	journal = {Research on Biomedical Engineering},
	volume = {39},
	number = {3},
	pages = {509 – 539},
	doi = {10.1007/s42600-023-00286-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160827620&doi=10.1007%2fs42600-023-00286-8&partnerID=40&md5=672b90808909670404c205eea1962e8a},
	abstract = {Purpose: In December 2019, the Covid-19 pandemic began in the world. To reduce mortality, in addiction to mass vaccination, it is necessary to massify and accelerate clinical diagnosis, as well as creating new ways of monitoring patients that can help in the construction of specific treatments for the disease. Objective: In this work, we propose rapid protocols for clinical diagnosis of COVID-19 through the automatic analysis of hematological parameters using evolutionary computing and machine learning. These hematological parameters are obtained from blood tests common in clinical practice. Method: We investigated the best classifier architectures. Then, we applied the particle swarm optimization algorithm (PSO) to select the most relevant attributes: serum glucose, troponin, partial thromboplastin time, ferritin, D-dimer, lactic dehydrogenase, and indirect bilirubin. Then, we assessed again the best classifier architectures, but now using the reduced set of features. Finally, we used decision trees to build four rapid protocols for Covid-19 clinical diagnosis by assessing the impact of each selected feature. The proposed system was used to support clinical diagnosis and assessment of disease severity in patients admitted to intensive and semi-intensive care units as a case study in the city of Paudalho, Brazil. Results: We developed a web system for Covid-19 diagnosis support. Using a 100-tree random forest, we obtained results for accuracy, sensitivity, and specificity superior to 99%. After feature selection, results were similar. The four empirical clinical protocols returned accuracies, sensitivities and specificities superior to 98%. Conclusion: By using a reduced set of hematological parameters common in clinical practice, it was possible to achieve results of accuracy, sensitivity, and specificity comparable to those obtained with RT-PCR. It was also possible to automatically generate clinical decision protocols, allowing relatively accurate clinical diagnosis even without the aid of the web decision support system. © 2023, The Author(s), under exclusive licence to The Brazilian Society of Biomedical Engineering.},
	author_keywords = {Clinical diagnosis support; Computer-aided diagnosis; Covid-19; Covid-19 rapid protocols; Hematological parameters; Software-based rapid test},
	keywords = {Artificial intelligence; Computer aided diagnosis; COVID-19; Decision support systems; Decision trees; Internet protocols; Particle swarm optimization (PSO); Software testing; Clinical diagnose support; Clinical diagnosis; Clinical practices; Covid-19; Covid-19 rapid protocol; Diagnosis support; Hematological parameters; Rapid test; Sensitivity and specificity; Software-based rapid test; Intensive care units},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Carabantes2023,
	author = {Carabantes, David and González-Geraldo, José L. and Jover, Gonzalo},
	title = {ChatGPT could be the reviewer of your next scientific paper. Evidence on the limits of AI-assisted academic reviews},
	year = {2023},
	journal = {Profesional de la Informacion},
	volume = {32},
	number = {5},
	doi = {10.3145/epi.2023.sep.16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174049856&doi=10.3145%2fepi.2023.sep.16&partnerID=40&md5=0b853d589db0bedade2aff7d13789606},
	abstract = {The irruption of artificial intelligence (AI) in all areas of our lives is a reality to which the university, as an institution of higher education, must respond prudently, but also with no hesitation. This paper discusses the potential that resources based on AI presents as potential reviewers of scientific articles in a hypothetical peer review of already published articles. Using different models (GPT-3.5 and GPT-4) and platforms (ChatPDF and Bing), we obtained three full reviews, both qualitative and quantitative, for each of the five articles examined, thus being able to delineate and contrast the results of all of them in terms of the human reviews that these same articles received at the time. The evidence found highlights the extent to which we can and should rely on generative language models to support our decisions as qualified experts in our field. Further-more, the results also corroborate the hallucinations inherent in these models while pointing out one of their current major shortcomings: the context window limit. On the other hand, the study also points out the inherent benefits of a model that is in a clear expansion phase, providing a detailed view of the potential and limitations that these models offer as possible assistants to the review of scientific articles, a key process in the communication and dissemination of academic research. © 2023, El Profesional de la Informacion. All rights reserved.},
	author_keywords = {Academic publication; Academic review; AI; AI-assisted review; Artificial intelligence; Bing; ChatGPT; ChatPDF; Contextual window; Generative artificial intelligence; Peer review; Scientific communication},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Hadj Abdelkader2023,
	author = {Hadj Abdelkader, Oussama and Bouzebiba, Hadjer and Pena, Danilo and Aguiar, António Pedro},
	title = {Energy-Efficient IoT-Based Light Control System in Smart Indoor Agriculture},
	year = {2023},
	journal = {Sensors},
	volume = {23},
	number = {18},
	doi = {10.3390/s23187670},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172718133&doi=10.3390%2fs23187670&partnerID=40&md5=c01939b352ebfd555555da60b3fa2df7},
	abstract = {Indoor agriculture is emerging as a promising approach for increasing the efficiency and sustainability of agri-food production processes. It is currently evolving from a small-scale horticultural practice to a large-scale industry as a response to the increasing demand. This led to the appearance of plant factories where agri-food production is automated and continuous and the plant environment is fully controlled. While plant factories improve the productivity and sustainability of the process, they suffer from high energy consumption and the difficulty of providing the ideal environment for plants. As a small step to address these limitations, in this article we propose to use internet of things (IoT) technologies and automatic control algorithms to construct an energy-efficient remote control architecture for grow lights monitoring in indoor farming. The proposed architecture consists of using a master–slave device configuration in which the slave devices are used to control the local light conditions in growth chambers while the master device is used to monitor the plant factory through wireless communication with the slave devices. The devices all together make a 6LoWPAN network in which the RPL protocol is used to manage data transfer. This allows for the precise and centralized control of the growth conditions and the real-time monitoring of plants. The proposed control architecture can be associated with a decision support system to improve yields and quality at low costs. The developed method is evaluated in emulation software (Contiki-NG v4.7),its scalability to the case of large-scale production facilities is tested, and the obtained results are presented and discussed. The proposed approach is promising in dealing with control, cost, and scalability issues and can contribute to making smart indoor agriculture more effective and sustainable. © 2023 by the authors.},
	author_keywords = {controlled environment agriculture; distributed control; internet of things; remote control; smart indoor farming; wireless sensor network},
	keywords = {Agricultural technology; Artificial intelligence; Automation; Data transfer; Decision support systems; Energy efficiency; Energy utilization; Information management; Internet of things; Network architecture; Quality control; Remote control; Scalability; Software testing; Sustainable development; Control architecture; Controlled environment agricultures; Distributed-control; Energy efficient; Food production; Light control systems; Plant factory; Production process; Slave device; Smart indoor farming; Wireless sensor networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Šedlbauer20241526,
	author = {Šedlbauer, Josef and Činčera, Jan and Slavík, Martin and Hartlová, Adéla},
	title = {Students' reflections on their experience with ChatGPT},
	year = {2024},
	journal = {Journal of Computer Assisted Learning},
	volume = {40},
	number = {4},
	pages = {1526 – 1534},
	doi = {10.1111/jcal.12967},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187478918&doi=10.1111%2fjcal.12967&partnerID=40&md5=e88185a73a8578f428f12ff45d609678},
	abstract = {Background: The emergence of Generative Artificial Intelligence has brought a number of ethical and practical issues to higher education. Solid experimental evidence is yet inadequate to set the functional rules for the new technology. Objectives: The objective of this study is to analyse the experience of undergraduate students’ interaction with ChatGPT and contribute to identifying the problems arising from the widespread use of artificial intelligence. Methods: Junior university students (N = 25) were assigned the task of working on their seminar essays with the aid of ChatGPT. Most students were novices with this tool (the study was conducted in the spring of 2023). Their essays were analysed qualitatively, according to the principles of the general inductive approach. Results and Conclusions: The initial attitudes towards artificial intelligence were almost equally distributed from enthusiastic to indifferent and cautious, with one student refusing to interact with the chatbot on ideological grounds. After the first experience, most of the students declared themselves adopters of the new technology. We have found some evidence for enhancing critical thinking competence when using ChatGPT, as well as examples of unquestioned reliance on its outputs. The tendency to personification of the chatbot was apparent in the students' essays. Implications: The findings show how easily the students embrace artificial intelligence and suggest a failure of any attempts for its strict regulation. This, on the other hand, underlines the need for emphasis on personal and research-oriented approaches in teaching and learning. © 2024 The Authors. Journal of Computer Assisted Learning published by John Wiley & Sons Ltd.},
	author_keywords = {artificial intelligence; ChatGPT; critical thinking; education; sustainability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Ghani2024114974,
	author = {Ghani, Israr and Wan Kadir, Wan Mohd Nasir and Arbain, Adila Firdaus and Ghani, Imran},
	title = {A Detection-Based Multi-Objective Test Case Selection Algorithm to Improve Time and Efficiency in Regression Testing},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {114974 – 114994},
	doi = {10.1109/ACCESS.2024.3435678},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200203676&doi=10.1109%2fACCESS.2024.3435678&partnerID=40&md5=d2f7bb5df78bc6ee18afeb8590128afd},
	abstract = {Regression testing is carried out to ensure that changes or enhancements are not impacting previous working software. Deciding how much retesting is required after modifications, bug fixes or before product deployments are difficult. Therefore, Test Case Selection (TCS) select the satisfactory subset of modified test cases from already executed test suites. The testing primary concerns in TCS for regression testing are efficiency (i.e., coverage, fault detection ability, redundancy) and time. The first challenge in TCS concerns the efficiency of multi-objective test case selection. The second challenge is to improve the execution time to detect the changes in a test suite, which makes it impractical to use these efficiency measures as a single goal for TCS. To overcome these challenges, there is a need to introduce an efficient detection-based multi-objective framework to improve the Time and efficiency of TCS. A multi-objective advanced and efficient regression test case selection (ARTeCS) framework is devised to improve the time performance and efficiency of a given TCS objective relative to the other TCS approaches. An algorithm to detect the changes in test cases using multiple TCS objectives. This comparison found that the enhanced ARTeCS algorithm improves redundancy efficiency by 44.02%. The selection technique showed ARTeCS improved the modified change detection by 43.00%, whereas the Hybrid Whale Optimization Algorithm (HWOA) stated 23% and ACO showed 33% only for selected test cases. Regarding average for fault detection, ACO scores 21%, HWOA scores 11%, and ARTeCS scores 31.08% with total execution times of 12, 21 and 09 seconds, respectively. In conclusion, the multiple-objective ARTeCS framework with four test suite selection parameters is more efficient than the existing multi-objective selection framework. © 2013 IEEE.},
	author_keywords = {multi-objective approach in TCS; regression testing; Software testing; TCS algorithm; TCS framework; test case selection},
	keywords = {Ability testing; Artificial intelligence; Fault detection; Redundancy; Regression analysis; Software testing; Code; Faults detection; Multi objective; Multi-objective approach in test case selection; Objective approaches; Regression testing; Selection algorithm; Selection framework; Software; Software algorithms; Software testings; Test case selection; Test case selection algorithm; Test case selection framework; Efficiency},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Abu-Abed2023,
	author = {Abu-Abed, Fares and Zhironkin, Sergey},
	title = {New Game Artificial Intelligence Tools for Virtual Mine on Unreal Engine},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {10},
	doi = {10.3390/app13106339},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160819964&doi=10.3390%2fapp13106339&partnerID=40&md5=4ff8c691abcb15f35c60c7b76eb8342b},
	abstract = {Currently, the gamification of virtual reality for training miners, especially for emergencies, and designing the extraction of minerals in difficult technological conditions has been embodied in the Virtual Mine software and hardware. From a software development point of view, Virtual Mine is indistinguishable from other virtual reality games, and this offers a chance to use the potential of rapidly developing game software in mining, including engines, 3D modeling tools, audio editors, etc., to solve a wide range of game development tasks. The chosen direction will optimize the work of developers by providing a tool for developing game artificial intelligence to solve problems that require implementing the behavior of game agents without using a rigidly defined choice of scenarios or chains of these scenarios. The aim of the work is to expand the possibilities of working with game artificial intelligence on the Unreal Engine game engine to make it more functional. As a result, a tool has been obtained that can be used to optimize the time and improve the quality of the development of game artificial intelligence for Virtual Mine using flexible development approaches. The asset editor was developed, application modes and their working tabs were defined, and a graphical node system for the behavioral graph editor was created. A system for executing a behavioral graph is given; algorithms for its operation and features for executing nodes of a behavioral graph are presented. © 2023 by the authors.},
	author_keywords = {3D modeling tools; artificial intelligence; behavior of game agents; game engines; IT industry; unreal engine; virtual mine; virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Betz2023783,
	author = {Betz, Johannes and Betz, Tobias and Fent, Felix and Geisslinger, Maximilian and Heilmeier, Alexander and Hermansdorfer, Leonhard and Herrmann, Thomas and Huch, Sebastian and Karle, Phillip and Lienkamp, Markus and Lohmann, Boris and Nobis, Felix and Ögretmen, Levent and Rowold, Matthias and Sauerbeck, Florian and Stahl, Tim and Trauth, Rainer and Werner, Frederik and Wischnewski, Alexander},
	title = {TUM autonomous motorsport: An autonomous racing software for the Indy Autonomous Challenge},
	year = {2023},
	journal = {Journal of Field Robotics},
	volume = {40},
	number = {4},
	pages = {783 – 809},
	doi = {10.1002/rob.22153},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146358202&doi=10.1002%2frob.22153&partnerID=40&md5=18e28675d0cc5db63f1b9fd4f0862ccb},
	abstract = {For decades, motorsport has been an incubator for innovations in the automotive sector and brought forth systems, like, disk brakes or rearview mirrors. Autonomous racing series such as Roborace, F1Tenth, or the Indy Autonomous Challenge (IAC) are envisioned as playing a similar role within the autonomous vehicle sector, serving as a proving ground for new technology at the limits of the autonomous systems capabilities. This paper outlines the software stack and approach of the TUM Autonomous Motorsport team for their participation in the IAC, which holds two competitions: A single-vehicle competition on the Indianapolis Motor Speedway and a passing competition at the Las Vegas Motor Speedway. Nine university teams used an identical vehicle platform: A modified Indy Lights chassis equipped with sensors, a computing platform, and actuators. All the teams developed different algorithms for object detection, localization, planning, prediction, and control of the race cars. The team from Technical University of Munich (TUM) placed first in Indianapolis and secured second place in Las Vegas. During the final of the passing competition, the TUM team reached speeds and accelerations close to the limit of the vehicle, peaking at around (Formula presented.) and (Formula presented.). This paper will present details of the vehicle hardware platform, the developed algorithms, and the workflow to test and enhance the software applied during the 2-year project. We derive deep insights into the autonomous vehicle's behavior at high speed and high acceleration by providing a detailed competition analysis. On the basis of this, we deduce a list of lessons learned and provide insights on promising areas of future work based on the real-world evaluation of the displayed concepts. © 2023 The Authors. Journal of Field Robotics published by Wiley Periodicals LLC.},
	author_keywords = {artificial intelligence; autonomous robot; dynamic obstacle avoidance; unmanned ground vehicle; vehicle robot},
	keywords = {Automotive industry; Intelligent robots; Object detection; Software testing; Automotive sector; Autonomous Vehicles; Dynamic obstacle avoidance; Indianapolis; Las Vegas; Motor-sport; Motorsports; Technical universities; Unmanned ground vehicle; Vehicle robots; Autonomous vehicles},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Soldani2025809,
	author = {Soldani, Jacopo and Forti, Stefano and Roveroni, Luca and Brogi, Antonio},
	title = {Explaining Microservices' Cascading Failures From Their Logs},
	year = {2025},
	journal = {Software - Practice and Experience},
	volume = {55},
	number = {5},
	pages = {809 – 828},
	doi = {10.1002/spe.3400},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002567219&doi=10.1002%2fspe.3400&partnerID=40&md5=96a08ec5d16a7e55cc70bc303058277d},
	abstract = {Context: Identifying the possible root causes of observed failures is crucial in microservice applications, as much as explaining how such possible root failures propagated across the microservices forming an application. This can indeed help pick countermeasures avoiding observed failures to happen again, e.g., by introducing circuit breakers or bulkheads avoiding the root failures to propagate and cause those observed. Objective: This paper aims at enabling to explain observed failures in microservice applications, either searching for all possible cascading failures or focusing only on those starting in a known root cause. Method: We propose a log-based root cause analysis technique, which declaratively determines the cascading failures that possibly caused an observed failure. We also enable exploiting our proposed technique in practice, by introducing a logging methodology to instrument applications to log their failures and service interactions, and by enabling to analyse such logs through yRCA, a prototype implementation of our proposed root cause analysis technique. Results: The practical usability of our proposed technique is assessed by means of a case study and controlled experiments. The case study shows the low effort for instrumenting a third-party application to produce the logs needed by our technique and its effectiveness in explaining injected failures. The controlled experiments further assess our technique's effectiveness and performances in explaining failures obtained with an existing chaos testbed. Conclusion: Our proposed technique can help to identify the cascading failures that possibly caused an observed failure in a microservice application. It can be used to determine all possible cascading failures, or to explain how cascading failures propagated from a known root cause (e.g., identified with some other existing root cause analyser). © 2024 The Author(s). Software: Practice and Experience published by John Wiley & Sons Ltd.},
	author_keywords = {declarative reasoning; explainability; microservices; root cause analysis cascading failures},
	keywords = {Artificial intelligence; Analysis techniques; Cascading failures; Case-studies; Controlled experiment; Declarative reasoning; Explainability; Microservice; Root cause; Root cause analysis; Root cause analyze cascading failure; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Giovannardi2024,
	author = {Giovannardi, Emanuele and Brusa, Alessandro and Petrone, Boris and Cavina, Nicolò and Tonelli, Roberto and Kitsopanidis, Ioannis},
	title = {AI-Based Virtual Sensing of Gaseous Pollutant Emissions at the Tailpipe of a High-Performance Vehicle},
	year = {2024},
	journal = {SAE International Journal of Engines},
	volume = {17},
	number = {4},
	doi = {10.4271/03-17-04-0029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184777925&doi=10.4271%2f03-17-04-0029&partnerID=40&md5=ae7248d8afcf3861a668d8ecbd9d1b4c},
	abstract = {This scientific publication presents the application of artificial intelligence (AI) techniques as a virtual sensor for tailpipe emissions of CO, NOx, and HC in a high-performance vehicle. The study aims to address critical challenges faced in real industrial applications, including signal alignment and signal dynamics management. A comprehensive pre-processing pipeline is proposed to tackle these issues, and a light gradient-boosting machine (LightGBM) model is employed to estimate emissions during real driving cycles. The research compares two modeling approaches: one involving a unique "direct model"and another using a "two-stage model"which leverages distinct models for the engine and the aftertreatment. The findings suggest that the direct model strikes the best balance between simplicity and accuracy. Furthermore, the study investigates two sensor setups: a standard configuration and an optimized one, which incorporates an additional lambda probe in the exhaust line after the main catalyst. The results indicate a significant enhancement in performance for NOx and CO estimations with the introduction of the third lambda probe, while HC results remain relatively unchanged. Additionally, the AI model is tested on two different electronic control unit (ECU) software calibrations, yielding excellent results in both cases. This suggests that machine learning models are robust to control software variation and can be used to optimize software calibrations in a virtual environment, reducing the reliance on extensive experimental testing. Moreover, the AI model's performance demonstrates compatibility with real-time implementation. In conclusion, this work establishes the viability and efficiency of AI techniques in accurately estimating tailpipe emissions from an engine in an industrial context. The study showcases the potential for AI to contribute to emission estimation and optimization processes, offering a promising pathway for an innovative industrial practice. © 2024 SAE International.},
	author_keywords = {Artificial intelligence; Emission prediction; Industrial application; Machine learning; Tailpipe emissions; Virtual sensors},
	keywords = {Control systems; E-learning; Engines; Machine learning; Nitrogen oxides; Probes; Virtual reality; Artificial intelligence techniques; Direct modelling; Emissions prediction; High-performance vehicles; Intelligence models; Lambda probe; Machine-learning; Software calibration; Tailpipe emission; Virtual sensor; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Green Open Access}
}

@ARTICLE{Franzoni2024,
	author = {Franzoni, Valentina and Tagliente, Silvia and Milani, Alfredo},
	title = {Generative Models for Source Code: Fine-Tuning Techniques for Structured Pattern Learning},
	year = {2024},
	journal = {Technologies},
	volume = {12},
	number = {11},
	doi = {10.3390/technologies12110219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210591837&doi=10.3390%2ftechnologies12110219&partnerID=40&md5=02427ea2b1d7bfccee8920695ed4ebfb},
	abstract = {This study addresses the problem of how to automatically generate source code that is not only functional, but also well-structured, readable, and maintainable. Existing generative models for source code often produce functional code, but they lack consistency in structure and adherence to coding standards, essential for integration into existing application development projects and long-term software maintenance. By training the model on specific code structures, including a dataset with Italian annotations, the proposed methodology ensures that the generated code is compliant with both the functional requirements and the pre-defined coding standards. The methodology proposed in this study applies transfer learning techniques on the DeepSeek Coder model, to refine pre-trained models to generate code that integrates additional structuring constraints. By training the model on specific code structures, including a dataset with Italian comments, the proposed methodology ensures that the generated code meets both functional requirements and coding structure. Experimental results, evaluated using the perplexity metric, demonstrate the effectiveness of the proposed approach, which impacts the goals of reducing errors, and ultimately improves software development quality. © 2024 by the authors.},
	author_keywords = {artificial intelligence; automatic code generation; generative models; software development; source code quality; transfer learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Layman202427,
	author = {Layman, Lucas and Vetter, Ron},
	title = {Generative Artificial Intelligence and the Future of Software Testing},
	year = {2024},
	journal = {Computer},
	volume = {57},
	number = {1},
	pages = {27 – 32},
	doi = {10.1109/MC.2023.3306998},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183077943&doi=10.1109%2fMC.2023.3306998&partnerID=40&md5=b75bacfb63d6710200a262db6e818d28},
	abstract = {This virtual roundtable focuses on applications of generative artificial intelligence (GenAI) to software testing with four leading experts from the field. Our experts reflect on transforming the work of software testing with GenAI, its impact on quality assurance engineers, and privacy concerns. COMPUTER0018-9162/24 ©2024 IEEE.},
	keywords = {Application programs; Artificial intelligence; Quality assurance; Leading experts; Privacy concerns; Software testings; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Bronze Open Access}
}

@ARTICLE{Apruzzese2023,
	author = {Apruzzese, Giovanni and Laskov, Pavel and Montes De Oca, Edgardo and Mallouli, Wissam and Brdalo Rapa, Luis and Grammatopoulos, Athanasios Vasileios and Di Franco, Fabio},
	title = {The Role of Machine Learning in Cybersecurity},
	year = {2023},
	journal = {Digital Threats: Research and Practice},
	volume = {4},
	number = {1},
	doi = {10.1145/3545574},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152936286&doi=10.1145%2f3545574&partnerID=40&md5=c29a63bd381c1b0b24547560ab9d0d13},
	abstract = {Machine Learning (ML) represents a pivotal technology for current and future information systems, and many domains already leverage the capabilities of ML. However, deployment of ML in cybersecurity is still at an early stage, revealing a significant discrepancy between research and practice. Such a discrepancy has its root cause in the current state of the art, which does not allow us to identify the role of ML in cybersecurity. The full potential of ML will never be unleashed unless its pros and cons are understood by a broad audience.This article is the first attempt to provide a holistic understanding of the role of ML in the entire cybersecurity domain - to any potential reader with an interest in this topic. We highlight the advantages of ML with respect to human-driven detection methods, as well as the additional tasks that can be addressed by ML in cybersecurity. Moreover, we elucidate various intrinsic problems affecting real ML deployments in cybersecurity. Finally, we present how various stakeholders can contribute to future developments of ML in cybersecurity, which is essential for further progress in this field. Our contributions are complemented with two real case studies describing industrial applications of ML as defense against cyber-threats.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author_keywords = {Additional Key Words and PhrasesCybersecurity; artificial intelligence; incident detection; machine learning},
	keywords = {Cybersecurity; 'current; Additional key word and phrasescybersecurity; Cyber security; Detection methods; Future information systems; Incident detection; Key words; Machine-learning; Root cause; State of the art; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 97; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Mauser2025,
	author = {Mauser, Lucas and Wagner, Stefan},
	title = {Centralization potential of automotive E/E architectures},
	year = {2025},
	journal = {Journal of Systems and Software},
	volume = {219},
	doi = {10.1016/j.jss.2024.112220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204492274&doi=10.1016%2fj.jss.2024.112220&partnerID=40&md5=9c7d84ca0560319405dcb0152469add4},
	abstract = {Current automotive E/E architectures are subject to significant transformations: Computing-power-intensive advanced driver-assistance systems, bandwidth-hungry infotainment systems, the connection of the vehicle with the internet and the consequential need for cyber-security drives the centralization of E/E architectures. A centralized architecture is often seen as a key enabler to master those challenges. Available research focuses mostly on the different types of E/E architectures and contrasts their advantages and disadvantages. There is a research gap on guidelines for system designers and function developers to analyze the potential of their systems for centralization. The present paper aims to quantify centralization potential reviewing relevant literature and conducting qualitative interviews with industry practitioners. In literature, we identified seven key automotive system properties reaching limitations in current automotive architectures: busload, functional safety, computing power, feature dependencies, development and maintenance costs, error rate, modularity and flexibility. These properties serve as quantitative evaluation criteria to estimate whether centralization would enhance overall system performance. In the interviews, we have validated centralization and its fundament – the conceptual systems engineering – as capabilities to mitigate these limitations. By focusing on practical insights and lessons learned, this research provides system designers with actionable guidance to optimize their systems, addressing the outlined challenges while avoiding monolithic architecture. This paper bridges the gap between theoretical research and practical application, offering valuable takeaways for practitioners. © 2024 Elsevier Inc.},
	author_keywords = {Automotive E/E architectures; Automotive system properties; Centralization; Feature dependencies; Function distribution; Software-defined vehicles; Systems engineering},
	keywords = {Artificial intelligence; Cognitive systems; Software architecture; Systems analysis; 'current; Automotive E/E architecture; Automotive system property; Automotive Systems; Automotives; Centralisation; Computing power; Feature dependency; Software-defined vehicle; System property; Advanced driver assistance systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access}
}

@ARTICLE{Jamarani2024,
	author = {Jamarani, Amirhossein and Haddadi, Saeid and Sarvizadeh, Raheleh and Haghi Kashani, Mostafa and Akbari, Mohammad and Moradi, Saeed},
	title = {Big data and predictive analytics: A sytematic review of applications},
	year = {2024},
	journal = {Artificial Intelligence Review},
	volume = {57},
	number = {7},
	doi = {10.1007/s10462-024-10811-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196094770&doi=10.1007%2fs10462-024-10811-5&partnerID=40&md5=e10e935d6e03abb583c8fec8722ef35e},
	abstract = {Big data involves processing vast amounts of data using advanced techniques. Its potential is harnessed for predictive analytics, a sophisticated branch that anticipates unknown future events by discerning patterns observed in historical data. Various techniques obtained from modeling, data mining, statistics, artificial intelligence, and machine learning are employed to analyze available history to extract discriminative patterns for predictors. This study aims to analyze the main research approaches on Big Data Predictive Analytics (BDPA) based on very up-to-date published articles from 2014 to 2023. In this article, we fully concentrate on predictive analytics using big data mining techniques, where we perform a Systematic Literature Review (SLR) by reviewing 109 articles. Based on the application and content of current studies, we introduce taxonomy including seven major categories of industrial, e-commerce, smart healthcare, smart agriculture, smart city, Information and Communications Technologies (ICT), and weather. The benefits and weaknesses of each approach, potentially important changes, and open issues, in addition to future paths, are discussed. The compiled SLR not only extends on BDPA’s strengths, open issues, and future works but also detects the need for optimizing the insufficient metrics in big data applications, such as timeliness, accuracy, and scalability, which would enable organizations to apply big data to shift from retrospective analytics to prospective predictive if fulfilled. © The Author(s) 2024.},
	author_keywords = {Big data; Big data applications; Predictive analytics; Systematic review},
	keywords = {Artificial intelligence; Data mining; Predictive analytics; 'current; Artificial intelligence learning; Big data applications; Data-mining techniques; Historical data; Machine-learning; Modeling data; Research approach; Systematic literature review; Systematic Review; Big data},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Aoujil2023139367,
	author = {Aoujil, Zakaria and Hanine, Mohamed and Flores, Emmanuel Soriano and Samad, Md. Abdus and Ashraf, Imran},
	title = {Artificial Intelligence and Behavioral Economics: A Bibliographic Analysis of Research Field},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {139367 – 139394},
	doi = {10.1109/ACCESS.2023.3339778},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179830154&doi=10.1109%2fACCESS.2023.3339778&partnerID=40&md5=31fcc288bb7719c3e50ee6a5fcaf2dae},
	abstract = {Behavioral economics and artificial intelligence (AI) have been two rapidly growing fields of research over the past few years. While behavioral economics aims to combine concepts from psychology, sociology, and neuroscience with classical economic thoughts to understand human decision-making processes in the complex economic environment, AI on the other hand, focuses on creating intelligent machines that can mimic human cognitive abilities such as learning, problem-solving, decision-making, and language understanding. The intersection of these two fields has led to thrilling research theories and practical applications. This study provides a bibliometric analysis of the literature on AI and behavioral economics to gain insight into research trends in this field. We conducted this bibliometric analysis using the Web of Science database on articles published between 2012 and 2022 that were related to AI and behavioral economics. VOSviewer and Bibliometrix R package were utilized to identify influential authors, journals, institutions, and countries in the field. Network analysis was also performed to identify the main research themes and their interrelationships. The analysis revealed that the number of publications on AI and behavioral economics has been increasing steadily over the past decade. We found that most studies focused on customer and consumer behavior, including topics such as decision-making under uncertainty, neuroeconomics, and behavioral game theory, combined mainly with machine learning and deep learning techniques. We also identified several emerging themes, including the use of AI in nudging and prospect theory in behavioral finance, as well as undeveloped themes such as AI-driven behavioral macroeconomics. The findings suggests that there is a need for more interdisciplinary collaboration between researchers in behavioral economics and AI. We also suggest that future research on AI and behavioral economics further consider the ethical implications of using AI and behavioral insights in decision-making. This study can serve as a valuable resource for researchers interested in AI and behavioral economics.  © 2013 IEEE.},
	author_keywords = {Artificial intelligence; behavioral economics; behavioral finance; bibliometric analysis; consumer behavior; decision making; investor behavior; machine learning; neuroeconomics},
	keywords = {Biological systems; Consumer behavior; Decision theory; Deep learning; Economics; Game theory; Problem solving; Uncertainty analysis; Behavioral economics; Behavioral science; Behavioural finances; Bibliometrics analysis; Biological system modeling; Code; Decisions makings; Investor behavior; Machine-learning; Neuroeconomics; Psychology; Uncertainty; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access}
}

@ARTICLE{Itsuji2024483,
	author = {Itsuji, Hiroaki and Uezono, Takumi and Toba, Tadanobu and Kundu, Subrata Kumar},
	title = {Real-Time Diagnostic Technique for AI-Enabled System},
	year = {2024},
	journal = {IEEE Open Journal of Intelligent Transportation Systems},
	volume = {5},
	pages = {483 – 494},
	doi = {10.1109/OJITS.2024.3435712},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200203268&doi=10.1109%2fOJITS.2024.3435712&partnerID=40&md5=4de0e4ebc8e0244d3c50311bad0395c1},
	abstract = {The last few decades have witnessed a dramatic evolution of Artificial Intelligence (AI) algorithms, represented by Deep Neural Networks (DNNs), resulting in AI-enabled systems being significantly dominant in various fields, including robotics, healthcare, and mobility. AI-enabled systems are currently used even for safety-critical applications, including automated driving, where they encounter reliability challenges from both hardware (HW) and software (SW) perspectives. However, there is no effective technique available that can diagnose HW and SW of AI-enabled systems in real-time during operation. Therefore, this paper proposes an intelligent real-time diagnostic technique for detecting HW and SW anomalies of AI-enabled systems and continuously improving the SW quality during operation. The proposed technique can detect HW anomalies to avoid unexpected changes in AI parameters and subsequent AI performance degradation using single context data with a detection accuracy of more than 92%. The proposed technique can also detect SW anomalies and identify edge cases in real-time, which could result in performance degradation by more than 50% compared to normal conditions. The identified edge cases can be used to continuously enhance the SW quality. Experimental results show the effectiveness of the technique for practical applications and thus can contribute to realize reliable and improved AI-enabled systems.  © 2020 IEEE.},
	author_keywords = {AI-enabled system; Artificial intelligence (AI); diagnostic technology; real-time monitoring; system reliability},
	keywords = {Application programs; Deep neural networks; Edge detection; Interactive computer systems; Redundancy; Safety engineering; Software reliability; Accuracy; Anomaly detection; Artificial intelligence; Artificial intelligence-enabled system; Diagnostic technologies; Hardware and software; Image edge detection; Real - Time system; Real time monitoring; System reliability; Real time systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Wang2024,
	author = {Wang, Wenpu and Huang, Shu},
	title = {The Application of Artificial Intelligence Teaching Software in College English Teaching},
	year = {2024},
	journal = {Applied Mathematics and Nonlinear Sciences},
	volume = {9},
	number = {1},
	doi = {10.2478/amns.2023.2.00657},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173485808&doi=10.2478%2famns.2023.2.00657&partnerID=40&md5=b8be3f4529c32c1025e2920eb94519d9},
	abstract = {This paper first proposes English learning software based on personalized recommendations to enhance students’ interest and motivation in learning college English based on artificial intelligence teaching. Secondly, it consists of constructing a learner model, an English learning resource personalized recommendation algorithm to constitute an English learning resource recommendation model, and using a collaborative decomposition algorithm and a regularization algorithm to integrate the potential, multifaceted trust relationship between users with the traditional scoring matrix, a process that emphasizes that trust between users in English teaching varies by different factors and strengths. Then, a personalized recommendation-based university English learning software was designed, and English learning software applications were analyzed by example. The results showed that the adjusted means of pre and post-test scores of the control group, experimental group one, and experimental group two were 77.37, 80.94, and 75.12, respectively, and the statistical score result F=5.33, P<05 indicated that there was a significant difference in the change of English performance of these three groups, which verified that the teaching software had a facilitating effect on the improvement of English teaching quality. © 2023 Wenpu Wang and Shu Huang, published by Sciendo.},
	author_keywords = {Collaborative decomposition algorithm; College English; Personalized recommendation; Regularization algorithm; Teaching software},
	keywords = {Application programs; Learning systems; Software testing; Students; Teaching; Collaborative decomposition algorithm; College English; Decomposition algorithm; English Learning; English teaching; Learning resource; Learning software; Personalized recommendation; Regularization algorithms; Teaching software; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Nešpor202469,
	author = {Nešpor, Jan},
	title = {AUTOMATED ADMINISTRATIVE DECISION-MAKING: WHAT IS THE BLACK BOX HIDING?},
	year = {2024},
	journal = {Acta Universitatis Carolinae Iuridica},
	volume = {70},
	number = {2},
	pages = {69 – 83},
	doi = {10.14712/23366478.2024.23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194417683&doi=10.14712%2f23366478.2024.23&partnerID=40&md5=6ee79847c56a44c132d33034fd6c497d},
	abstract = {The exploration of the "black box"phenomenon underscores opacity challenges in automated administrative decision-making systems, prompting a discussion on the paradox of transparency. Advocating for the concept of "qualified transparency", the article aims to navigate the delicate balance between understanding and safeguarding sensitive information. Ethical imperatives, including respect for human autonomy, harm prevention, fairness, and explicability, are considered, culminating in recommendations for human participation, ethicality or accountability by design considerations, and the implementation of regulatory sandboxes to test such models prior to broad integration. Ultimately, the article advocates for a comprehensive discourse on transitioning from a human-centric to an automated public administration model, acknowledging the complexity and potential risks involved.  © 2024 The Author.},
	author_keywords = {artificial intelligence; automated administrative decision-making; transparency},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Ahmad2023,
	author = {Ahmad, Khlood and Abdelrazek, Mohamed and Arora, Chetan and Agrahari Baniya, Arbind and Bano, Muneera and Grundy, John},
	title = {Requirements engineering framework for human-centered artificial intelligence software systems},
	year = {2023},
	journal = {Applied Soft Computing},
	volume = {143},
	doi = {10.1016/j.asoc.2023.110455},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163544926&doi=10.1016%2fj.asoc.2023.110455&partnerID=40&md5=de91d4822852a3733b100d5ff3c485e6},
	abstract = {Context: Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Objective: Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. Method: In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. Results: The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. Conclusion: We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software. © 2023 The Author(s)},
	author_keywords = {Artificial intelligence; Conceptual modeling; Empirical software engineering; Human-centered; Machine learning; Requirements engineering; Software engineering; Virtual reality},
	keywords = {E-learning; Engineering education; Machine learning; Requirements engineering; Software engineering; Conceptual model; Empirical Software Engineering; Engineering frameworks; Human-centered; In-buildings; Intelligence software; Machine-learning; Requirement engineering; Software solution; Software-systems; Virtual reality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Anjum2025164,
	author = {Anjum, Asraar and Hrairi, Meftah and Aabid, Abdul and Yatim, Norfazrina and Ali, Maisarah},
	title = {Integrating AI and statistical methods for enhancing civil structures: current trends, practical issues and future direction},
	year = {2025},
	journal = {Frattura ed Integrita Strutturale},
	volume = {19},
	number = {71},
	pages = {164 – 181},
	doi = {10.3221/IGF-ESIS.71.12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208634464&doi=10.3221%2fIGF-ESIS.71.12&partnerID=40&md5=32e0dcbbacd5df44c44ab9aabab61491},
	abstract = {The integration of artificial intelligence (AI) and statistical methods has revolutionized civil engineering by enhancing accuracy, efficiency, and reliability in various processes. This review systematically examines how advanced optimization techniques, including artificial neural networks (ANNs), Design of Experiments (DOE), and fuzzy logic (FL), are transforming civil engineering practices. It emphasizes the significant roles these methods play in addressing modern challenges such as structural health monitoring, damage detection, seismic design optimization, and concrete condition assessment. The review delves into case studies and real-world applications, showcasing the potential of these methods to create more resilient, sustainable, and cost-effective infrastructures. It critically examines the limitations and scalability of these techniques, identifying gaps in current research and practical challenges in real-world applications. The investigation also highlights the need for substantial computational resources, data privacy, security, and software interoperability. By addressing these issues, the review not only shows advancements in optimization techniques but also outlines future research directions, aiming to bridge the gap between theoretical developments and practical applications in civil engineering. This review serves as an essential resource for researchers, professionals, and policymakers interested in leveraging optimization techniques to advance civil engineering practices. © 2025, Gruppo Italiano Frattura. All rights reserved.},
	author_keywords = {Artificial Intelligence (AI); Civil Structures; Design of Experiments (DOE); Fuzzy Logic; Optimization Techniques},
	keywords = {Cost engineering; 'current; Artificial intelligence; Civil structure; Design of experiment; Efficiency and reliability; Engineering practices; Fuzzy-Logic; Optimization techniques; Practical issues; Real-world; Seismic design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Kabashkin2025,
	author = {Kabashkin, Igor},
	title = {AI and Evolutionary Computation for Intelligent Aviation Health Monitoring},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {7},
	doi = {10.3390/electronics14071369},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002355209&doi=10.3390%2felectronics14071369&partnerID=40&md5=92752869100db5429905fc2b9f6a9e66},
	abstract = {This paper presents a novel framework integrating evolutionary computation and artificial intelligence for aircraft health monitoring and management systems. The research addresses critical challenges in modern aircraft maintenance through a comprehensive approach combining real-time fault detection, predictive maintenance, and multi-objective optimization. The framework employs deep learning models for fault detection, achieving about 97% classification accuracy with an F1-score of 0.97, while remaining useful life prediction yields an R2 score of 0.89 with a mean absolute error of 9.8 h. Evolutionary algorithms optimize maintenance strategies, reducing downtime and costs by up to 22% compared to traditional methods. The methodology includes robust data processing protocols, feature engineering techniques, and a modular system architecture supporting real-time monitoring and decision-making. Simulation experiments demonstrate the framework’s effectiveness in balancing maintenance objectives while maintaining high reliability. The research provides practical implementation guidelines and addresses key challenges in computational efficiency, data quality, and system integration. The results show significant improvements in maintenance planning efficiency and system reliability compared to traditional approaches. The framework’s modular design enables scalability and adaptation to various aircraft systems, offering broader applications in complex technical system maintenance. © 2025 by the author.},
	author_keywords = {aircraft health monitoring; artificial intelligence; evolutionary computation; fault detection; maintenance optimization; multi-objective optimization; predictive maintenance; remaining useful life prediction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{González-Manzano2025,
	author = {González-Manzano, Lorena and Garcia-Alfaro, Joaquin},
	title = {Software vulnerability detection under poisoning attacks using CNN-based image processing},
	year = {2025},
	journal = {International Journal of Information Security},
	volume = {24},
	number = {2},
	doi = {10.1007/s10207-025-00989-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219748986&doi=10.1007%2fs10207-025-00989-2&partnerID=40&md5=a9e3834d3b3d12baf419356a84202afb},
	abstract = {Design flows, code errors, or inadequate countermeasures may occur in software development. Some of them lead to vulnerabilities in the code, opening the door to attacks. Assorted techniques are developed to detect vulnerable code samples, making artificial intelligence techniques, such as Machine Learning (ML), a common practice. Nonetheless, the security of ML is a major concern. This includes the the case of ML-based detection whose training process is affected by data poisoning. More generally, vulnerability detection can be evaded unless poisoning attacks are properly handled. This paper tackles this problem. A novel vulnerability detection system based on ML-based image processing, using Convolutional Neural Network (CNN), is proposed. The system, hereinafter called IVul, is evaluated under the presence of backdoor attacks, a precise type of poisoning in which a pattern is introduced in the training data to alter the expected behavior of the learned models. IVul is evaluated with more than three thousand code samples associated with two representative programming languages (C# and PHP). IVul outperforms other comparable state-of-the-art vulnerability detectors in the literature, reaching 82% to 99% detection accuracy. Besides, results show that the type of attack may affect a particular language more than another, though, in general, PHP is more resilient to proposed attacks than C#. © The Author(s) 2025.},
	author_keywords = {Artificial Intelligence; Convolutional neural networks; Machine learning; Poisoning attack; Software vulnerability detection},
	keywords = {Adversarial machine learning; C (programming language); Image coding; Network security; Problem oriented languages; Artificial intelligence techniques; Convolutional neural network; Design flows; Images processing; Machine-learning; Network-based; Poisoning attacks; Software vulnerabilities; Software vulnerability detection; Vulnerability detection; Convolutional neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Mustaqeem2024,
	author = {Mustaqeem, Mohd and Mustajab, Suhel and Alam, Mahfooz and Jeribi, Fathe and Alam, Shadab and Shuaib, Mohammed},
	title = {A trustworthy hybrid model for transparent software defect prediction: SPAM-XAI},
	year = {2024},
	journal = {PLoS ONE},
	volume = {19},
	number = {7 July},
	doi = {10.1371/journal.pone.0307112},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198363847&doi=10.1371%2fjournal.pone.0307112&partnerID=40&md5=1c51064eaa98ec865c2fde043df74115},
	abstract = {Maintaining quality in software development projects is becoming very difficult because the complexity of modules in the software is growing exponentially. Software defects are the primary concern, and software defect prediction (SDP) plays a crucial role in detecting faulty modules early and planning effective testing to reduce maintenance costs. However, SDP faces challenges like imbalanced data, high-dimensional features, model overfitting, and outliers. Moreover, traditional SDP models lack transparency and interpretability, which impacts stakeholder confidence in the Software Development Life Cycle (SDLC). We propose SPAM-XAI, a hybrid model integrating novel sampling, feature selection, and eXplainable-AI (XAI) algorithms to address these challenges. The SPAM-XAI model reduces features, optimizes the model, and reduces time and space complexity, enhancing its robustness. The SPAM-XAI model exhibited improved performance after experimenting with the NASA PROMISE repository’s datasets. It achieved an accuracy of 98.13% on CM1, 96.00% on PC1, and 98.65% on PC2, surpassing previous state-of-the-art and baseline models with other evaluation matrices enhancement compared to existing methods. The SPAM-XAI model increases transparency and facilitates understanding of the interaction between features and error status, enabling coherent and comprehensible predictions. This enhancement optimizes the decision-making process and enhances the model’s trustworthiness in the SDLC. © 2024 Mustaqeem et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	keywords = {Algorithms; Artificial Intelligence; Humans; Models, Theoretical; Software; adult; algorithm; article; decision making; explainable artificial intelligence; feature selection; human; life cycle; male; prediction; software; trustworthiness; algorithm; artificial intelligence; theoretical model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Rodríguez-Nieto2024274,
	author = {Rodríguez-Nieto, Daniel and Ojeda, Marta and Navas, Eduardo and Fernández, Roemi},
	title = {Software Architecture for the HortiRobot Dual-Arm Robotic Manipulation System.; [Arquitectura Software para el Sistema Robótico de Manipulación Dual HortiRobot]},
	year = {2024},
	journal = {RIAI - Revista Iberoamericana de Automatica e Informatica Industrial},
	volume = {21},
	number = {3},
	pages = {274 – 285},
	doi = {10.4995/riai.2024.20611},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197661477&doi=10.4995%2friai.2024.20611&partnerID=40&md5=45348f8c49d604e495abdf2981680d45},
	abstract = {The software architecture is a crucial component in any autonomous robotic system, as it defines the organizational structure and interactions of the different modules that compose it. For an autonomous robotic system to perform various tasks, diverse processes are required, such as perceiving the environment, representing knowledge, making decisions, and planning movements. While the development of each of these processes is fundamental, their integration into a functional architecture for implementation is equally important. This integration has profound implications for resource management, adaptability to different environments and tasks, flexibility to modify or expand functionalities and address new requirements, and ease of system maintenance and updates. Therefore, this article presents the software architecture designed to control, communicate, and integrate the various modules that make up a mobile bimanipulator, highlighting among its main advantages the ease of debugging errors and conducting tests for new applications without the inherent risk of damaging the physical equipment. To demonstrate the feasibility of the proposal, the implementation of the architecture is validated through its application to the mobile dual-arm robotic system HortiRobot, designed to perform various tasks involved in the life cycle of agricultural crops. © 2024 Universidad Politecnica de Valencia.. All rights reserved.},
	author_keywords = {Agricultural robotics; artificial intelligence; dual-arm robotic manipulation; intelligent perception; learning; software architecture},
	keywords = {Agricultural robots; Crops; Intelligent robots; Life cycle; Program debugging; Robot programming; Software architecture; Software testing; Agricultural robotics; Autonomous robotic systems; Dual arm; Dual-arm robotic manipulation; Intelligent perception; Learning; Manipulation system; Organizational interactions; Organizational structures; Robotic manipulation; Application programs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Dai2024,
	author = {Dai, Ling and Wu, Zhijun and Pan, Xiaocheng and Zheng, Dingchang and Kang, Mengli and Zhou, Mingming and Chen, Guanyu and Liu, Haipeng and Tian, Xin},
	title = {Design and implementation of an automatic nursing assessment system based on CDSS technology},
	year = {2024},
	journal = {International Journal of Medical Informatics},
	volume = {183},
	doi = {10.1016/j.ijmedinf.2023.105323},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180533910&doi=10.1016%2fj.ijmedinf.2023.105323&partnerID=40&md5=e8dd14d072636817bc50288c04605fb5},
	abstract = {Background: Various quantitative and quality assessment tools are currently used in nursing to evaluate a patient's physiological, psychological, and socioeconomic status. The results play important roles in evaluating the efficiency of healthcare, improving the treatment plans, and lowing relevant clinical risks. However, the manual process of the assessment imposes a substantial burden and can lead to errors in digitalization. To fill these gaps, we proposed an automatic nursing assessment system based on clinical decision support system (CDSS). The framework underlying the CDSS included experts, evaluation criteria, and voting roles for selecting electronic assessment sheets over paper ones. Methods: We developed the framework based on an expert voting flow to choose electronic assessment sheets. The CDSS was constructed based on a nursing process workflow model. A multilayer architecture with independent modules was used. The performance of the proposed system was evaluated by comparing the adverse events’ incidence and the average time for regular daily assessment before and after the implementation. Results: After implementation of the system, the adverse nursing events’ incidence decreased significantly from 0.43 % to 0.37 % in the first year and further to 0.27 % in the second year (p-value: 0.04). Meanwhile, the median time for regular daily assessments further decreased from 63 s to 51 s. Conclusions: The automatic assessment system helps to reduce nurses’ workload and the incidence of adverse nursing events. © 2023 The Author(s)},
	author_keywords = {clinical decision support system (CDSS); clinical information system; clinical quality management; nursing assessment; nursing informatics},
	keywords = {Decision Support Systems, Clinical; Efficiency; Health Facilities; Humans; Nursing Assessment; Nursing Process; Artificial intelligence; Decision support systems; Information management; Medical informatics; Medical information systems; Quality control; Quality management; Assessment system; Clinical decision support system; Clinical decision support systems; Clinical information system; Clinical quality management; Design and implementations; Nursing assessment; Nursing informatics; Quality assessment; Quantitative assessments; adverse event; Article; autoanalysis; clinical decision support system; electronic medical record; health care quality; human; incidence; medical documentation; medical information system; nursing assessment; nursing diagnosis; nursing informatics; nursing process; patient safety; practice guideline; quality assessment tool; theoretical model; health care facility; nursing assessment; nursing process; productivity; Nursing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Laine2024,
	author = {Laine, Joakim and Minkkinen, Matti and Mäntymäki, Matti},
	title = {Ethics-based AI auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders},
	year = {2024},
	journal = {Information and Management},
	volume = {61},
	number = {5},
	doi = {10.1016/j.im.2024.103969},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192254335&doi=10.1016%2fj.im.2024.103969&partnerID=40&md5=fb80b24cd8b0204fcb87d8c52297787a},
	abstract = {This systematic literature review synthesizes the conceptualizations of ethical principles in AI auditing literature and the knowledge contributions to the stakeholders of AI auditing. We explain how the literature discusses fairness, transparency, non-maleficence, responsibility, privacy, trust, beneficence, and freedom/autonomy. Conceptualizations vary along social/technical- and process/outcome-oriented dimensions. The main stakeholders of ethics-based AI auditing are system developers and deployers, the wider public, researchers, auditors, AI system users, and regulators. AI auditing provides three types of knowledge contributions to stakeholders: 1) guidance; 2) methods, tools, and frameworks; and 3) awareness and empowerment. © 2024},
	author_keywords = {AI auditing; AI ethics; AI governance; Artificial intelligence; Auditing; Ethics-based AI auditing; Systematic literature review},
	keywords = {Artificial intelligence; AI auditing; AI ethic; AI governance; AI systems; Auditing; Ethic-based AI auditing; Ethical principles; Knowledge contributions; System developers; Systematic literature review; Ethical technology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Mlynska20232104,
	author = {Mlynska, Lucja and Malouhi, Amer and Ingwersen, Maja and Güttler, Felix and Gräger, Stephanie and Teichgräber, Ulf},
	title = {Artificial intelligence for assistance of radiology residents in chest CT evaluation for COVID-19 pneumonia: a comparative diagnostic accuracy study},
	year = {2023},
	journal = {Acta Radiologica},
	volume = {64},
	number = {6},
	pages = {2104 – 2110},
	doi = {10.1177/02841851231162085},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150519329&doi=10.1177%2f02841851231162085&partnerID=40&md5=d6f15cb2b598fa0ba41dee0b4d270247},
	abstract = {Background: In hospitals, it is crucial to rule out coronavirus disease 2019 (COVID-19) timely and reliably. Artificial intelligence (AI) provides sufficient accuracy to identify chest computed tomography (CT) scans with signs of COVID-19. Purpose: To compare the diagnostic accuracy of radiologists with different levels of experience with and without assistance of AI in CT evaluation for COVID-19 pneumonia and to develop an optimized diagnostic pathway. Material and Methods: The retrospective, single-center, comparative case-control study included 160 consecutive participants who had undergone chest CT scan between March 2020 and May 2021 without or with confirmed diagnosis of COVID-19 pneumonia in a ratio of 1:3. Index tests were chest CT evaluation by five radiological senior residents, five junior residents, and an AI software. Based on the diagnostic accuracy in every group and on comparison of groups, a sequential CT assessment pathway was developed. Results: Areas under receiver operating curves were 0.95 (95% confidence interval [CI]=0.88–0.99), 0.96 (95% CI=0.92–1.0), 0.77 (95% CI=0.68–0.86), and 0.95 (95% CI=0.9–1.0) for junior residents, senior residents, AI, and sequential CT assessment, respectively. Proportions of false negatives were 9%, 3%, 17%, and 2%, respectively. With the developed diagnostic pathway, junior residents evaluated all CT scans with the support of AI. Senior residents were only required as second readers in 26% (41/160) of the CT scans. Conclusion: AI can support junior residents with chest CT evaluation for COVID-19 and reduce the workload of senior residents. A review of selected CT scans by senior residents is mandatory. © The Foundation Acta Radiologica 2023.},
	author_keywords = {Artificial intelligence; computed tomography; COVID-19; deep learning; neural networks; SARS-CoV-2},
	keywords = {Artificial Intelligence; Case-Control Studies; COVID-19; COVID-19 Testing; Humans; Pneumonia; Radiology; Retrospective Studies; SARS-CoV-2; Tomography, X-Ray Computed; Computerized tomography; Deep neural networks; Diagnosis; Software testing; Accuracy study; Case-control study; Computed tomography; Computed tomography scan; Confidence interval; Deep learning; Diagnostic accuracy; Index tests; Intelligence software; Neural-networks; aged; Article; artificial intelligence; artificial neural network; case control study; clinical assessment; clinical pathway; cohort analysis; comparative study; confidence interval; controlled study; coronavirus disease 2019; deep learning; diagnostic accuracy; diagnostic test accuracy study; false negative result; female; human; major clinical study; male; pneumonia; radiologist; real time polymerase chain reaction; receiver operating characteristic; resident; retrospective study; Severe acute respiratory syndrome coronavirus 2; thorax radiography; x-ray computed tomography; artificial intelligence; coronavirus disease 2019; COVID-19 testing; diagnostic imaging; pneumonia; procedures; radiology; x-ray computed tomography; Coronavirus; COVID-19},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Green Open Access}
}

@ARTICLE{Garrad2023,
	author = {Garrad, Phillip and Unnikrishnan, Saritha},
	title = {Reinforcement learning in VANET penetration testing},
	year = {2023},
	journal = {Results in Engineering},
	volume = {17},
	doi = {10.1016/j.rineng.2023.100970},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149917954&doi=10.1016%2fj.rineng.2023.100970&partnerID=40&md5=388f4c523d9f12c974b02972aa52f9b3},
	abstract = {The recent popularity of Connected and Autonomous Vehicles (CAV) corresponds with an increase in the risk of cyber-attacks. These cyber-attacks are instigated by white-coat hackers, and cyber-criminals. As Connected Vehicles move towards full autonomy the impact of these cyber-attacks also grows. The current research highlights challenges faced in cybersecurity testing of CAV, including access, the cost of representative test setup and the lack of experts in the field. Possible solutions of how these challenges can be overcome are reviewed and discussed. From these findings a software simulated Vehicular Ad Hoc NETwork (VANET) is established as a cost-effective representative testbed. Penetration tests are then performed on this simulation, demonstrating a cyber-attack in CAV. Studies have shown Artificial Intelligence (AI) to improve runtime, increase efficiency and comprehensively cover all the typical test aspects, in penetration testing in other industries. In this research a Reinforcement Learning model, called Q-Learning, is applied to automate the software simulation. The expectation from this implementation is to see improvements in runtime and efficiency for the VANET model. The results show this approach to be promising and using AI in penetration testing for VANET to improve efficiency in most cases. Each case is reviewed in detail before discussing possible ways to improve the implementation and get a truer reflection of the real-world application. © The Authors},
	author_keywords = {Artificial intelligence; Connected vehicles; Cybersecurity; Penetration testing; Software simulation},
	keywords = {Computer crime; Cost effectiveness; Crime; Cyber attacks; Efficiency; Learning algorithms; Network security; Personal computing; Reinforcement learning; Software testing; Vehicular ad hoc networks; Autonomous Vehicles; Connected vehicle; Cyber security; Cyber-attacks; Network penetration testing; Penetration testing; Reinforcement learnings; Runtimes; Software simulation; Vehicular Adhoc Networks (VANETs); Vehicles},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Higgins2023797,
	author = {Higgins, David C. and Johner, Christian},
	title = {Validation of Artificial Intelligence Containing Products Across the Regulated Healthcare Industries},
	year = {2023},
	journal = {Therapeutic Innovation and Regulatory Science},
	volume = {57},
	number = {4},
	pages = {797 – 809},
	doi = {10.1007/s43441-023-00530-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159654842&doi=10.1007%2fs43441-023-00530-4&partnerID=40&md5=b7906baf943ada896013fabbf0e8bd9d},
	abstract = {Purpose: The introduction of artificial intelligence / machine learning (AI/ML) products to the regulated fields of pharmaceutical research and development (R&D) and drug manufacture, and medical devices (MD) and in vitro diagnostics (IVD), poses new regulatory problems: a lack of a common terminology and understanding leads to confusion, delays and product failures. Validation as a key step in product development, common to each of these sectors including computerized systems and AI/ML development, offers an opportune point of comparison for aligning people and processes for cross-sectoral product development. Methods: A comparative approach, built upon workshops and a subsequent written sequence of exchanges, is summarized in a look-up table suitable for mixed-teams work. Results: 1. A bottom-up, definitions led, approach which leads to a distinction between broad vs narrow validation, and their relationship to regulatory regimes. 2. Common basis introduction to the primary methodologies for software validation, including AI-containing software validation. 3. Pharmaceutical drug development and MD/IVD-specific perspectives on compliant AI software development, as a basis for collaboration. Conclusions: Alignment of the terms and methodologies used in validation of software products containing artificial intelligence/machine learning (AI/ML) components across the regulated industries of human health is a vital first step in streamlining processes and improving workflows. © 2023, The Author(s), under exclusive licence to The Drug Information Association, Inc.},
	author_keywords = {Artificial intelligence; Medical device; Methodology; Regulatory affairs; Software; Validation},
	keywords = {Artificial Intelligence; Health Care Sector; Humans; Pharmaceutical Preparations; Software; drug; Article; artificial intelligence; clinical evaluation; comparative study; controlled study; drug industry; health care industry; human; machine learning; methodology; product development; quality control; software validation; usability testing; workshop; health care cost; software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Green Open Access}
}

@ARTICLE{Ramos2025,
	author = {Ramos, Tafline and Dean, Amanda and McGregor, David},
	title = {AI-Augmented Software Engineering: Revolutionizing or Challenging Software Quality and Testing?},
	year = {2025},
	journal = {Journal of Software: Evolution and Process},
	volume = {37},
	number = {2},
	doi = {10.1002/smr.2741},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209802239&doi=10.1002%2fsmr.2741&partnerID=40&md5=af4df148226b40fe1dc0d5c7e501a686},
	abstract = {With organizations seeking faster, cheaper, and smarter ways of delivering higher quality software, many are looking towards generative artificial intelligence (AI) to drive efficiencies and innovation throughout the software development lifecycle. However, generative AI can suffer from several fundamental issues, including a lack of traceability in concept generation and decision-making, the potential for making incorrect inferences (hallucinations), shortcomings in response quality, and bias. Quality engineering (QE) has long been utilized to enable more efficient and effective delivery of higher quality software. A core aspect of QE is adopting quality models to support various lifecycle practices, including requirements definition, quality risk assessments, and testing. In this position paper, we introduce the application of QE to AI systems, consider shortcomings in existing AI quality models from the International Organization for Standardization (ISO), and propose extensions to ISO models based on the results of a survey. We also reflect on skills that IT graduates may need in the future, to support delivery of better-quality AI. © 2024 John Wiley & Sons Ltd.},
	author_keywords = {AI; artificial intelligence; generative AI; ISO; quality; quality engineering; quality models; testing},
	keywords = {Generative adversarial networks; Generative artificial intelligence; High-quality software; International organization for standardizations; Looking toward; Quality; Quality engineering; Quality modeling; Software development life-cycle; Software Quality; Software testings; Software quality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Geethal20234526,
	author = {Geethal, Charaka and Bohme, Marcel and Pham, Van-Thuan},
	title = {Human-in-the-Loop Automatic Program Repair},
	year = {2023},
	journal = {IEEE Transactions on Software Engineering},
	volume = {49},
	number = {10},
	pages = {4526 – 4549},
	doi = {10.1109/TSE.2023.3305052},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168713494&doi=10.1109%2fTSE.2023.3305052&partnerID=40&md5=54179bf4493f0d4fd86d368b593963ec},
	abstract = {learn2fix is a human-in-the-loop interactive program repair technique, which can be applied when no bug oracle - except the user who is reporting the bug - is available. This approach incrementally learns the condition under which the bug is observed by systematic negotiation with the user. In this process, learn2fix generates alternative test inputs and sends some of those to the user for obtaining their labels. A limited query budget is assigned to the user for this task. A query is a Yes/No question: 'When executing this alternative test input, the program under test produces the following output; is the bug observed?'. Using the labelled test inputs, learn2fix incrementally learns an automatic bug oracle to predict the user's response. A classification algorithm in machine learning is used for this task. Our key challenge is to maximise the oracle's accuracy in predicting the tests that expose the bug given a practical, small budget of queries. After learning the automatic oracle, an existing program repair tool attempts to repair the bug using the alternative tests that the user has labelled. Our experiments demonstrate that learn2fix trains a sufficiently accurate automatic oracle with a reasonably low labelling effort (lt. 20 queries), and the oracles represented by interpolation-based classifiers produce more accurate predictions than those represented by approximation-based classifiers. Given the user-labelled test inputs, generated using the interpolation-based approach, the GenProg and Angelix automatic program repair tools produce patches that pass a much larger proportion of validation tests than the manually constructed test suites provided by the repair benchmark.  © 1976-2012 IEEE.},
	author_keywords = {active machine learning; Automated test oracles; classification algorithms; semi-automatic program repair},
	keywords = {Artificial intelligence; Budget control; Costs; Forecasting; Interpolation; Learning algorithms; Learning systems; Program debugging; Query processing; Software testing; Active machine learning; Automated test; Automated test oracle; Automatic programs; Classification algorithm; Computer bugs; Fuzzing; Human-in-the-loop; Labelings; Semi-automatic program repair; Semi-automatics; Test oracles; Repair},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Laveaux2023,
	author = {Laveaux, Maurice and Willemse, Tim A.C.},
	title = {Decomposing monolithic processes in a process algebra with multi-actions},
	year = {2023},
	journal = {Journal of Logical and Algebraic Methods in Programming},
	volume = {132},
	doi = {10.1016/j.jlamp.2023.100858},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147983712&doi=10.1016%2fj.jlamp.2023.100858&partnerID=40&md5=22679202b2ddb602b92339eada30ce9f},
	abstract = {A monolithic process is a single recursive equation with data parameters, which only uses non-determinism, action prefixing, and recursion. We present a technique that decomposes such a monolithic process into multiple processes where each process defines behaviour for a subset of the parameters of the monolithic process. For this decomposition we can show that a composition of these processes is strongly bisimilar to the monolithic process under a suitable synchronisation context. Minimising the resulting processes before determining their composition can be used to derive a state space that is smaller than the one obtained by a monolithic exploration. We apply the decomposition technique to several specifications to show that this works in practice. Finally, we prove that state invariants can be used to further improve the effectiveness of this decomposition technique. © 2023 The Author(s)},
	author_keywords = {Bisimulation; Decomposition; Monolithic processes; Process algebra},
	keywords = {Artificial intelligence; Decomposition; Bisimulations; Decomposition technique; Monolithic process; Monolithics; Multiple process; Non Determinism; Process algebras; Recursions; Recursive equations; State-space; Algebra},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Sánchez-Ruiz2023,
	author = {Sánchez-Ruiz, Luis M. and Moll-López, Santiago and Nuñez-Pérez, Adolfo and Moraño-Fernández, José Antonio and Vega-Fleitas, Erika},
	title = {ChatGPT Challenges Blended Learning Methodologies in Engineering Education: A Case Study in Mathematics},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {10},
	doi = {10.3390/app13106039},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160830935&doi=10.3390%2fapp13106039&partnerID=40&md5=296ab78a7d054bbcc35a28cbf31cf5d3},
	abstract = {This research aims to explore the potential impact of the ChatGPT on b-learning methodologies in engineering education, specifically in mathematics. The study focuses on how the use of these artificial intelligence tools can affect the acquisition of critical thinking, problem-solving, and group work skills among students. The research also analyzes the students’ perception of the reliability, usefulness, and importance of these tools in academia. The study collected data through a survey of 110 students enrolled in a Mathematics I course in BEng Aerospace Engineering where a blended methodology, including flipped teaching, escape room gamification, problem-solving, and laboratory sessions and exams with a computer algebraic system were used. The data collected were analyzed using statistical methods and tests for significance. Results indicate students have quickly adopted ChatGPT tool, exhibiting high confidence in their responses (3.4/5) and general usage in the learning process (3.61/5), alongside a positive evaluation. However, concerns arose regarding the potential impact on developing lateral competencies essential for future engineers (2.8/5). The study concludes that the use of ChatGPT in blended learning methodologies poses new challenges for education in engineering, which requires the adaptation of teaching strategies and methodologies to ensure the development of essential skills for future engineers. © 2023 by the authors.},
	author_keywords = {artificial intelligence; blended learning; ChatGPT; game-based learning; GPT-3.5; GPT-4},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 117; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Serban2024,
	author = {Serban, Alex and van der Blom, Koen and Hoos, Holger and Visser, Joost},
	title = {Software engineering practices for machine learning — Adoption, effects, and team assessment},
	year = {2024},
	journal = {Journal of Systems and Software},
	volume = {209},
	doi = {10.1016/j.jss.2023.111907},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179133424&doi=10.1016%2fj.jss.2023.111907&partnerID=40&md5=6e38d78e6097c3db9de699702d42b134},
	abstract = {Machine learning (ML) is extensively used in production-ready applications, calling for mature engineering techniques to ensure robust development, deployment and maintenance. Given the potential negative impact machine learning (ML) can have on people, society or the environment, engineering techniques that can ensure robustness against technical errors and adversarial attacks are of considerable importance. In this work, we investigate how teams of experts develop, deploy and maintain software with ML components. Moreover, we link what teams do to the effects they aim to achieve and provide means for improvement. Towards this goal, we performed a mixed-methods study with a sequential exploratory strategy. First, we performed a systematic literature review through which we mined both academic and grey literature, and compiled a catalogue of engineering practices for ML. Second, we validated this catalogue using a large-scale survey, which measured the degree of adoption of the practices and their perceived effects. Third, we ran validation interviews with practitioners to add depth to the survey results. The catalogue covers a broad range of practices for engineering software systems with ML components and for ensuring non-functional properties that fall under the umbrella of trustworthy ML, such as fairness, security or accountability. Here, we present the results of our study, which indicate, for example, that larger and more experienced teams tend to adopt more practices, but that trustworthiness practices tend to be neglected. Moreover, we show that the effects measured in our survey, such as team agility or accountability, can be predicted quite accurately from groups of practices. This allowed us to contrast the importance of the practices for these effects as well as adoption rates, revealing, for example, that widely adopted practices are, in reality, less important with respect to some effects. For instance, writing reusable scripts for data cleaning and merging is highly adopted, but has a limited impact on reproducibility. Overall, our study provides a quantitative assessment of ML engineering practices and their impact on desirable properties of software with ML components, by which we open multiple avenues for improving the adoption of useful practices. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)},
	author_keywords = {Artificial Intelligence; Engineering practices; Machine learning; Maturity Models; Software engineering},
	keywords = {Application programs; Engineering education; Network security; Effect assessments; Engineering practices; Engineering techniques; Impact machines; Machine-learning; Maturity model; Open science; Robust development; Software engineering practices; Team assessment; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Meziane2023,
	author = {Meziane, Hind and Ouerdi, Noura},
	title = {A survey on performance evaluation of artificial intelligence algorithms for improving IoT security systems},
	year = {2023},
	journal = {Scientific Reports},
	volume = {13},
	number = {1},
	doi = {10.1038/s41598-023-46640-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178388074&doi=10.1038%2fs41598-023-46640-9&partnerID=40&md5=06e58357fbf7e6b668bb3dab1a898ce3},
	abstract = {Security is an important field in the Internet of Things (IoT) systems. The IoT and security are topical domains. Because it was obtained 35,077 document results from the Scopus database. Hence, the AI (Artificial Intelligence) has proven its efficiency in several domains including security, digital marketing, healthcare, big data, industry, education, robotic, and entertainment. Thus, the contribution of AI to the security of IoT systems has become a huge breakthrough. This contribution adopts the artificial intelligence (AI) as a base solution for the IoT security systems. Two different subsets of AI algorithms were considered: Machine Learning (ML) and Deep Learning (DL) methods. Nevertheless, it is difficult to determine which AI method and IoT dataset are best (more suitable) for classifying and/or detecting intrusions and attacks in the IoT domain. The large number of existing publications on this phenomenon explains the need for the current state of research that covers publications on IoT security using AI methods. Thus, this study compares the results regarding AI algorithms that have been mentioned in the related works. The goal of this paper is to compare the performance assessment of the existing AI algorithms in order to choose the best algorithm as well as whether the chosen algorithm can be used for classifying or/and detecting intrusions and attacks in order to improve security in the IoT domain. This study compares these methods in term of accuracy rate. Evaluating the current state of IoT security, AI and IoT datasets is the main aim for considering our future work. After that, this paper proposes, as result, a new and general taxonomy of AI techniques for IoT security (classification and detection techniques). Finally, the obtained results from this assessment survey that was dedicated to research conducted between 2018 and 2023 were satisfactory. This paper provides a good reference for researchers and readers in the IoT domain. © 2023, The Author(s).},
	keywords = {algorithm; article; artificial intelligence; big data; deep learning; diagnosis; human; internet of things; machine learning; marketing; security; taxonomy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Eramo2024,
	author = {Eramo, Romina and Said, Bilal and Oriol, Marc and Bruneliere, Hugo and Morales, Sergio},
	title = {An architecture for model-based and intelligent automation in DevOps},
	year = {2024},
	journal = {Journal of Systems and Software},
	volume = {217},
	doi = {10.1016/j.jss.2024.112180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200822665&doi=10.1016%2fj.jss.2024.112180&partnerID=40&md5=8a71eb00ba03c6f4eb2d9f0422cb1d8a},
	abstract = {The increasing complexity of modern systems poses numerous challenges at all stages of system development and operation. Continuous software and system engineering processes, e.g., DevOps, are increasingly adopted and spread across organizations. In parallel, many leading companies have begun to apply artificial intelligence (AI) principles and techniques, including Machine Learning (ML), to improve their products. However, there is no holistic approach that can support and enhance the growing challenges of DevOps. In this paper, we propose a software architecture that provides the foundations of a model-based framework for the development of AI-augmented solutions incorporating methods and tools for continuous software and system engineering and validation. The key characteristic of the proposed architecture is that it allows leveraging the advantages of both AI/ML and Model Driven Engineering (MDE) approaches and techniques in a DevOps context. This architecture has been designed, developed and applied in the context of the European large collaborative project named AIDOaRt. In this paper, we also report on the practical evaluation of this architecture. This evaluation is based on a significant set of technical solutions implemented and applied in the context of different real industrial case studies coming from the AIDOaRt project. Moreover, we analyze the collected results and discuss them according to both architectural and technical challenges we intend to tackle with the proposed architecture. © 2024 The Authors},
	author_keywords = {Artificial Intelligence; Continuous software engineering; DevOps; Mode-driven engineering; Software architecture},
	keywords = {Artificial intelligence; Continuous software engineerings; Development and operations; Intelligent automation; Machine-learning; Mode-driven engineering; Model-based OPC; Proposed architectures; Software and systems engineerings; System development; Systems operation; Software architecture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Soltan2024e93,
	author = {Soltan, Andrew A S and Thakur, Anshul and Yang, Jenny and Chauhan, Anoop and D'Cruz, Leon G and Dickson, Phillip and Soltan, Marina A and Thickett, David R and Eyre, David W and Zhu, Tingting and Clifton, David A},
	title = {A scalable federated learning solution for secondary care using low-cost microcomputing: privacy-preserving development and evaluation of a COVID-19 screening test in UK hospitals},
	year = {2024},
	journal = {The Lancet Digital Health},
	volume = {6},
	number = {2},
	pages = {e93 – e104},
	doi = {10.1016/S2589-7500(23)00226-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183591764&doi=10.1016%2fS2589-7500%2823%2900226-1&partnerID=40&md5=f5016e46a9b0078177f66b5d684d098f},
	abstract = {Background: Multicentre training could reduce biases in medical artificial intelligence (AI); however, ethical, legal, and technical considerations can constrain the ability of hospitals to share data. Federated learning enables institutions to participate in algorithm development while retaining custody of their data but uptake in hospitals has been limited, possibly as deployment requires specialist software and technical expertise at each site. We previously developed an artificial intelligence-driven screening test for COVID-19 in emergency departments, known as CURIAL-Lab, which uses vital signs and blood tests that are routinely available within 1 h of a patient's arrival. Here we aimed to federate our COVID-19 screening test by developing an easy-to-use embedded system—which we introduce as full-stack federated learning—to train and evaluate machine learning models across four UK hospital groups without centralising patient data. Methods: We supplied a Raspberry Pi 4 Model B preloaded with our federated learning software pipeline to four National Health Service (NHS) hospital groups in the UK: Oxford University Hospitals NHS Foundation Trust (OUH; through the locally linked research University, University of Oxford), University Hospitals Birmingham NHS Foundation Trust (UHB), Bedfordshire Hospitals NHS Foundation Trust (BH), and Portsmouth Hospitals University NHS Trust (PUH). OUH, PUH, and UHB participated in federated training, training a deep neural network and logistic regressor over 150 rounds to form and calibrate a global model to predict COVID-19 status, using clinical data from patients admitted before the pandemic (COVID-19-negative) and testing positive for COVID-19 during the first wave of the pandemic. We conducted a federated evaluation of the global model for admissions during the second wave of the pandemic at OUH, PUH, and externally at BH. For OUH and PUH, we additionally performed local fine-tuning of the global model using the sites’ individual training data, forming a site-tuned model, and evaluated the resultant model for admissions during the second wave of the pandemic. This study included data collected between Dec 1, 2018, and March 1, 2021; the exact date ranges used varied by site. The primary outcome was overall model performance, measured as the area under the receiver operating characteristic curve (AUROC). Removable micro secure digital (microSD) storage was destroyed on study completion. Findings: Clinical data from 130 941 patients (1772 COVID-19-positive), routinely collected across three hospital groups (OUH, PUH, and UHB), were included in federated training. The evaluation step included data from 32 986 patients (3549 COVID-19-positive) attending OUH, PUH, or BH during the second wave of the pandemic. Federated training of a global deep neural network classifier improved upon performance of models trained locally in terms of AUROC by a mean of 27·6% (SD 2·2): AUROC increased from 0·574 (95% CI 0·560–0·589) at OUH and 0·622 (0·608–0·637) at PUH using the locally trained models to 0·872 (0·862–0·882) at OUH and 0·876 (0·865–0·886) at PUH using the federated global model. Performance improvement was smaller for a logistic regression model, with a mean increase in AUROC of 13·9% (0·5%). During federated external evaluation at BH, AUROC for the global deep neural network model was 0·917 (0·893–0·942), with 89·7% sensitivity (83·6–93·6) and 76·6% specificity (73·9–79·1). Site-specific tuning of the global model did not significantly improve performance (change in AUROC <0·01). Interpretation: We developed an embedded system for federated learning, using microcomputing to optimise for ease of deployment. We deployed full-stack federated learning across four UK hospital groups to develop a COVID-19 screening test without centralising patient data. Federation improved model performance, and the resultant global models were generalisable. Full-stack federated learning could enable hospitals to contribute to AI development at low cost and without specialist technical expertise at each site. Funding: The Wellcome Trust, University of Oxford Medical and Life Sciences Translational Fund. © 2024 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license},
	keywords = {Artificial Intelligence; COVID-19; Hospitals; Humans; Privacy; Secondary Care; State Medicine; United Kingdom; Clinical research; Costs; COVID-19; Deep neural networks; Diagnosis; Forecasting; Hospitals; Learning algorithms; Learning systems; Privacy-preserving techniques; Sensitive data; Software testing; Statistical tests; Clinical data; Embedded-system; Global models; Low-costs; Modeling performance; National health services; Patient data; Receiver operating characteristic curves; Screening tests; Technical expertise; Article; artificial intelligence; classifier; clinical study; coronavirus disease 2019; COVID-19 testing; deep neural network; diagnostic accuracy; diagnostic test accuracy study; digital health technology; external validity; health care cost; health data; hospital admission; human; major clinical study; outcome assessment; pandemic; patient coding; prediction; predictive value; privacy; secondary health care; sensitivity and specificity; United Kingdom; hospital; national health service; privacy; Hospital data processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Zhixin2023,
	author = {Zhixin, Li and Gang, Luo and Zhixian, Ji and Silin, Pan},
	title = {The development and validation of an artificial intelligence-based screening method for atrial septal defect in children's chest x-rays},
	year = {2023},
	journal = {Frontiers in Pediatrics},
	volume = {11},
	doi = {10.3389/fped.2023.1203933},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171830486&doi=10.3389%2ffped.2023.1203933&partnerID=40&md5=4252443efed67350c431e84bf7770927},
	abstract = {Purpose: For precise diagnosis and effective management of atrial septal defects, it is of utmost significance to conduct elementary screenings on children. The primary aim of this study is to develop and authenticate an objective methodology for detecting atrial septal defects by employing deep learning (DL) on chest x-ray (CXR) examinations. Methods: This retrospective study encompassed echocardiographs and corresponding Chest x-rays that were consistently gathered at Qingdao Women's and Children's Hospital from 2018 to 2022. Based on a collaborative diagnosis report by two cardiologists with over 10 years of experience in echocardiography, these radiographs were classified as positive or negative for atrial septal defect, and then divided into training and validation datasets. An artificial intelligence model was formulated by utilizing the training dataset and fine-tuned using the validation dataset. To evaluate the efficacy of the model, an assessment of the area under the curve, sensitivity, specificity, accuracy, positive predictive value, and negative predictive value was conducted employing the validation dataset. Results: This research encompassed a total of 420 images from individuals. The screening accuracy and recall rate of the model surpass 90%. Conclusions: One of profound neural network models predicated on chest x-ray radiographs (a traditional, extensively employed, and economically viable examination) proves highly advantageous in the assessment for atrial septal defect. 2023 ZhixIin, Gang, Zhixian and Silin.},
	author_keywords = {artificial intelligence; atrial septal defect; chest x-ray; congenital heart disease; screening method},
	keywords = {Article; artificial intelligence; cardiologist; child; controlled study; deep learning; diagnostic accuracy; diagnostic test accuracy study; digital imaging; echocardiography; female; heart atrium septum defect; human; major clinical study; male; predictive value; preschool child; receiver operating characteristic; retrospective study; screening; sensitivity and specificity; thorax radiography; validation process},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Dolata2024123,
	author = {Dolata, Mateusz and Crowston, Kevin},
	title = {Making Sense of AI Systems Development},
	year = {2024},
	journal = {IEEE Transactions on Software Engineering},
	volume = {50},
	number = {1},
	pages = {123 – 140},
	doi = {10.1109/TSE.2023.3338857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179821823&doi=10.1109%2fTSE.2023.3338857&partnerID=40&md5=1c04825a70421f300fa45877b5be5ef6},
	abstract = {We identify and describe episodes of sensemaking around challenges in modern Artificial-Intelligence (AI)-based systems development that emerged in projects carried out by IBM and client companies. All projects used IBM Watson as the development platform for building tailored AI-based solutions to support workers or customers of the client companies. Yet, many of the projects turned out to be significantly more challenging than IBM and its clients had expected. The analysis reveals that project members struggled to establish reliable meanings about the technology, the project, context, and data to act upon. The project members report multiple aspects of the projects that they were not expecting to need to make sense of yet were problematic. Many issues bear upon the current-generation AI's inherent characteristics, such as dependency on large data sets and continuous improvement as more data becomes available. Those characteristics increase the complexity of the projects and call for balanced mindfulness to avoid unexpected problems.  © 1976-2012 IEEE.},
	author_keywords = {Artificial intelligence; empirical study; industry; social issues; software engineering; systems development},
	keywords = {Artificial intelligence; Job analysis; Probabilistic logics; Artificial intelligence systems; Cognition; Development platform; Empirical studies; Sense making; Social issues; Software; System development; Task analysis; Workers'; Software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Green Open Access}
}

@ARTICLE{Langford2024,
	author = {Langford, Michael Austin and Zilberman, Sol and Cheng, Betty},
	title = {Anunnaki: A Modular Framework for Developing Trusted Artificial Intelligence},
	year = {2024},
	journal = {ACM Transactions on Autonomous and Adaptive Systems},
	volume = {19},
	number = {3},
	doi = {10.1145/3649453},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203277578&doi=10.1145%2f3649453&partnerID=40&md5=03854a522cb9ecf0c4b249f0b751c7ab},
	abstract = {Trustworthy artificial intelligence (Trusted AI) is of utmost importance when learning-enabled components (LECs) are used in autonomous, safety-critical systems. When reliant on deep learning, these systems need to address the reliability, robustness, and interpretability of learning models. In addition to developing strategies to address these concerns, appropriate software architectures are needed to coordinate LECs and ensure they deliver acceptable behavior even under uncertain conditions. This work describes Anunnaki, a model-driven framework comprising loosely-coupled modular services designed to monitor and manage LECs with respect to Trusted AI assurance concerns when faced with different sources of uncertainty. More specifically, the Anunnaki framework supports the composition of independent, modular services to assess and improve the resilience and robustness of AI systems. The design of Annunaki was guided by several key software engineering principles (e.g., modularity, composability, and reusability) in order to facilitate its use and maintenance to support different aggregate monitoring and assurance analysis tools for LESs and their respective data sets. We demonstrate Anunnaki on two autonomous platforms, a terrestrial rover, and an unmanned aerial vehicle. Our studies show how Anunnaki can be used to manage the operations of different autonomous learning-enabled systems with vision-based LECs while exposed to uncertain environmental conditions.  © 2024 Copyright held by the owner/author(s).},
	author_keywords = {artificial intelligence; deep learning; models at run time; self-adaptive systems; Software engineering},
	keywords = {Adversarial machine learning; Computer software maintenance; Computer software reusability; Deep learning; Software architecture; Software reliability; Unmanned aerial vehicles (UAV); Deep learning; Developing strategy; Interpretability; Learning models; Models at run time; Modular framework; Modulars; Reliability robustness; Safety critical systems; Self-adaptive system; Reusability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Bronze Open Access}
}

@ARTICLE{John2023,
	author = {John, Meenu Mary and Olsson, Helena Holmström and Bosch, Jan},
	title = {Towards an AI-driven business development framework: A multi-case study},
	year = {2023},
	journal = {Journal of Software: Evolution and Process},
	volume = {35},
	number = {6},
	doi = {10.1002/smr.2432},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125909057&doi=10.1002%2fsmr.2432&partnerID=40&md5=81497923d97888427d6200fc1d3459cf},
	abstract = {Artificial intelligence (AI) and the use of machine learning (ML) and deep learning (DL) technologies are becoming increasingly popular in companies. These technologies enable companies to leverage big quantities of data to improve system performance and accelerate business development. However, despite the appeal of ML/DL, there is a lack of systematic and structured methods and processes to help data scientists and other company roles and functions to develop, deploy and evolve models. In this paper, based on multi-case study research in six companies, we explore practices and challenges practitioners experience in developing ML/DL models as part of large software-intensive embedded systems. Based on our empirical findings, we derive a conceptual framework in which we identify three high-level activities that companies perform in parallel with the development, deployment and evolution of models. Within this framework, we outline activities, iterations and triggers that optimize model design as well as roles and company functions. In this way, we provide practitioners with a blueprint for effectively integrating ML/DL model development into the business to achieve better results than other (algorithmic) approaches. In addition, we show how this framework helps companies solve the challenges we have identified and discuss checkpoints for terminating the business case. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley & Sons Ltd.},
	author_keywords = {AI-driven business development framework; artificial intelligence; challenges; deep learning; iterations and triggers; machine learning},
	keywords = {Embedded systems; Learning systems; Artificial intelligence-driven business development framework; Business development; Case-studies; Challenge; Deep learning; Development frameworks; Iteration and trigger; Learning models; Machine-learning; Software process; Deep learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Negri-Ribalta2024,
	author = {Negri-Ribalta, Claudia and Geraud-Stewart, Rémi and Sergeeva, Anastasia and Lenzini, Gabriele},
	title = {A systematic literature review on the impact of AI models on the security of code generation},
	year = {2024},
	journal = {Frontiers in Big Data},
	volume = {7},
	doi = {10.3389/fdata.2024.1386720},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194234884&doi=10.3389%2ffdata.2024.1386720&partnerID=40&md5=458280e395be23f2965893e094d881eb},
	abstract = {Introduction: Artificial Intelligence (AI) is increasingly used as a helper to develop computing programs. While it can boost software development and improve coding proficiency, this practice offers no guarantee of security. On the contrary, recent research shows that some AI models produce software with vulnerabilities. This situation leads to the question: How serious and widespread are the security flaws in code generated using AI models? Methods: Through a systematic literature review, this work reviews the state of the art on how AI models impact software security. It systematizes the knowledge about the risks of using AI in coding security-critical software. Results: It reviews what security flaws of well-known vulnerabilities (e.g., the MITRE CWE Top 25 Most Dangerous Software Weaknesses) are commonly hidden in AI-generated code. It also reviews works that discuss how vulnerabilities in AI-generated code can be exploited to compromise security and lists the attempts to improve the security of such AI-generated code. Discussion: Overall, this work provides a comprehensive and systematic overview of the impact of AI in secure coding. This topic has sparked interest and concern within the software security engineering community. It highlights the importance of setting up security measures and processes, such as code verification, and that such practices could be customized for AI-aided code production. Copyright © 2024 Negri-Ribalta, Geraud-Stewart, Sergeeva and Lenzini.},
	author_keywords = {artificial intelligence; code generation; programming; security; software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Faruqui2024195188,
	author = {Faruqui, Nuruzzaman and Thatoi, Priyabrata and Choudhary, Rohit and Roncevic, Ivana and Alqahtani, Hamed and Sarker, Iqbal H. and Khanam, Shapla},
	title = {AI-Analyst: An AI-Assisted SDLC Analysis Framework for Business Cost Optimization},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {195188 – 195203},
	doi = {10.1109/ACCESS.2024.3519423},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212665327&doi=10.1109%2fACCESS.2024.3519423&partnerID=40&md5=273e1d9a990b58d95d4f7f0e0ffc8808},
	abstract = {Managing the System Development Lifecycle (SDLC) is a complex task because of its involvement in coordinating diverse activities, stakeholders, and resources while ensuring project goals are met efficiently. The complex nature of the SDLC process leaves plenty of scope for human error, which impacts the overall business cost. This paper introduces AI-Analyst, an AI-assisted framework developed using the transformer-based model with more than 150 million parameters to assist with SDLC management. It minimizes manual effort errors, optimizes resource allocation, and improves decision-making processes, resulting in substantial cost savings. The statistical analysis shows that it saves around 53.33% of costs in an experimental project. The transformer model has been trained with a uniquely prepared dataset tailored for SDLC through transfer learning. It achieved impressive results, with an accuracy of 91.5%, precision of 91.9%, recall of 91.3%, and an F1-score of 91.5%, demonstrating its high reliability and performance. The perplexity score of 15 further indicates the model's strong language understanding capabilities to retrieve relations from complex characteristics of Natural Language Processing (NLP). The AI-Analyst framework represents a significant advancement in integrating Large Language Models (LLMs) into SDLC, offering a scalable and cost-effective solution for optimizing business processes.  © 2013 IEEE.},
	author_keywords = {AI; artificial intelligence; business cost optimization; large language model; LLM; PMP; project management automation; SDLC; system analyst; system development lifecycle; transfer learning; Transformer model},
	keywords = {Cost benefit analysis; Project management; Resource allocation; Transfer learning; Business cost optimization; Costs Optimization; Language model; Large language model; Management automations; PMP; Project management automation; System development; System development lifecycle; Systems analysts; Transfer learning; Transformer modeling; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access}
}

@ARTICLE{Mishra202550226,
	author = {Mishra, Lalit Narayan and Senapati, Biswaranjan},
	title = {Retail Resilience Engine: An Agentic AI Framework for Building Reliable Retail Systems With Test-Driven Development Approach},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {50226 – 50243},
	doi = {10.1109/ACCESS.2025.3552592},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001557240&doi=10.1109%2fACCESS.2025.3552592&partnerID=40&md5=bca2abec383eafb60c92e48878692a51},
	abstract = {System reliability and operational resilience are two critical success factors in the retail industry that are directly connected to customer satisfaction and business sustainability. Staying competitive in today's dynamic and rapidly evolving market requires rapid adaptability. However, it contradicts the reliability and resilience. This paper proposes an innovative solution, the Retail Resilience Engine (RRE), to establish a balance between these success factors and market demand. It is a unique framework that combines Test-Driven Development (TDD) with a Large Language Model (LLM). This framework follows the state-of-the-art Agentic-AI architecture. It effectively evaluates the decision-making process at rapid speed in retail by incorporating diverse factors, including inventory management, demand forecasting, and customer feedback. As a result, the system reliability is improved significantly. The experimental analysis of the proposed framework shows its decision-making is similar to human experts with a similarity index of 97.5%. It further proves the reliability of the system. The framework also scales effectively, maintaining high accuracy, precision, recall, and F1 scores across varying dataset sizes. The robustness analysis of the system demonstrates the agility enhancement across diverse retail domains, ensuring consistent performance with accuracy exceeding 90% across all tested scenarios. The integration of a creative filtering mechanism further enhances the performance of the RRE framework by preventing 98.2% of the irrelevant inputs. Overall, the proposed RRE framework demonstrates the impressive potential to transform retail systems by enhancing reliability, scalability, and decision-making quality through an Agentic-AI approach.  © 2025 IEEE.},
	author_keywords = {agentic-AI; artificial intelligence; large language model; resilience engine; Retail systems; test-driven development},
	keywords = {Commerce; Competition; Customer satisfaction; Decision making; Quality control; Reliability; Sales; Agentic-AI; Decisions makings; Development approach; Language model; Large language model; Resilience engine; Retail system; Success factors; System reliability; Test driven development; Inventory control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Langford20231543,
	author = {Langford, Michael Austin and Chan, Kenneth H. and Fleck, Jonathon Emil and McKinley, Philip K. and Cheng, Betty H. C.},
	title = {MoDALAS: addressing assurance for learning-enabled autonomous systems in the face of uncertainty},
	year = {2023},
	journal = {Software and Systems Modeling},
	volume = {22},
	number = {5},
	pages = {1543 – 1563},
	doi = {10.1007/s10270-023-01090-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150260706&doi=10.1007%2fs10270-023-01090-9&partnerID=40&md5=bc4e8303f240cd482e29646d5266be10},
	abstract = {Increasingly, safety-critical systems include artificial intelligence and machine learning components (i.e., learning-enabled components (LECs)). However, when behavior is learned in a training environment that fails to fully capture real-world phenomena, the response of an LEC to untrained phenomena is uncertain and therefore cannot be assured as safe. Automated methods are needed for self-assessment and adaptation to decide when learned behavior can be trusted. This work introduces a model-driven approach to manage self-adaptation of a learning-enabled system (LES) to account for run-time contexts for which the learned behavior of LECs cannot be trusted. The resulting framework enables an LES to monitor and evaluate goal models at run time to determine whether or not LECs can be expected to meet functional objectives and enables system adaptation accordingly. Using this framework enables stakeholders to have more confidence that LECs are used only in contexts comparable to those validated at design time. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {Artificial intelligence; Autonomous vehicles; Behavior oracles; Cyber physical systems; Goal-based modeling; Machine learning; Models at run time; Self-adaptive systems},
	keywords = {Adaptive systems; Autonomous vehicles; Embedded systems; Learning systems; Machine components; Machine learning; Safety engineering; Uncertainty analysis; Autonomous Vehicles; Behavior oracle; Cybe-physical systems; Cyber-physical systems; Goal-based models; Machine-learning; Models at run time; Self- adaptations; Self-adaptive system; Uncertainty; Cyber Physical System},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Qin2023,
	author = {Qin, Jieyao and Lu, Mingxi and Li, Bin and Li, Xiaorui and You, Guangming and Tan, Linjian and Zhai, Yikui and Huang, Meilin and Wu, Yingzhu},
	title = {A Rapid Quantitative Analysis of Bicomponent Fibers Based on Cross-Sectional In-Situ Observation},
	year = {2023},
	journal = {Polymers},
	volume = {15},
	number = {4},
	doi = {10.3390/polym15040842},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148950178&doi=10.3390%2fpolym15040842&partnerID=40&md5=eecd0879ee935fe75274ede9601a7314},
	abstract = {To accelerate the industrialization of bicomponent fibers, fiber-based flexible devices, and other technical fibers and to protect the property rights of inventors, it is necessary to develop fast, economical, and easy-to-test methods to provide some guidance for formulating relevant testing standards. A quantitative method based on cross-sectional in-situ observation and image processing was developed in this study. First, the cross-sections of the fibers were rapidly prepared by the non-embedding method. Then, transmission and reflection metallographic microscopes were used for in-situ observation and to capture the cross-section images of fibers. This in-situ observation allows for the rapid identification of the type and spatial distribution structure of the bicomponent fiber. Finally, the mass percentage content of each component was calculated rapidly by AI software according to its density, cross-section area, and total test samples of each component. By comparing the ultra-depth of field microscope, differential scanning calorimetry (DSC), and chemical dissolution method, the quantitative analysis was fast, accurate, economical, simple to operate, energy-saving, and environmentally friendly. This method will be widely used in the intelligent qualitative identification and quantitative analysis of bicomponent fibers, fiber-based flexible devices, and blended textiles. © 2023 by the authors.},
	author_keywords = {artificial intelligence (AI); bicomponent fibers; cross-sectional in-situ observation; image processing; melting and dissolving; quantitative analysis},
	keywords = {Differential scanning calorimetry; Energy conservation; Image analysis; Software testing; Textiles; Artificial intelligence; Bi-component fibers; Cross-sectional in-situ observation; Flexible device; Images processing; In-situ observations; Industrialisation; Melting and dissolving; Property right; Technical fibers; Fibers},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ahmad202554561,
	author = {Ahmad, Azeem and Sun, Xin and Naeem, Muhammad Rashid and Javed, Yasir and Akour, Mohammad and Sandahl, Kristian},
	title = {Understanding Flaky Tests Through Linguistic Diversity: A Cross-Language and Comparative Machine Learning Study},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {54561 – 54584},
	doi = {10.1109/ACCESS.2025.3553626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002263225&doi=10.1109%2fACCESS.2025.3553626&partnerID=40&md5=a6d7fb74b8db1f708753dcf0893338d1},
	abstract = {Software development is significantly impeded by flaky tests, which intermittently pass or fail without requiring code modifications, resulting in a decline in confidence in automated testing frameworks. Code smells (i.e., test case or production code) are the primary cause of test flakiness. In order to ascertain the prevalence of test smells, researchers and practitioners have examined numerous programming languages. However, one isolated experiment was conducted, which focused solely on one programming language. Across a variety of programming languages, such as Java, Python, C++, Go, and JavaScript, this study examines the predictive accuracy of a variety of machine learning classifiers in identifying flaky tests. We compare the performance of classifiers such as Random Forest, Decision Tree, Naive Bayes, Support Vector Machine, and Logistic Regression in both single-language and cross-language settings. In order to ascertain the impact of linguistic diversity on the flakiness of test cases, models were trained on a single language and subsequently tested on a variety of languages. The following key findings indicate that Random Forest and Logistic Regression consistently outperform other classifiers in terms of accuracy, adaptability, and generalizability, particularly in cross-language environments. Additionally, the investigation contrasts our findings with those of previous research, exhibiting enhanced precision and accuracy in the identification of flaky tests as a result of meticulous classifier selection. We conducted a thorough statistical analysis, which included t-tests, to assess the importance of classifier performance differences in terms of accuracy and F1-score across a variety of programming languages. This analysis emphasizes the substantial discrepancies between classifiers and their effectiveness in detecting flaky tests. The datasets and experiment code utilized in this study are accessible through an open source GitHub repository to facilitate reproducibility is available at: https://github.com/PELAB-LiU/FlakyCrossLanguage. Our results emphasize the effectiveness of probabilistic and ensemble classifiers in improving the reliability of automated testing, despite certain constraints, including the potential biases introduced by language-specific structures and dataset variability. This research provides developers and researchers with practical insights that can be applied to the mitigation of flaky tests in a variety of software environments. © 2013 IEEE.},
	author_keywords = {artificial intelligence; Flaky tests; machine learning; non-deterministic tests; software testing},
	keywords = {C++ (programming language); Computer software selection and evaluation; Decision trees; Java programming language; Logistic regression; Open source software; Problem oriented languages; Software reliability; Support vector regression; Automated testing; Cross languages; Deterministics; Flaky test; Linguistic diversity; Machine-learning; Non-deterministic test; Random forests; Software testings; Test case; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Idrisov2024,
	author = {Idrisov, Baskhad and Schlippe, Tim},
	title = {Program Code Generation with Generative AIs},
	year = {2024},
	journal = {Algorithms},
	volume = {17},
	number = {2},
	doi = {10.3390/a17020062},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185823884&doi=10.3390%2fa17020062&partnerID=40&md5=1be981ce59c3e8f52932d798e2f3b0e7},
	abstract = {Our paper compares the correctness, efficiency, and maintainability of human-generated and AI-generated program code. For that, we analyzed the computational resources of AI- and human-generated program code using metrics such as time and space complexity as well as runtime and memory usage. Additionally, we evaluated the maintainability using metrics such as lines of code, cyclomatic complexity, Halstead complexity and maintainability index. For our experiments, we had generative AIs produce program code in Java, Python, and C++ that solves problems defined on the competition coding website leetcode.com. We selected six LeetCode problems of varying difficulty, resulting in 18 program codes generated by each generative AI. GitHub Copilot, powered by Codex (GPT-3.0), performed best, solving 9 of the 18 problems (50.0%), whereas CodeWhisperer did not solve a single problem. BingAI Chat (GPT-4.0) generated correct program code for seven problems (38.9%), ChatGPT (GPT-3.5) and Code Llama (Llama 2) for four problems (22.2%) and StarCoder and InstructCodeT5+ for only one problem (5.6%). Surprisingly, although ChatGPT generated only four correct program codes, it was the only generative AI capable of providing a correct solution to a coding problem of difficulty level hard. In summary, 26 AI-generated codes (20.6%) solve the respective problem. For 11 AI-generated incorrect codes (8.7%), only minimal modifications to the program code are necessary to solve the problem, which results in time savings between 8.9% and even 71.3% in comparison to programming the program code from scratch. © 2024 by the authors.},
	author_keywords = {AI program code generation; artificial intelligence; generative AIs; program code efficiency; program code maintainability},
	keywords = {C++ (programming language); Codes (symbols); Computer software; Maintainability; AI program code generation; Computational resources; Generative AI; Program code; Program code efficiency; Program code maintainability; Program-code generation; Runtime and memory usage; Time and space complexity; Efficiency},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access, Gold Open Access}
}

@ARTICLE{Soeffner20242213,
	author = {Soeffner, Jan},
	title = {Meaning–thinking–AI},
	year = {2024},
	journal = {AI and Society},
	volume = {39},
	number = {5},
	pages = {2213 – 2220},
	doi = {10.1007/s00146-023-01709-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163079832&doi=10.1007%2fs00146-023-01709-x&partnerID=40&md5=e7db554f811c77b09793968f33cbbb76},
	abstract = {This paper makes the case for a sharper terminology regarding AIs cognitive abilities. In arguing that thinking requires more than content production, I offer a definition of meaning drawing on a clear distinction between living and machine intelligence. A pivotal argument is the re-use of the Turing Test (TT) for understanding which theories of meaning and consciousness are no longer plausible—because they have been reproduced by software without thereby gaining conscious experience. In following the few theories that have not (yet) failed this reversed Turing Test (RTT), the focus turns towards rethinking the human condition in times of AI along the lines of three questions: What if a machine developed consciousness? What if AI proceeded without developing a consciousness? What, if machinic and human intelligence merged? These three questions in the end lead to examining three related possible futures of humanism as now determined by the relation between Human Intelligence and AI. © The Author(s) 2023.},
	author_keywords = {Artificial intelligence; Consciousness; Humanism; Meaning; Singularity; Turing test},
	keywords = {Software testing; Cognitive ability; Consciousness; Content production; Human intelligence; Humanism; Machine intelligence; Meaning; Singularity; Theory of meanings; Turing tests; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Pesqueira2024,
	author = {Pesqueira, Antonio and de Bem Machado, Andreia and Bolog, Sama and Sousa, Maria José and Pereira, Rúben},
	title = {Exploring the impact of EU tendering operations on future AI governance and standards in pharmaceuticals},
	year = {2024},
	journal = {Computers and Industrial Engineering},
	volume = {198},
	doi = {10.1016/j.cie.2024.110655},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207654327&doi=10.1016%2fj.cie.2024.110655&partnerID=40&md5=515d7b23a68b32348cff082b79d8503e},
	abstract = {This research examines the incorporation of artificial intelligence (AI) into the domain of tender management (TM) within the pharmaceutical industry, with a particular emphasis on operational efficiency, governance, and compliance with European regulatory standards. A comparative analysis of four companies—two that have adopted AI and two that have not—reveals significant discrepancies in the management of TM processes between AI-driven and traditional companies. The study employs the Delphi method to ascertain expert consensus on eight critical areas of AI governance, including data privacy, transparency, and ethical AI use. The findings indicate that companies integrating AI demonstrate enhanced decision-making capabilities, accelerated processing times, and enhanced stakeholder engagement. However, they also encounter challenges pertaining to ethical governance and regulatory compliance. The research highlights the necessity of aligning the adoption of AI with the latest European directives, such as the AI Act and General Data Protection Regulation (GDPR), to ensure both operational efficiency and adherence to ethical standards. The broader implications of the study underscore the necessity for pharmaceutical companies to develop robust governance frameworks, prioritize ethical considerations, and maintain regulatory compliance to fully leverage the potential of AI. Additionally, the study contributes to the ongoing scholarly discourse by providing empirical evidence on the interplay between AI, ethics, and governance, thereby encouraging further interdisciplinary research. This work emphasizes the critical role of strategic AI adoption in maintaining competitive advantage while safeguarding societal trust and adhering to legal requirements. © 2024 The Author(s)},
	author_keywords = {Artificial Intelligence (AI); Ethics; Governance; Operational Efficiency; Pharmaceutical Industry; Tender Management (TM)},
	keywords = {Data privacy; Decision making; Efficiency; Artificial intelligence; Comparative analyzes; Decisions makings; Delphi method; Governance; Management process; Operational efficiencies; Pharmaceutical industry; Regulatory standards; Tender management; Competition},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Yasheng2025,
	author = {Yasheng, Paierhati and Yusufu, Alimujiang and Yimiti, Yasenjiang and Luan, Haopeng and Peng, Cong and Song, Xinghua},
	title = {Web-based machine learning application for interpretable prediction of prolonged length of stay after lumbar spinal stenosis surgery: a retrospective cohort study with explainable AI},
	year = {2025},
	journal = {Frontiers in Physiology},
	volume = {16},
	doi = {10.3389/fphys.2025.1542240},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219602045&doi=10.3389%2ffphys.2025.1542240&partnerID=40&md5=ba5b09eea523e6cf2b23a62d46a2d1ae},
	abstract = {Objectives: Lumbar spinal stenosis (LSS) is an increasingly important issue related to back pain in elderly patients, resulting in significant socioeconomic burdens. Postoperative complications and socioeconomic effects are evaluated using the clinical parameter of hospital length of stay (LOS). This study aimed to develop a machine learning-based tool that can calculate the risk of prolonged length of stay (PLOS) after surgery and interpret the results. Methods: Patients were registered from the spine surgery department in our hospital. Hospital stays greater than or equal to the 75th percentile for LOS was considered extended PLOS after spine surgery. We screened the variables using the least absolute shrinkage and selection operator (LASSO) and permutation importance value and selected nine features. We then performed hyperparameter selection via grid search with nested cross-validation. Receiver operating characteristics curve, calibration curve and decision curve analysis was carried out to assess model performance. The result of the final selected model was interpreted using Shapley Additive exPlanations (SHAP), and Local Interpretable Model-agnostic Explanations (LIME) were used for model interpretation. To facilitate model utilization, a web application was deployed. Results: A total of 540 patients were involved, and several features were finally selected. The final optimal random forest (RF) model achieved an area under the curve (ROC) of 0.93 on the training set and 0.83 on the test set. Based on both SHAP and LIME analyses, intraoperative blood loss emerged as the most significant contributor to the outcome. Conclusion: Machine learning in association with SHAP and LIME can provide a clear explanation of personalized risk prediction, and spine surgeons can gain a perceptual grasp of the impact of important model components. Utilization and future clinical research of our RF model are made simple and accessible through the web application. Copyright © 2025 Yasheng, Yusufu, Yimiti, Luan, Peng and Song.},
	author_keywords = {interpretable model; lumbar spinal stenosis; postoperative length of stay; SHAP value; spine surgery},
	keywords = {alanine aminotransferase; albumin; alkaline phosphatase; aspartate aminotransferase; C reactive protein; adult; area under the curve; Article; artificial intelligence; body mass; calibration; cohort analysis; controlled study; cross validation; diagnostic test accuracy study; discectomy; erythrocyte sedimentation rate; explainable artificial intelligence; female; hand strength; human; k nearest neighbor; least absolute shrinkage and selection operator; length of stay; leukocyte count; local interpretable model agnostic explanation; lumbar spinal stenosis; machine learning; major clinical study; male; nested cross validation; nuclear magnetic resonance imaging; prediction; random forest; receiver operating characteristic; retrospective study; Shapley additive explanation; spine malformation; support vector machine},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Gallofré Ocaña2023,
	author = {Gallofré Ocaña, Marc and Opdahl, Andreas L.},
	title = {A Software Reference Architecture for Journalistic Knowledge Platforms},
	year = {2023},
	journal = {Knowledge-Based Systems},
	volume = {276},
	doi = {10.1016/j.knosys.2023.110750},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164699156&doi=10.1016%2fj.knosys.2023.110750&partnerID=40&md5=4252d815319ccda56400e04ccab3e845},
	abstract = {Newsrooms and journalists today rely on many different artificial-intelligence, big-data and knowledge-based systems to support efficient and high-quality journalism. However, making the different systems work together remains a challenge, calling for new unified journalistic knowledge platforms. A software reference architecture for journalistic knowledge platforms could help news organisations by capturing tried-and-tested best practices and providing a generic blueprint for how their IT infrastructure should evolve. To the best of our knowledge, no suitable architecture has been proposed in the literature. Therefore, this article proposes a software reference architecture for integrating artificial intelligence and knowledge bases to support journalists and newsrooms. The design of the proposed architecture is grounded on the research literature and on our experiences with developing a series of prototypes in collaboration with industry. Our aim is to make it easier for news organisations to evolve their existing independent systems for news production towards integrated knowledge platforms and to direct further research. Because journalists and newsrooms are early adopters of integrated knowledge platforms, our proposal can hopefully also inform architectures in other domains with similar needs. © 2023 The Author(s)},
	author_keywords = {Artificial intelligence; Big data; Journalism; Knowledge graphs; Newsrooms; Software reference architecture},
	keywords = {Blueprints; Knowledge graph; Software testing; Best practices; Collaboration with industries; High quality; IT infrastructures; Journalism; Knowledge graphs; Knowledge-based systems; Newsroom; Proposed architectures; Software reference architectures; Big data},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Karimi2024,
	author = {Karimi, Meysam and Kolahdouz-Rahimi, Shekoufeh and Troya, Javier},
	title = {Ant-colony optimization for automating test model generation in model transformation testing},
	year = {2024},
	journal = {Journal of Systems and Software},
	volume = {208},
	doi = {10.1016/j.jss.2023.111882},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177730716&doi=10.1016%2fj.jss.2023.111882&partnerID=40&md5=cbe0dfb8462bd79877c0c39a253f8cbb},
	abstract = {In model transformation (MT) testing, test data generation is of key importance. However, test suites are not available out of the box, and existing approaches to generate them require to provide not only the metamodel to which the models must conform, but some other domain-specific artifacts. For instance, an MT developer aiming to perform an incremental implementation of an MT may need to count on a quality test suite from the very beginning, even before all MT requirements are clear, only having the metamodels as input. We propose a black-box approach for the generation of test models where only the input metamodel of the MT is available. We propose an Ant-Colony Optimization algorithm for the search of test models satisfying the objectives of maximizing internal diversity and maximizing external diversity. We provide a tool prototype that implements this approach and generates the models in the well-established XMI interchange format. A comparison study with state-of-the-art frameworks shows that models are generated in reasonable times with low memory consumption. We empirically demonstrate the adequacy of our approach to generate effective test models, obtaining an overall mutation score above 80% from an evaluation with more than 5000 MT mutants. © 2023 Elsevier Inc.},
	author_keywords = {Ant colony optimization; Automated model generation; Model transformation testing; Model-Driven Engineering},
	keywords = {Artificial intelligence; Metadata; Automated model generations; Domain specific; Meta model; Model generation; Model transformation; Model transformation testing; Model-driven Engineering; Quality test; Test data generation; Test models; Ant colony optimization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Green Open Access}
}

@ARTICLE{Hariyanti2023305,
	author = {Hariyanti, Eva and Janeswari, Made Balin and Moningka, Malvin Mikhael and Aziz, Fikri Maulana and Putri, Annisa Rahma and Hapsari, Oxy Setyo and Sutha, Nyoman Agus Arya Dwija and Sinaga, Yohannes Alexander Agusti and Bendesa, Manik Prasanthi},
	title = {Implementations of Artificial Intelligence in Various Domains of IT Governance: A Systematic Literature Review},
	year = {2023},
	journal = {Journal of Information Systems Engineering and Business Intelligence},
	volume = {9},
	number = {2},
	pages = {305 – 319},
	doi = {10.20473/jisebi.9.2.305-319},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176096021&doi=10.20473%2fjisebi.9.2.305-319&partnerID=40&md5=fe4f0bfe874e3b51528b15eac99ec568},
	abstract = {Background: Artificial intelligence (AI) has become increasingly prevalent in various industries, including IT governance. By integrating AI into the governance environment, organizations can benefit from the consolidation of frameworks and best practices. However, the adoption of AI across different stages of the governance process is unevenly distributed. Objective: The primary objective of this study is to perform a systematic literature review on applying artificial intelligence (AI) in IT governance processes, explicitly focusing on the Deming cycle. This study overlooks the specific details of the AI methods used in the various stages of IT governance processes. Methods: The search approach acquires relevant papers from Elsevier, Emerald, Google Scholar, Springer, and IEEE Xplore. The obtained results were then filtered using predefined inclusion and exclusion criteria to ensure the selection of relevant studies. Results: The search yielded 359 papers. Following our inclusion and exclusion criteria, we pinpointed 42 primary studies that discuss how AI is implemented in every domain of IT Governance related to the Deming cycle. Conclusion: We found that AI implementation is more dominant in the plan, do, and check stages of the Deming cycle, with a particular emphasis on domains such as risk management, strategy alignment, and performance measurement since most AI applications are not able to perform well in different contexts as well as the other usage driven by its unique capabilities. © 2023 The Authors. Published by Universitas Airlangga.},
	author_keywords = {Artificial Intelligence; Deming cycle; Governance; IT Governance domain; Systematic literature review},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Chalutz Ben-Gal2023,
	author = {Chalutz Ben-Gal, Hila},
	title = {Artificial intelligence (AI) acceptance in primary care during the coronavirus pandemic: What is the role of patients' gender, age and health awareness? A two-phase pilot study},
	year = {2023},
	journal = {Frontiers in Public Health},
	volume = {10},
	doi = {10.3389/fpubh.2022.931225},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146854087&doi=10.3389%2ffpubh.2022.931225&partnerID=40&md5=62d0af8aaddf989dfc55553ab92ad504},
	abstract = {Background: Artificial intelligence (AI) is steadily entering and transforming the health care and Primary Care (PC) domains. AI-based applications assist physicians in disease detection, medical advice, triage, clinical decision-making, diagnostics and digital public health. Recent literature has explored physicians' perspectives on the potential impact of digital public health on key tasks in PC. However, limited attention has been given to patients' perspectives of AI acceptance in PC, specifically during the coronavirus pandemic. Addressing this research gap, we administered a pilot study to investigate criteria for patients' readiness to use AI-based PC applications by analyzing key factors affecting the adoption of digital public health technology. Methods: The pilot study utilized a two-phase mixed methods approach. First, we conducted a qualitative study with 18 semi-structured interviews. Second, based on the Technology Readiness and Acceptance Model (TRAM), we conducted an online survey (n = 447). Results: The results indicate that respondents who scored high on innovativeness had a higher level of readiness to use AI-based technology in PC during the coronavirus pandemic. Surprisingly, patients' health awareness and sociodemographic factors, such as age, gender and education, were not significant predictors of AI-based technology acceptance in PC. Conclusions: This paper makes two major contributions. First, we highlight key social and behavioral determinants of acceptance of AI-enabled health care and PC applications. Second, we propose that to increase the usability of digital public health tools and accelerate patients' AI adoption, in complex digital public health care ecosystems, we call for implementing adaptive, population-specific promotions of AI technologies and applications. Copyright © 2023 Chalutz Ben-Gal.},
	author_keywords = {artificial intelligence; coronavirus pandemic; digital public health; health awareness; pilot study; primary care},
	keywords = {Artificial Intelligence; Ecosystem; Humans; Pandemics; Pilot Projects; Primary Health Care; artificial intelligence; ecosystem; human; pandemic; pilot study; primary health care},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Drosos2024,
	author = {Drosos, Georgios-Petros and Sotiropoulos, Thodoris and Alexopoulos, Georgios and Mitropoulos, Dimitris and Su, Zhendong},
	title = {When Your Infrastructure Is a Buggy Program: Understanding Faults in Infrastructure as Code Ecosystems},
	year = {2024},
	journal = {Proceedings of the ACM on Programming Languages},
	volume = {8},
	number = {OOPSLA2},
	doi = {10.1145/3689799},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207087667&doi=10.1145%2f3689799&partnerID=40&md5=1ea74c42d551ab00457b1932f695e18b},
	abstract = {Modern applications have become increasingly complex and their manual installation and configuration is no longer practical. Instead, IT organizations heavily rely on Infrastructure as Code (IaC) technologies, to automate the provisioning, configuration, and maintenance of computing infrastructures and systems. IaC systems typically offer declarative, domain-specific languages (DSLs) that allow system administrators and developers to write high-level programs that specify the desired state of their infrastructure in a reliable, predictable, and documented fashion. Just like traditional programs, IaC software is not immune to faults, with issues ranging from deployment failures to critical misconfigurations that often impact production systems used by millions of end users. Surprisingly, despite its crucial role in global infrastructure management, the tooling and techniques for ensuring IaC reliability still have room for improvement. In this work, we conduct a comprehensive analysis of 360 bugs identified in IaC software within prominent IaC ecosystems including Ansible, Puppet, and Chef. Our work is the first in-depth exploration of bug characteristics in these widely-used IaC environments. Through our analysis we aim to understand: (1) how these bugs manifest, (2) their underlying root causes, (3) their reproduction requirements in terms of system state (e.g., operating system versions) or input characteristics, and (4) how these bugs are fixed. Based on our findings, we evaluate the state-of-the-art techniques for IaC reliability, identify their limitations, and provide a set of recommendations for future research. We believe that our study helps researchers to (1) better understand the complexity and peculiarities of IaC software, and (2) develop advanced tooling for more reliable and robust system configurations.  © 2024 Owner/Author.},
	author_keywords = {Ansible; bug; Chef; deployment; IaC; infrastructure as code; Puppet; testing},
	keywords = {Application programs; Artificial intelligence; Computer debugging; Infrastructure as a service (IaaS); Problem oriented languages; Program debugging; Reliability analysis; Software testing; Ansible; Bug; Chef; Deployment; Infrastructure as code; IT organizations; Modern applications; Program understanding; Puppet; Software reliability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Jasmine Lizy2023138,
	author = {Jasmine Lizy, P. and Chenthalir Indra, N.},
	title = {Outlier detection based energy efficient and reliable routing protocol using deep learning algorithm},
	year = {2023},
	journal = {Cognitive Computation and Systems},
	volume = {5},
	number = {2},
	pages = {138 – 152},
	doi = {10.1049/ccs2.12083},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166738278&doi=10.1049%2fccs2.12083&partnerID=40&md5=488d1692cd97622b0f7724bb161a23ed},
	abstract = {Wireless sensor network have also played a vital role in the observation and management of agricultural land in terms of climate, water usage, crops, etc. Due to the open communication system and low battery power of sensors, the agricultural sector still faces issues with energy consumption, information forwarding, and privacy. Thus, an energy-efficient routing during transmission in WSN-based smart agriculture is suggested in this study applying a feed-forward neural network to detect outliers. Outlier identification, CH-selection, and Relay Node (RN) selection are the three phases of this suggested method. Outlier detection is performed in the deployed nodes for categorises attack nodes from the normal nodes. CH-selection is performed using a chaotic moth-flame optimization technique according to distance, node degree, centrality factor and residual energy level, these parameters determine which node will become a Cluster Head. Then reliable routing protocol is designed using NB-based probability method for RN selection. MATLAB software is used to test the proposed Outlier Detection based Energy Efficient and Reliable Routing Protocol and verify its performance. The effectiveness of the proposed-model is tested with some prior wireless sensor network routing protocols environment-fusion multipath routing protocol, dynamic Multi-hop Energy Efficient Routing Protocol, SEMantic CLustering, and Reliable and energy efficient routing protocol. Outlier Detection based Energy Efficient and Reliable Routing Protocol algorithm attained a 0.91 (%)Packet Delivery ratio, 0.08% of packet loss, 0.91% of Average residual energy, 2.8 (Mbps) throughput, and 26 (sec) Delay. © 2023 The Authors. Cognitive Computation and Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Shenzhen University.},
	author_keywords = {artificial intelligence; fuzzy control},
	keywords = {Anomaly detection; Data communication systems; Data handling; Deep learning; Energy efficiency; Energy utilization; Errors; Feedforward neural networks; Fuzzy control; Information management; Power management (telecommunication); Routing protocols; Semantics; Sensor nodes; Software testing; Agricultural land; Agricultural sector; Battery power; Communications systems; Energy efficient routing protocol; Open communication; Outlier Detection; Relay node selections; Reliable routing protocols; Water usage; MATLAB},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Meng2024,
	author = {Meng, Xiangbin and Yan, Xiangyu and Zhang, Kuo and Liu, Da and Cui, Xiaojuan and Yang, Yaodong and Zhang, Muhan and Cao, Chunxia and Wang, Jingjia and Wang, Xuliang and Gao, Jun and Wang, Yuan-Geng-Shuo and Ji, Jia-ming and Qiu, Zifeng and Li, Muzi and Qian, Cheng and Guo, Tianze and Ma, Shuangquan and Wang, Zeying and Guo, Zexuan and Lei, Youlan and Shao, Chunli and Wang, Wenyao and Fan, Haojun and Tang, Yi-Da},
	title = {The application of large language models in medicine: A scoping review},
	year = {2024},
	journal = {iScience},
	volume = {27},
	number = {5},
	doi = {10.1016/j.isci.2024.109713},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192333534&doi=10.1016%2fj.isci.2024.109713&partnerID=40&md5=ef3aa8c4dbce9bdd4d3619a93fe3f73c},
	abstract = {This study systematically reviewed the application of large language models (LLMs) in medicine, analyzing 550 selected studies from a vast literature search. LLMs like ChatGPT transformed healthcare by enhancing diagnostics, medical writing, education, and project management. They assisted in drafting medical documents, creating training simulations, and streamlining research processes. Despite their growing utility in assisted diagnosis and improving doctor-patient communication, challenges persisted, including limitations in contextual understanding and the risk of over-reliance. The surge in LLM-related research indicated a focus on medical writing, diagnostics, and patient communication, but highlighted the need for careful integration, considering validation, ethical concerns, and the balance with traditional medical practice. Future research directions suggested a focus on multimodal LLMs, deeper algorithmic understanding, and ensuring responsible, effective use in healthcare. © 2024 The Authors},
	author_keywords = {Artificial intelligence; Health informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 66; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{AlSobeh2024121,
	author = {AlSobeh, Anas and Shatnawi, Amani and Al-Ahmad, Bilal and Aljmal, Alhan and Khamaiseh, Samer},
	title = {AI-Powered AOP: Enhancing Runtime Monitoring with Large Language Models and Statistical Learning},
	year = {2024},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {15},
	number = {11},
	pages = {121 – 133},
	doi = {10.14569/IJACSA.2024.0151113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000671274&doi=10.14569%2fIJACSA.2024.0151113&partnerID=40&md5=f74255f8948c723e915055b12be55305},
	abstract = {Modern software systems must adapt to dynamic artificial intelligence (AI) environments and evolving requirements. Aspect-oriented programming (AOP) effectively isolates crosscutting concerns (CCs) into single modules called aspects, enhancing quality metrics, and simplifying testing. However, AOP implementation can lead to unexpected program outputs and behavior changes. This paper proposes an AI-enhanced, adaptive monitoring framework for validating program behaviors during aspect weaving that integrates AOP interfaces (AOPIs) with large language models (LLMs), i.e. GPT-Codex AI, to dynamically generate and optimize monitoring aspects and statistical models in realtime. This enables intelligent run-time analysis, adaptive model checking, and natural language (NL) interaction. We tested the framework on ten diverse Java classes from JHotdraw 7.6 by extracting context and numerical data and building a dataset for analysis. By dynamically refining aspects and models based on observed behavior, its results showed that the framework maintained the integrity of the Java OOP class while providing predictive insights into potential conflicts and optimizations. Results demonstrate the framework’s efficacy in detecting subtle behavioral changes induced by aspect weaving, with a 94% accuracy in identifying potential conflicts and a 37% reduction in false positives compared to traditional static analysis techniques. Furthermore, the integration of explainable AI provides developers with clear, actionable explanations for flagged behaviors through NL interfaces, enhancing interpretability and trust in the system. © (2024), (Science and Information Organization). All Rights Reserved.},
	author_keywords = {Artificial Intelligence (AI); Aspect-Oriented Programming (AOP); Codex AI; cross-cutting concerns; dynamic program analysis; joinpoints; Large Language Models (LLMs); pointcut; runtime monitoring; software validation; statistical model checking},
	keywords = {Java programming language; Model checking; Natural language processing systems; Software testing; Artificial intelligence; Aspect-oriented; Aspect-oriented programming; Codex artificial intelligence; Crosscutting concern; Dynamic program analysis; Joinpoint; Language model; Large language model; Point cut; Runtime Monitoring; Software validation; Statistical model checking; Aspect oriented programming},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Mustaqeem2025866,
	author = {Mustaqeem, Mohd and Alam, Mahfooz and Mustajab, Suhel and Alshanketi, Faisal and Alam, Shadab and Shuaib, Mohammed},
	title = {Comprehensive Bibliographic Survey and Forward-Looking Recommendations for Software Defect Prediction: Datasets, Validation Methodologies, Prediction Approaches, and Tools},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {866 – 903},
	doi = {10.1109/ACCESS.2024.3517419},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212392066&doi=10.1109%2fACCESS.2024.3517419&partnerID=40&md5=56214816e90efaecc477d3a9d3eb457a},
	abstract = {The development of reliable software depends heavily on the effective collaboration between teams responsible for development and testing. Despite ongoing efforts, many software programs still contain bugs that can lead to financial losses and business risks. Therefore, detecting and fixing software defects after release is crucial. While binary classification methods have been commonly used for this purpose, recent Artificial Intelligence (AI) advancements offer new opportunities for software teams to create more robust software. To address challenges in Software Defect Prediction (SDP), we conducted a thorough bibliographic survey of 79 research articles from the year 2011 to 2023 that examined previous models, datasets, data validation techniques, defect detection, prediction methods, and SDP tools. The survey revealed that previous research often lacked appropriate datasets with the necessary characteristics and data validation methods. Additionally, many standard datasets suffer from a lack of labels, which hinders effective defect detection. Systematic literature reviews on SDP are scarce, further emphasizing the importance of this study. Based on the findings, we provide crucial recommendations for designing effective SDP models and tools. The proposed survey outlines an architecture for constructing SDP datasets with the appropriate characteristics, as well as multi-label classification and data validation methodologies for software defects. This approach aims to enhance SDP research and contribute to the development of high-quality software products by improving defect prediction accuracy. © 2024 The Authors.},
	author_keywords = {artificial intelligence; bibliographic survey; classification; machine learning; search-based techniques; Software defect prediction; statistical validation},
	keywords = {Bibliographic survey; Data validation; Defect detection; Machine-learning; Search-based; Search-based technique; Software defect prediction; Software defects; Statistical validation; Validation methodologies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Godwin2024,
	author = {Godwin, Ryan C. and Melvin, Ryan L.},
	title = {Toward efficient data science: A comprehensive MLOps template for collaborative code development and automation},
	year = {2024},
	journal = {SoftwareX},
	volume = {26},
	doi = {10.1016/j.softx.2024.101723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189747704&doi=10.1016%2fj.softx.2024.101723&partnerID=40&md5=7aebc70a2ce4b42acad521b97af74dae},
	abstract = {In the era of big data analytics and AI applications, data provenance is as important as ever, particularly as applications emerge in vital industries like healthcare. Additionally, as the suites of tools and packages grow exponentially, code transparency and experiment record keeping are essential to ensuring full traceability of AI and ML models. This manuscript presents an open-source Machine Learning Operations (MLOps) Template that provides a consistent framework to support collaborative development and improve efficiency. The template provides a robust and reliable software structure incorporating essential development aspects. These tools include automated code documentation, built-in package management, experiment tracking, configuration and logging infrastructure, and more. The template is built on an agglomeration of best practices gleaned from industry and academia alike, providing a great starting point for any ML/AI project. © 2024},
	author_keywords = {Artificial intelligence; Machine learning; Machine learning operations; Software reliability; Software template; Traceability},
	keywords = {Data Analytics; Open source software; Records management; Software reliability; Code automation; Code development; Collaborative codes; Data analytics; Machine learning operation; Machine-learning; Software templates; Software-Reliability; Traceability; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access}
}

@ARTICLE{Schuett2023,
	author = {Schuett, Jonas},
	title = {Risk Management in the Artificial Intelligence Act},
	year = {2023},
	journal = {European Journal of Risk Regulation},
	doi = {10.1017/err.2023.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148048311&doi=10.1017%2ferr.2023.1&partnerID=40&md5=90287d59988149bc938926c105d3021f},
	abstract = {The proposed Artificial Intelligence Act (AI Act) is the first comprehensive attempt to regulate artificial intelligence (AI) in a major jurisdiction. This article analyses Article 9, the key risk management provision in the AI Act. It gives an overview of the regulatory concept behind the norm, determines its purpose and scope of application, offers a comprehensive interpretation of the specific risk management requirements and outlines ways in which the requirements can be enforced. This article can help providers of high-risk systems to comply with the requirements set out in Article 9. In addition, it can inform revisions of the current draft of the AI Act and efforts to develop harmonised standards on AI risk management.  © The Author(s), 2023. Published by Cambridge University Press.},
	author_keywords = {AI Act; Article 9; artificial intelligence; risk management},
	type = {Article},
	publication_stage = {Article in press},
	source = {Scopus},
	note = {Cited by: 54; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Bravo2024,
	author = {Bravo, Flor A. and Cruz-Bohorquez, Juan M.},
	title = {Engineering Education in the Age of AI: Analysis of the Impact of Chatbots on Learning in Engineering},
	year = {2024},
	journal = {Education Sciences},
	volume = {14},
	number = {5},
	doi = {10.3390/educsci14050484},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194268839&doi=10.3390%2feducsci14050484&partnerID=40&md5=211d0a409c53d3fc20666ba84db8e973},
	abstract = {The purpose of this paper is to explore the influence of using AI chatbots on learning within the context of engineering education. We framed this study on the principles of how learning works in order to describe the contributions and challenges of AI chatbots in five categories: (1) facilitating the acquisition, completion, or activation of prior knowledge and helping organize knowledge and making connections; (2) enhancing student motivation to learn; (3) fostering self-directed learning and the acquisition, practice, and application of the skills and knowledge they acquire; (4) supporting goal-directed practice and feedback; and (5) addressing student diversity and creating a positive classroom environment. To elicit the uses, benefits, and drawbacks of using AI chatbots in students’ learning, we conducted a thematic analysis of qualitative data gathered from surveying 38 student volunteers from 5 different electronic and mechatronic engineering courses at a South American university. Based on a literature review and an evidence-based discussion, we offer practical suggestions for instructors who want to promote the use of AI to enhance their students’ learning. © 2024 by the authors.},
	author_keywords = {AI chatbots; artificial intelligence; ChatGPT; higher education; learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Mendez20241834,
	author = {Mendez, Manuel and Becerra-Teron, Antonio and Almendros-Jimenez, Jesus M. and Merayo, Mercedes G. and Nunez, Manuel},
	title = {Combining Metamorphic Testing and Machine Learning to Enhance OpenStreetMap},
	year = {2024},
	journal = {IEEE Transactions on Reliability},
	volume = {73},
	number = {4},
	pages = {1834 – 1848},
	doi = {10.1109/TR.2024.3379366},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189617796&doi=10.1109%2fTR.2024.3379366&partnerID=40&md5=ba3e840a5ab763d2954db4360e5b1af9},
	abstract = {Metamorphic testing (MT) is a useful tool to test systems where an oracle is not available. MT relies on the definition of metamorphic relations (MR), that is, certain properties that relate a set of inputs and the set of outputs produced by the system under test (SUT) as response to these inputs. Usually, a violation of an MR implies that the SUT is faulty. However, some work on MT accepts, for certain SUTs, that the violation of an MR almost always is a symptom of an error but assumes the potential existence of false positives. This is the case, for instance, of our recent work where we applied MT to improve OpenStreetMap (OSM). Our MRs were able to uncover a large amount of errors in all the analyzed maps but we suffered the presence of a nonnegligible number of false positives. Therefore, an expert had to manually check the suspicious elements identified by our MRs. If we analyze large maps, then this manual task is unfeasible. In this article we solve the main limitation of our previous approach: we accurately and automatically discard false positives. Our new framework combines MT, along the same lines of our previous work, and machine learning. Specifically, we provide three models, one for each MR, based on the random forest model. The models were extensively trained with real data obtained from the application of MRs to maps of cities located in different continents. In order to evaluate the usefulness of our models, we tested them using different cities, in countries that were not considered in the training set. The results were very good: accuracy of the models is never lower than 0.90, it is usually much higher and in many situations reaches 1.0. The computation of the F1-scores yielded similar results.  © 1963-2012 IEEE.},
	author_keywords = {Machine learning (ML); metamorphic testing (MT); openstreetmap (OSM); quality of maps},
	keywords = {Artificial intelligence; Computer software maintenance; Forestry; Software testing; Computational modelling; Machine learning; Machine-learning; Manual; Metamorphic testing; Openstreetmap; Quality of map; Road; Tagging; Urban areas; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Padmanabhan2024847,
	author = {Padmanabhan, Mani},
	title = {A Systematic Review of AI Based Software Test Case Optimization},
	year = {2024},
	journal = {International Research Journal of Multidisciplinary Scope},
	volume = {5},
	number = {4},
	pages = {847 – 859},
	doi = {10.47857/irjms.2024.v05i04.01451},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210429925&doi=10.47857%2firjms.2024.v05i04.01451&partnerID=40&md5=cd25386214be24954d449bd23dd3dca1},
	abstract = {Software test case optimization for real-time systems is a vulnerability detection methodology that assesses the resilience of targeted programs by subjecting them to irregular input data. As the volume, size, and intricacy of software continue to escalate, conventional manual test case generation has encountered challenges like insufficient logical coverage, minimal automation levels, and inadequate test scenarios. These difficulties underscore the need for innovative approaches that maximize software dependability and performance. An artificial intelligence powered fuzzing technique, which exhibits remarkable proficiency in data analysis and classification prediction. This paper examines the recent advancements in fuzzing research and conducts a comprehensive review of artificial intelligence driven fuzzing approaches in software test cases optimization. The major review explains the test case validation workflow and discusses the optimization of distinct phases within fuzzing utilizing in the software testing. Particular emphasis is placed on the implementation of artificial intelligence in the following software testing phases. This process involves position selection, which includes organizing and cleaning data; generating test cases that cover different inputs and expected outputs; selecting fuzzy input values for testing edge cases; validating the results of each test case to ensure accuracy and reliability. Finally, it synthesizes the obstacles and complexities associated with integrating artificial intelligence into software test case optimization techniques and anticipate potential future directions in the software testing. © 2024, Iquz Galaxy Publisher. All rights reserved.},
	author_keywords = {Artificial Intelligence; Software Testing; Test Case Optimization; Test Case Validation Techniques},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Wen2025,
	author = {Wen, Shao-Fang and Shukla, Ankur and Katt, Basel},
	title = {Artificial intelligence for system security assurance: A systematic literature review},
	year = {2025},
	journal = {International Journal of Information Security},
	volume = {24},
	number = {1},
	doi = {10.1007/s10207-024-00959-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212038912&doi=10.1007%2fs10207-024-00959-0&partnerID=40&md5=f4bdb864124bc1593b0af53c6c4a459e},
	abstract = {System Security Assurance (SSA) has emerged as a critical methodology for organizations to verify the trustworthiness of their systems by evaluating security measures against industry standards, legal requirements, and best practices to identify any weakness and demonstrate compliance. In recent years, the role of Artificial Intelligence (AI) in enhancing cybersecurity has received increased attention, with an increasing number of literature reviews highlighting its diverse applications. However, there remains a significant gap in comprehensive reviews that specifically address the integration of AI within SSA frameworks. This systematic literature review seeks to fill this research gap by assessing the current state of AI in SSA, identifying key areas where AI contributes to improve SSA processes, highlighting the limitations of current methodologies, and providing the guidance for future advancements in the field of AI-driven SSA. © The Author(s) 2024.},
	author_keywords = {Artificial intelligence; Cybersecurity; Security assurance; Systematic literature review},
	keywords = {'current; Best practices; Cyber security; Industry standards; Legal requirements; Literature reviews; Security assurance; Security measure; System security; Systematic literature review},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Tang2023,
	author = {Tang, Yuk Ming and Chau, Ka Yin and Lau, Yui-Yip and Zheng, Zehang},
	title = {Data-Intensive Inventory Forecasting with Artificial Intelligence Models for Cross-Border E-Commerce Service Automation},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {5},
	doi = {10.3390/app13053051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149939167&doi=10.3390%2fapp13053051&partnerID=40&md5=dc7c2716f7d677c62eae63a37c2ebf73},
	abstract = {Building an adaptative, flexible, resilient, and reliable inventory management system provides a reliable supply of cross-border e-commerce commodities, enhances supply chain members with a flow of products, fulfills ever-changing customer requirements, and enables e-commerce service automation. This study uses an e-commerce company as a case study to collect intensive inventory data. The key process of the AI approach for an intensive data forecasting framework is constructed. The study shows that the AI model’s optimization process needs to be combined with the problems of specific companies and information for analysis and optimization. The study provides optimization suggestions and highlights the key processes of the AI-predicting inventory model. The XGBoost method demonstrates the best performance in terms of accuracy (RMSE = 46.64%) and reasonable computation time (9 min 13 s). This research can be generalized and used as a useful basis for further implementing algorithms in other e-commerce enterprises. In doing so, this study highlights the current trend of logistics 4.0 solutions via the adoption of robust data-intensive inventory forecasting with artificial intelligence models for cross-border e-commerce service automation. As expected, the research findings improve the alleviation of the bullwhip impact and sustainable supply chain development. E-commerce enterprises may provide a better plan for their inventory management so as to minimize excess inventory or stock-outs, and improve their sales strategies and promotional and marketing activities. © 2023 by the authors.},
	author_keywords = {artificial intelligence; cross-border e-commerce; data-intensive; Extreme Gradient Boosting; inventory forecasting; model; replenishment automation; supply chain management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 38; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{de Biase20241455,
	author = {de Biase, Maria Stella and Bernardi, Simona and Marrone, Stefano and Merseguer, José and Palladino, Angelo},
	title = {Completion of SysML state machines from Given–When–Then requirements},
	year = {2024},
	journal = {Software and Systems Modeling},
	volume = {23},
	number = {6},
	pages = {1455 – 1491},
	doi = {10.1007/s10270-024-01228-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210494526&doi=10.1007%2fs10270-024-01228-3&partnerID=40&md5=e60dd34d78054e328428358e8ef361cc},
	abstract = {MDE enables the centrality of the models in semi-automated development processes. However, its level of usage in industrial settings is still not adequate for the benefits MDE can introduce. This paper proposes a semi-automatic approach for the completion of high-level models in the lifecycle of critical systems, which exhibit an event-driven behaviour. The proposal suggests a specification guideline that starts from a partial SysML model of a system and on a set of requirements, expressed in the well-known Given–When–Then paradigm. On the basis of such requirements, the approach enables the semi-automatic generation of new SysML state machines model elements. Accordingly, the approach focuses on the completion of the state machines by adding proper transitions (with triggers, guards and effects) among pre-existing states. Also, traceability modelling elements are added to the model. Two case studies demonstrate the feasibility of the proposed approach. © The Author(s) 2024.},
	author_keywords = {Behaviour-driven development; Critical systems design; Event-driven systems design; Requirements engineering; SysML},
	keywords = {Artificial intelligence; Behavior-driven development; Critical system design; Critical systems; Development process; Event-driven system; Event-driven system design; Model elements; Requirement engineering; State-machine; SysML; Requirements engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Kaliraj202464212,
	author = {Kaliraj, S. and Kishoore, A.M. and Sivakumar, V.},
	title = {Software Fault Prediction Using Cross-Project Analysis: A Study on Class Imbalance and Model Generalization},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {64212 – 64227},
	doi = {10.1109/ACCESS.2024.3397494},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193024527&doi=10.1109%2fACCESS.2024.3397494&partnerID=40&md5=112fa004dba2f4cdb7023322341c1d37},
	abstract = {Software fault prediction is a critical aspect of software engineering aimed at improving software quality and reliability. However, it faces significant challenges, including the class imbalance issue in fault data and the need for robust predictive models that generalize well across different projects. In this research, we delve into these challenges and investigate the impact of class imbalance and model generalization on software fault prediction using cross-project analysis. Our study addresses three primary research questions: Firstly, we examine the critical issue of class imbalance in fault prediction, which poses a significant hurdle to accurate model performance. Through extensive experimentation with various classifiers on diverse datasets from different software projects, we highlight the variations in classifier performance and the necessity of addressing class imbalance for reliable predictions. Secondly, we evaluate the reliability of cross-project prediction, aiming to understand how effectively predictive models trained on one project can generalize to predict faults in other projects. We demonstrate the importance of training with datasets sharing similar characteristics with the target project for achieving reliable cross-project prediction. Thirdly, we analyze the impact of increasing training samples from different projects on prediction accuracy, emphasizing the benefits of utilizing cross-project analysis to enhance predictive model performance. In addition to addressing these research questions, we provide a comprehensive comparison of classifier performance metrics, including accuracy, precision, recall, and F1 Score. Our findings not only shed light on the challenges and opportunities in software fault prediction but also emphasize the importance of considering class imbalance and model generalization for developing robust and reliable fault prediction models. This research contributes to advancing the field by providing insights into effective modeling approaches and highlighting the motivation behind addressing these challenges.  © 2013 IEEE.},
	author_keywords = {Class imbalance; cross-project analysis; machine learning classifiers; model generalization; performance metrics; software fault prediction},
	keywords = {Artificial intelligence; Classification (of information); Computer software selection and evaluation; Fault detection; Forecasting; Learning systems; Quality control; Reliability analysis; Class imbalance; Cross-project analyse; Faults detection; Faults diagnosis; Learning classifiers; Machine learning classifier; Machine-learning; Model generalization; Performance metrices; Predictive models; Project analysis; Software; Software fault prediction; Software-Reliability; Software reliability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Ray202424,
	author = {Ray, George},
	title = {Text summarization using the relationship structure of a document},
	year = {2024},
	journal = {Issues in Information Systems},
	volume = {25},
	number = {1},
	pages = {24 – 32},
	doi = {10.48009/1_iis_2024_103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004244483&doi=10.48009%2f1_iis_2024_103&partnerID=40&md5=665eecf54975f5264862a4315498e9ca},
	abstract = {A new approach for textual analytics is introduced that creates a relationship structure for a document by using the relationship words in the system of basic English. Relationship words interlock concepts in sentences. The author of a document proposes certain relationships between concepts and uses relationship words in sentences to present a pattern of those relationships. In this paper, the concept of a document’s relationship structure is used for automating summaries for documents. In addition to developing an automated approach to analyze, understand and present knowledge in documents, this article also considers how to validate the correctness of an artificial intelligence algorithm. © 2024 International Association for Computer Information Systems. All rights reserved.},
	author_keywords = {artificial intelligence; Rouge scoring; text summarizer; thinking machines; validating AI},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Piccolo2023,
	author = {Piccolo, Stephen R. and Denny, Paul and Luxton-Reilly, Andrew and Payne, Samuel H. and Ridge, Perry G.},
	title = {Evaluating a large language model’s ability to solve programming exercises from an introductory bioinformatics course},
	year = {2023},
	journal = {PLoS Computational Biology},
	volume = {19},
	number = {9 September},
	doi = {10.1371/journal.pcbi.1011511},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173538194&doi=10.1371%2fjournal.pcbi.1011511&partnerID=40&md5=ebd5def92b1d9c25832cb5a27d2af3af},
	abstract = {Computer programming is a fundamental tool for life scientists, allowing them to carry out essential research tasks. However, despite various educational efforts, learning to write code can be a challenging endeavor for students and researchers in life-sciences disciplines. Recent advances in artificial intelligence have made it possible to translate human-language prompts to functional code, raising questions about whether these technologies can aid (or replace) life scientists’ efforts to write code. Using 184 programming exercises from an introductory-bioinformatics course, we evaluated the extent to which one such tool —OpenAI’s ChatGPT—could successfully complete programming tasks. ChatGPT solved 139 (75.5%) of the exercises on its first attempt. For the remaining exercises, we provided natural-language feedback to the model, prompting it to try different approaches. Within 7 or fewer attempts, ChatGPT solved 179 (97.3%) of the exercises. These findings have implications for life-sciences education and research. Instructors may need to adapt their pedagogical approaches and assessment techniques to account for these new capabilities that are available to the general public. For some programming tasks, researchers may be able to work in collaboration with machine-learning models to produce functional code. Copyright: © 2023 Piccolo et al.},
	keywords = {Artificial intelligence; Education computing; Functional programming; Functional codes; Fundamental tools; Human language; Language model; Life scientists; Life-sciences; Natural languages; Programming exercise; Programming tasks; Science disciplines; article; bioinformatics; biomedicine; ChatGPT; education; exercise; human; human experiment; large language model; machine learning; Bioinformatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Takan2023,
	author = {Takan, Savaş and Ergün, Duygu and Katipoğlu, Gökmen},
	title = {Gamified Text Testing for Sustainable Fairness},
	year = {2023},
	journal = {Sustainability (Switzerland)},
	volume = {15},
	number = {3},
	doi = {10.3390/su15032292},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147877613&doi=10.3390%2fsu15032292&partnerID=40&md5=61a4f9ddec14df7856a792455d716317},
	abstract = {AI fairness is an essential topic as regards its topical and social-societal implications. However, there are many challenges posed by automating AI fairness. Based on the challenges around automating fairness in texts, our study aims to create a new fairness testing paradigm that can gather disparate proposals on fairness on a single platform, test them, and develop the most effective method, thereby contributing to the general orientation on fairness. To ensure and sustain mass participation in solving the fairness problem, gamification elements are used to mobilize individuals’ motivation. In this framework, gamification in the design allows participants to see their progress and compare it with other players. It uses extrinsic motivation elements, i.e., rewarding participants by publicizing their achievements to the masses. The validity of the design is demonstrated through the example scenario. Our design represents a platform for the development of practices on fairness and can be instrumental in making contributions to this issue sustainable. We plan to further realize a plot application of this structure designed with the gamification method in future studies. © 2023 by the authors.},
	author_keywords = {artificial intelligence fairness; centrality; confusion matrix; fair-test; gamification; mutation test; testing; zero-suppression decision diagram},
	keywords = {artificial intelligence; automation; design; diagram; participatory approach; sustainability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}@ARTICLE{Uzir2021,
	author = {Uzir, Md Uzir Hossain and Al Halbusi, Hussam and Lim, Rodney and Jerin, Ishraq and Abdul Hamid, Abu Bakar and Ramayah, Thurasamy and Haque, Ahasanul},
	title = {Applied Artificial Intelligence and user satisfaction: Smartwatch usage for healthcare in Bangladesh during COVID-19},
	year = {2021},
	journal = {Technology in Society},
	volume = {67},
	doi = {10.1016/j.techsoc.2021.101780},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118726488&doi=10.1016%2fj.techsoc.2021.101780&partnerID=40&md5=463477b9cf9c1c5c2034c0396ce727cd},
	abstract = {The evolution of Artificial Intelligence (AI) has revolutionized many aspects of human life, including healthcare. Amidst the Covid-19 pandemic, AI-enabled smartwatches are being used to help users to self-monitor and self-manage their health. Using a framework based on Stimulus-Organism-Response (S–O-R) theory, this present study aimed to explore the use of AI-enabled smartwatches for health purposes, in particular the effects of product quality, service quality, perceived convenience, and perceived ease of use on user experience, trust and user satisfaction. Based on a purposive survey sample of 486 smartphone users in Bangladesh, data collected was analyzed using SPSS software for elementary analyses and PLS-SEM for hypotheses testing. The findings showed that the predictors, namely product quality, service quality, perceived convenience, and perceived ease of use, significantly affected user experience and trust. Similarly, user experience and trust were influential on user satisfaction and played partial mediating roles between predictors and user satisfaction. Besides, gender and age moderate the relationships of experience and trust with customer satisfaction. These findings support the S–O-R theoretical framework and have practical implications for brand and marketing managers of smartwatches in developing product features and understanding users' attitudes and behaviours. © 2021 Elsevier Ltd},
	author_keywords = {Applied artificial intelligence; COVID-19; Smartwatches; User experience; User satisfaction; User trust},
	keywords = {Bangladesh; Varanidae; Artificial intelligence; Biology; Customer satisfaction; Health care; Quality control; Software testing; Applied artificial intelligence; Bangladesh; COVID-19; Perceived ease-of-use; Quality services; Service Quality; Smartwatch; User trust; Users' experiences; Users' satisfactions; artificial intelligence; COVID-19; health care; health services; health status; Quality of service},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 90; All Open Access, Green Open Access}
}

@ARTICLE{Souza-Pereira2021,
	author = {Souza-Pereira, Leonice and Ouhbi, Sofia and Pombo, Nuno},
	title = {A process model for quality in use evaluation of clinical decision support systems},
	year = {2021},
	journal = {Journal of Biomedical Informatics},
	volume = {123},
	doi = {10.1016/j.jbi.2021.103917},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116006762&doi=10.1016%2fj.jbi.2021.103917&partnerID=40&md5=55a0744772e9aaa4343a85040cc10e81},
	abstract = {Context: Clinical decision support systems (CDSSs) are used to help healthcare professionals in making decisions, offering them a tool for improved medical care practices based on monitoring and management procedures. Although CDSSs exhibit many advantages, challenges remain in terms of their adoption in the clinician community. One such issue is related to user satisfaction and the system reliability. Ensuring the quality of CDSSs is a way to improve their acceptance and adoption. Objective: This study aims to propose a process model for evaluation of the quality in use characteristics of a CDSS to identify deficiencies that reduce its use by healthcare professionals. Methods: We reviewed the existing literature on CDSS assessment and developed a process model based on the international standards ISO/IEC 25010 System and software quality models, and ISO/IEC 25022 Measurement of quality in use. To select measures for evaluating these characteristics, we adopted the Goal-Question-Metric (GQM) method. We evaluated the quality in use characteristics because they can represent system usability. Measurement of these characteristics helps us understand user needs, improve the user experience, and mitigate the low acceptance of CDSS, particularly by the primary users. Results: We developed a process model for measuring the quality in use (QiU) characteristics of CDSSs, explaining its applicability through an illustrative example focused on the characteristics of satisfaction and efficiency. Conclusion: We consider that the proposed process model will benefit the CDSS adoption and contribute to the improvement of the quality of such systems by measuring its QiU. © 2021 Elsevier Inc.},
	author_keywords = {CDSS; Clinical decision support systems; Evaluation model; Quality in use; Quality measures},
	keywords = {Decision Support Systems, Clinical; Publications; Reproducibility of Results; Artificial intelligence; Computer software selection and evaluation; Decision making; Decision support systems; ISO Standards; Quality control; Clinical decision support systems; Evaluation models; Health care professionals; ISO/IEC; Making decision; Measurements of; Process-models; Quality in use; Quality measures; adoption; article; clinical assessment; clinical decision support system; clinical evaluation; human; process model; satisfaction; software; standard; systematic review; usability; publication; reproducibility; Health care},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Bronze Open Access}
}

@ARTICLE{Hribar2021,
	author = {Hribar, Nena and Šimić, Goran and Vukadinović, Simonida and Šprajc, Polona},
	title = {Decision-making in sustainable energy transition in Southeastern Europe: probabilistic network-based model},
	year = {2021},
	journal = {Energy, Sustainability and Society},
	volume = {11},
	number = {1},
	doi = {10.1186/s13705-021-00315-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118233661&doi=10.1186%2fs13705-021-00315-3&partnerID=40&md5=d0af14932ef663f7b557bf1bcbae142b},
	abstract = {Background: Sustainable energy transition of a country is complex and long-term process, which requires decision-making in all stages and at all levels, including a large number of different factors, with different causality. The main objective of this paper is the development of a probabilistic model for decision-making in sustainable energy transition in developing countries of SE Europe. The model will be developed according to the specificities of the countries for which it is intended—SE Europe. These are countries where energy transition is slower and more difficult due to many factors: high degree of uncertainty, low transparency, corruption, investment problems, insufficiently reliable data, lower level of economic development, high level of corruption and untrained human resources. All these factors are making decision-making more challenging and demanding. Methods: Research was done by using content analysis, artificial intelligence methods, software development method and testing. The model was developed by using MSBNx—Microsoft Research’s Bayesian Network Authoring and Evaluation Tool. Results: Due to the large number of insufficiently clear, but interdependent factors, the model is developed on the principle of probabilistic (Bayesian) networks of factors of interest. The paper presents the first model for supporting decision-making in the field of energy sustainability for the region of Southeastern Europe, which is based on the application of Bayesian Networks. Conclusion: Testing of the developed model showed certain characteristics, discussed in paper. The application of developed model will make it possible to predict the short-term and long-term consequences that may occur during energy transition by varying these factors. Recommendations are given for further development of the model, based on Bayesian networks. © 2021, The Author(s).},
	author_keywords = {Bayesian networks; Decision-making; SE Europe; Sustainable energy transition},
	keywords = {Europe; Bayesian networks; Crime; Developing countries; Economics; Energy conservation; Investments; Software design; Software testing; Statistical tests; Bayesia n networks; Decisions makings; Developed model; Energy transitions; Long term process; Network-based modeling; Probabilistic network; SE europe; South Eastern Europe; Sustainable energy transition; alternative energy; artificial intelligence; corruption; energy; investment; model; sustainability; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Esfandyari20217673,
	author = {Esfandyari, Sajad and Rafe, Vahid},
	title = {GALP: a hybrid artificial intelligence algorithm for generating covering array},
	year = {2021},
	journal = {Soft Computing},
	volume = {25},
	number = {11},
	pages = {7673 – 7689},
	doi = {10.1007/s00500-021-05788-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106168051&doi=10.1007%2fs00500-021-05788-0&partnerID=40&md5=2963be6fcff972a65a608930e6ec42e6},
	abstract = {Today, there are a lot of useful algorithms for covering array (CA) generation, one of the branches of combinatorial testing. The major CA challenge is the generation of an array with the minimum number of test cases (efficiency) in an appropriate run-time (performance), for large systems. CA generation strategies are classified into several categories: computational and meta-heuristic, to name the most important ones. Generally, computational strategies have high performance and yield poor results in terms of efficiency, in contrast, meta-heuristic strategies have good efficiency and lower performance. Among the strategies available, some are efficient strategies but suffer from low performance; conversely, some others have good performance, but is not such efficient. In general, there is not a strategy that enjoys both above-mentioned metrics. In this paper, it is tried to combine the genetic algorithm and the Augmented Lagrangian Particle Swarm Optimization with Fractional Order Velocity to produce the appropriate test suite in terms of efficiency and performance. Also, a simple and effective minimizing function is employed to increase efficiency. The evaluation results show that the proposed strategy outperforms the existing approaches in terms of both efficiency and performance. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
	author_keywords = {ALPSOFV; Combinatorial testing; Covering array; Genetic algorithm},
	keywords = {Constrained optimization; Efficiency; Genetic algorithms; Heuristic algorithms; Particle swarm optimization (PSO); Augmented Lagrangians; Combinatorial testing; Computational strategy; Efficiency and performance; Efficient strategy; Evaluation results; Fractional order; Hybrid artificial intelligences; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Green Open Access}
}

@ARTICLE{Jabborov2023130491,
	author = {Jabborov, Ahror and Kharlamova, Arina and Kholmatova, Zamira and Kruglov, Artem and Kruglov, Vasily and Succi, Giancarlo},
	title = {Taxonomy of Quality Assessment for Intelligent Software Systems: A Systematic Literature Review},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {130491 – 130507},
	doi = {10.1109/ACCESS.2023.3333920},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178066293&doi=10.1109%2fACCESS.2023.3333920&partnerID=40&md5=560bfd8597208116a9c9f6f5d41d4ef0},
	abstract = {The increasing integration of AI software into various aspects of our daily lives has amplified the importance of evaluating the quality of these intelligent systems. The rapid proliferation of AI-based software projects and the growing reliance on these systems underscore the urgency of examining their quality for practical applications in both industry and academia. This systematic literature review delves into the study of quality assessment metrics and methods for AI-based systems, pinpointing key attributes and properties of intelligent software projects that are crucial for determining their quality. Furthermore, a comprehensive analysis of this domain will enable researchers to devise novel methods and metrics for effectively and efficiently evaluating the quality of such systems. Despite its importance, this area of development is still relatively nascent and evolving. This paper presents a systematic review of the current state of the taxonomy of quality assessment for AI-based software. We analyzed 271 articles from six different sources that focused on the quality assessment of intelligent software systems. The primary objective of this work is to provide an overview of the field and consolidate knowledge, which will aid researchers in identifying additional areas for future research. Moreover, our findings reveal the necessity to establish remedial strategies and develop tools to automate the process of identifying appropriate actions in response to abnormal metric values. © 2013 IEEE.},
	author_keywords = {AI system evaluation; AI-based software; Artificial intelligence; feature selection; intelligent systems; machine learning; quality assessment; quality models; software attributes},
	keywords = {Application programs; Feature Selection; Learning systems; Quality assurance; Quality control; Taxonomies; AI system evaluation; AI systems; AI-based software; Features selection; Machine-learning; Measurable property; Property; Quality assessment; Quality modeling; Software; Software attribute; Software Measurement; Software-systems; System evaluation; Systematic; Validation technique; Intelligent systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@ARTICLE{Penn2021337,
	author = {Penn, Jonnie},
	title = {Algorithmic Silence: A Call to Decomputerize},
	year = {2021},
	journal = {Journal of Social Computing},
	volume = {2},
	number = {4},
	pages = {337 – 356},
	doi = {10.23919/JSC.2021.0023},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135144392&doi=10.23919%2fJSC.2021.0023&partnerID=40&md5=a6efa18907e9c52d0029ad7f44d06b45},
	abstract = {Tech critics become technocrats when they overlook the daunting administrative density of a digital-first society. The author implores critics to reject structural dependencies on digital tools rather than naturalize their integration through critique and reform. At stake is the degree to which citizens must defer to unelected experts to navigate such density. Democracy dies in the darkness of sysadmin. The argument and a candidate solution proceed as follows. Since entropy is intrinsic to all physical systems, including digital systems, perfect automation is a fiction. Concealing this fiction, however, are five historical forces usually treated in isolation: ghost work, technical debt, intellectual debt, the labor of algorithmic critique, and various types of participatory labor. The author connects these topics to emphasize the systemic impositions of digital decision tools, which compound entangled genealogies of oppression and temporal attrition. In search of a harmonious balance between the use of 'AI' tools and the non-digital decision systems they are meant to supplant, the author draws inspiration from an unexpected source: musical notation. Just as musical notes require silence to be operative, the author positions algorithmic silence-the deliberate exclusion of highly abstract digital decision systems from human decision-making environments-as a strategic corrective to the fiction of total automation. Facial recognition bans and the Right to Disconnect are recent examples of algorithmic silence as an active trend.  © 2020 Tsinghua University Press.},
	author_keywords = {AI ethics; algorithmic silence; artificial intelligence; automation; decomputerization; history; labor; technocracy},
	keywords = {Decision making; Digital devices; Face recognition; Music; Philosophical aspects; AI ethic; Algorithmic silence; Algorithmics; Decision systems; Decomputerization; Digital system; Digital tools; Physical systems; Sysadmins; Technocracy; Automation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Steimers2022,
	author = {Steimers, André and Schneider, Moritz},
	title = {Sources of Risk of AI Systems},
	year = {2022},
	journal = {International Journal of Environmental Research and Public Health},
	volume = {19},
	number = {6},
	doi = {10.3390/ijerph19063641},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126452843&doi=10.3390%2fijerph19063641&partnerID=40&md5=3c009655eaebb1c6f2fe1c7136b906e6},
	abstract = {Artificial intelligence can be used to realise new types of protective devices and assistance systems, so their importance for occupational safety and health is continuously increasing. However, established risk mitigation measures in software development are only partially suitable for applications in AI systems, which only create new sources of risk. Risk management for systems that for systems using AI must therefore be adapted to the new problems. This work objects to contribute hereto by identifying relevant sources of risk for AI systems. For this purpose, the differences between AI systems, especially those based on modern machine learning methods, and classical software were analysed, and the current research fields of trustworthy AI were evaluated. On this basis, a taxonomy could be created that provides an overview of various AI-specific sources of risk. These new sources of risk should be taken into account in the overall risk assessment of a system based on AI technologies, examined for their criticality and managed accordingly at an early stage to prevent a later system failure. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Artificial intelligence; Assistance systems; Occupational safety; Protective devices; Risk management},
	keywords = {Artificial Intelligence; Machine Learning; Occupational Health; Software; Technology; artificial intelligence; equipment; health and safety; health geography; machine learning; management practice; occupational exposure; risk assessment; technology adoption; accuracy; Article; artificial intelligence; automation; calibration; data completeness; data consistency; fairness; human; machine learning; occupational safety; privacy; risk assessment; risk management; social aspects and related phenomena; statistical significance; taxonomy; technology; timeliness; occupational health; software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Li2022,
	author = {Li, Siwei and An, Jingjing and Li, Yaqiu and Zhu, Xiagu and Zhao, Dongdong and Wang, Lixian and Sun, Yonghui and Yang, Yuanzhao and Bi, Changhao and Zhang, Xueli and Wang, Meng},
	title = {Automated high-throughput genome editing platform with an AI learning in situ prediction model},
	year = {2022},
	journal = {Nature Communications},
	volume = {13},
	number = {1},
	doi = {10.1038/s41467-022-35056-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143110198&doi=10.1038%2fs41467-022-35056-0&partnerID=40&md5=0a33a406d2450bf3c4dea366ae2c1de6},
	abstract = {A great number of cell disease models with pathogenic SNVs are needed for the development of genome editing based therapeutics or broadly basic scientific research. However, the generation of traditional cell disease models is heavily dependent on large-scale manual operations, which is not only time-consuming, but also costly and error-prone. In this study, we devise an automated high-throughput platform, through which thousands of samples are automatically edited within a week, providing edited cells with high efficiency. Based on the large in situ genome editing data obtained by the automatic high-throughput platform, we develop a Chromatin Accessibility Enabled Learning Model (CAELM) to predict the performance of cytosine base editors (CBEs), both chromatin accessibility and the context-sequence are utilized to build the model, which accurately predicts the result of in situ base editing. This work is expected to accelerate the development of BE-based genetic therapies. © 2022, The Author(s).},
	keywords = {Artificial Intelligence; Chromatin; Gene Editing; Learning; Names; cytosine; guide RNA; accessibility; cell; cell component; disease; genome; prediction; Article; artificial intelligence; automation; chromatin; computer prediction; controlled study; gene editing; high throughput sequencing; human; human cell; machine learning; mammal cell; single nucleotide polymorphism; learning; nomenclature},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Alhumam20221463,
	author = {Alhumam, Abdulaziz},
	title = {Explainable Software Fault Localization Model: From Blackbox to Whitebox},
	year = {2022},
	journal = {Computers, Materials and Continua},
	volume = {73},
	number = {1},
	pages = {1463 – 1482},
	doi = {10.32604/cmc.2022.029473},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130152678&doi=10.32604%2fcmc.2022.029473&partnerID=40&md5=495128bcfb1036c453273937b026c1fb},
	abstract = {The most resource-intensive and laborious part of debugging is finding the exact location of the fault from the more significant number of code snippets. Plenty of machine intelligence models has offered the effective localization of defects. Some models can precisely locate the faulty with more than 95% accuracy, resulting in demand for trustworthy models in fault localization. Confidence and trustworthiness within machine intelligence-based software models can only be achieved via explainable artificial intelligence in Fault Localization (XFL). The current study presents a model for generating counterfactual interpretations for the fault localization model’s decisions. Neural system approximations and disseminated presentation of input information may be achieved by building a nonlinear neural network model. That demonstrates a high level of proficiency in transfer learning, even with minimal training data. The proposed XFL would make the decision-making transparent simultaneously without impacting the model’s performance. The proposed XFL ranks the software program statements based on the possible vulnerability score approximated from the training data. The model’s performance is further evaluated using various metrics like the number of assessed statements, confidence level of fault localization, and Top-N evaluation strategies. © 2022 Tech Science Press. All rights reserved.},
	author_keywords = {explainable artificial intelligence; Software fault localization; statement ranking; vulnerability detection},
	keywords = {Artificial intelligence; Computer software; Fault detection; Black boxes; Explainable artificial intelligence; Fault localization; Localization modeling; Machine intelligence; Performance; Software fault localization; Statement ranking; Training data; Vulnerability detection; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Garlington20221,
	author = {Garlington, Benjamin},
	title = {CISE: Community Engagement of CEB Cloud Ecosystem in Box},
	year = {2022},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {13},
	number = {4},
	pages = {1 – 15},
	doi = {10.14569/IJACSA.2022.0130401},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130054581&doi=10.14569%2fIJACSA.2022.0130401&partnerID=40&md5=c2d1fafb73c608e82d6031db8f03c000},
	abstract = {The explosion of digital and observational data is having a profound effect on the nature of scientific inquiry, requiring new approaches to manipulating and analyzing large and complex data and increasing the need for collaborative solid research teams to address these challenges. These data, along with the availability of computational resources and recent advances in artificial intelligence, machine learning software tools, and methods, can enable unprecedented science and innovation. Unfortunately, these software tools and techniques are not uniformly accessible to all communities, mainly scientists and engineers at Minority Serving Institutions (MSI). Cloud computing resources are natural channels to enhance these institutions' research productivity. However, utilizing cloud computing resources for research effectively requires a significant investment in time and effort, awkward manipulation of data sets, and deployment of cloud-based applications workflows that support analysis and visualization tools. © 2022. All Rights Reserved.},
	author_keywords = {cloud-ecosystem; Collaboration; conceptual; data-as-a-service; engagement; infrastructure-as-a-service; methodological; minority serving institutions (MSI); narrowcasting; outreach; platform-as-a-service; software-as-a-service; storage-as-a-service},
	keywords = {Artificial intelligence; Computer software; Data visualization; Digital storage; Ecosystems; Infrastructure as a service (IaaS); Investments; Software as a service (SaaS); Cloud-ecosystem; Collaboration; Conceptual; Engagement; Methodological; Minority serving institution; Minority serving institutions; Narrowcasting; Outreach; Software-as-a- Service (SaaS); Platform as a Service (PaaS)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Liu2022,
	author = {Liu, Ruhan and Wang, Xiangning and Wu, Qiang and Dai, Ling and Fang, Xi and Yan, Tao and Son, Jaemin and Tang, Shiqi and Li, Jiang and Gao, Zijian and Galdran, Adrian and Poorneshwaran, J.M. and Liu, Hao and Wang, Jie and Chen, Yerui and Porwal, Prasanna and Wei Tan, Gavin Siew and Yang, Xiaokang and Dai, Chao and Song, Haitao and Chen, Mingang and Li, Huating and Jia, Weiping and Shen, Dinggang and Sheng, Bin and Zhang, Ping},
	title = {DeepDRiD: Diabetic Retinopathy—Grading and Image Quality Estimation Challenge},
	year = {2022},
	journal = {Patterns},
	volume = {3},
	number = {6},
	doi = {10.1016/j.patter.2022.100512},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131696394&doi=10.1016%2fj.patter.2022.100512&partnerID=40&md5=5c24945db6f7ccbdc15e573e9433bac1},
	abstract = {We described a challenge named “Diabetic Retinopathy (DR)—Grading and Image Quality Estimation Challenge” in conjunction with ISBI 2020 to hold three sub-challenges and develop deep learning models for DR image assessment and grading. The scientific community responded positively to the challenge, with 34 submissions from 574 registrations. In the challenge, we provided the DeepDRiD dataset containing 2,000 regular DR images (500 patients) and 256 ultra-widefield images (128 patients), both having DR quality and grading annotations. We discussed details of the top 3 algorithms in each sub-challenges. The weighted kappa for DR grading ranged from 0.93 to 0.82, and the accuracy for image quality evaluation ranged from 0.70 to 0.65. The results showed that image quality assessment can be used as a further target for exploration. We also have released the DeepDRiD dataset on GitHub to help develop automatic systems and improve human judgment in DR screening and diagnosis. © 2022 The Author(s)},
	author_keywords = {artificial intelligence; challenge; deep learning; diabetic retinopathy; DSML2: Proof-of-concept Data science output has been formulated, implemented, and tested for one domain/problem; fundus image; image quality analysis; retinal image; screening; ultra-widefield},
	keywords = {Deep learning; Diagnosis; Grading; Image quality; Quality control; Challenge; Deep learning; Diabetic retinopathy; Domain problems; DSML2: proof-of-concept data science output have been formulated, implemented, and tested for one domain/problem; Fundus image; Image quality analysis; Proof of concept; Retinal image; Ultra-widefield; Wide-field; Eye protection},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 111; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Deshays2021,
	author = {Deshays, Ronan and Segovia, Pablo and Duviella, Eric},
	title = {Design of a matlab hec-ras interface to test advanced control strategies on water systems},
	year = {2021},
	journal = {Water (Switzerland)},
	volume = {13},
	number = {6},
	doi = {10.3390/w13060763},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102813978&doi=10.3390%2fw13060763&partnerID=40&md5=08dc1d61dba364e541bd639d8e694fef},
	abstract = {The software package HEC-RAS (Hydrologic Engineering Centers River Analysis System) is widely used by the water engineering community to analyze hydraulic systems and perform development planning. Furthermore, it integrates a control module that allows implementing basic controllers. For more complex approaches, developers from the automatic control and artificial intelligence (AI) communities usually design, implement, and test new algorithms using dedicated software such as MATLAB. However, models of hydraulic systems employed in MATLAB are often very simple. The main objective of the paper is to design a simulation architecture by coupling HEC-RAS with MATLAB, thus improving the accuracy of the dynamics of the hydraulic systems considered in the control simulations. The main feature of the MATLAB HEC-RAS interface design is that it allows one to execute customized code at regular time intervals during the simulation. In this way, closed-loop control and optimization algorithms can be implemented and tested. Moreover, the generic interface allows for any configuration of hydrographical systems. The proposed interface is presented in this paper, and the performance of the approach is demonstrated considering two case studies of different nature. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Automation; Control design; HEC-RAS; MATLAB; Model integration; Simulation; Water systems},
	keywords = {Artificial intelligence; Automation; Closed loop control systems; Hydraulic equipment; Software testing; Advanced control strategy; Closed-loop control; Control simulation; Development planning; Generic interfaces; Hydraulic system; Simulation architecture; Water engineering; accuracy assessment; algorithm; artificial intelligence; control system; design method; development strategy; engineering; implementation process; optimization; performance assessment; simulation; software; MATLAB},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Abdullah2021203,
	author = {Abdullah, Azrol and Manap, Nazura Abdul},
	title = {The Malaysian Perspective On Imposing Civil Liabilities In Road Accidents Involving Autonomous Vehicle},
	year = {2021},
	journal = {UUM Journal of Legal Studies},
	volume = {12},
	number = {2},
	pages = {203 – 228},
	doi = {10.32890/UUMJLS2021.12.2.9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112367061&doi=10.32890%2fUUMJLS2021.12.2.9&partnerID=40&md5=37b131cf9e81550577ca872d16a4ca0f},
	abstract = {The advancement of artificial intelligence (AI) technology has become the fundamental catalyst in the research and development of autonomous vehicle (AV). AVs equipped with AI are expected to perform better than humans and forecasted to reduce the number of road accidents. AV will improve humans’ quality of life, such as creating more mobility for the elderly and disabled, increasing productivity, and creating an environmentally friendly system. Despite AV’s promising abilities, reports indicate that AV can go phut, causing road fatalities to the AV user and other road users. The autonomous nature of AV exacerbates the difficulty in determining who is at fault. This article aims to examine the ability of the existing legal framework to identify the person at fault so as to determine the tortious liability in road accidents involving AV. This article demonstrated that the existing legal scheme is insufficient to determine tortious liability in road accidents involving AV. This article explored the possibility of shouldering the liability on the manufacturer, the user, and even on the AV itself. This article also investigated alternative approaches that could be adopted to resolve issues on the distribution of tortious liability in road accidents involving AV. The outcome of this article could contribute to issues relating to the liability of AI. © 2021, UUM Journal of Legal Studies. All Rights Reserved.},
	author_keywords = {Artificial intelligence; autonomous vehicle; liability; negligence; tort.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Green Open Access}
}

@ARTICLE{Maille2021333,
	author = {Maille, Baptiste and Wilkin, Marie and Million, Matthieu and Rességuier, Noémie and Franceschi, Frédéric and Koutbi-Franceschi, Linda and Hourdain, Jérôme and Martinez, Elisa and Zabern, Maxime and Gardella, Christophe and Tissot-Dupont, Hervé and Singh, Jagmeet P. and Deharo, Jean-Claude and Fiorina, Laurent},
	title = {Smartwatch Electrocardiogram and Artificial Intelligence for Assessing Cardiac-Rhythm Safety of Drug Therapy in the COVID-19 Pandemic. The QT-logs study},
	year = {2021},
	journal = {International Journal of Cardiology},
	volume = {331},
	pages = {333 – 339},
	doi = {10.1016/j.ijcard.2021.01.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101063130&doi=10.1016%2fj.ijcard.2021.01.002&partnerID=40&md5=1d78a7c29047255314af1848958e89ce},
	abstract = {Background: QTc interval monitoring, for the prevention of drug-induced arrhythmias is necessary, especially in the context of coronavirus disease 2019 (COVID-19). For the provision of widespread use, surrogates for 12‑lead ECG QTc assessment may be useful. This prospective observational study compared QTc duration assessed by artificial intelligence (AI-QTc) (Cardiologs®, Paris, France) on smartwatch single‑lead electrocardiograms (SW-ECGs) with those measured on 12‑lead ECGs, in patients with early stage COVID-19 treated with a hydroxychloroquine−azithromycin regimen. Methods: Consecutive patients with COVID-19 who needed hydroxychloroquine−azithromycin therapy, received a smartwatch (Withings Move ECG®, Withings, France). At baseline, day-6 and day-10, a 12‑lead ECG was recorded, and a SW-ECG was transmitted thereafter. Throughout the drug regimen, a SW-ECG was transmitted every morning at rest. Agreement between manual QTc measurement on a 12‑lead ECG and AI-QTc on the corresponding SW-ECG was assessed by the Bland-Altman method. Results: 85 patients (30 men, mean age 38.3 ± 12.2 years) were included in the study. Fair agreement between manual and AI-QTc values was observed, particularly at day-10, where the delay between the 12‑lead ECG and the SW-ECG was the shortest (−2.6 ± 64.7 min): 407 ± 26 ms on the 12‑lead ECG vs 407 ± 22 ms on SW-ECG, bias −1 ms, limits of agreement −46 ms to +45 ms; the difference between the two measures was <50 ms in 98.2% of patients. Conclusion: In real-world epidemic conditions, AI-QTc duration measured by SW-ECG is in fair agreement with manual measurements on 12‑lead ECGs. Following further validation, AI-assisted SW-ECGs may be suitable for QTc interval monitoring. REGISTRATION: ClinicalTrial.gov NCT04371744. © 2021 Elsevier B.V.},
	author_keywords = {Artificial intelligence; COVID-19; Hydroxychloroquine-azythromycine; QTc interval; Smartwatch},
	keywords = {Adult; Arrhythmias, Cardiac; Artificial Intelligence; Azithromycin; COVID-19; Electrocardiography; Female; Humans; Hydroxychloroquine; Long QT Syndrome; Male; Middle Aged; Pandemics; azithromycin; hydroxychloroquine; hydroxychloroquine sulfate; azithromycin; hydroxychloroquine; adult; algorithm; Article; artificial intelligence; controlled study; coronavirus disease 2019; disease course; drug safety; electrocardiogram; electrocardiography monitoring; epidemic; female; heart rhythm; human; major clinical study; male; observational study; outcome assessment; pandemic; priority journal; prospective study; QTc interval; treatment duration; drug therapy; electrocardiography; heart arrhythmia; long QT syndrome; middle aged; pandemic},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 49; All Open Access, Green Open Access}
}

@ARTICLE{Shi2022,
	author = {Shi, Ji and Zou, Wei and Zhang, Chao and Tan, Lingxiao and Zou, Yanyan and Peng, Yue and Huo, Wei},
	title = {CAMFuzz: Explainable Fuzzing with Local Interpretation},
	year = {2022},
	journal = {Cybersecurity},
	volume = {5},
	number = {1},
	doi = {10.1186/s42400-022-00116-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137146196&doi=10.1186%2fs42400-022-00116-x&partnerID=40&md5=31063c02ddeaa2ce0ab37f9ab71ea872},
	abstract = {Grey-box fuzzing techniques have been widely used in software bug finding. In general, there are many decisions to make in the fuzzing process, including which code block in the target program should be explored first, which bytes of an input seed should be mutated to reach the target code block, and how to mutate the chosen input bytes. However, existing solutions usually rely on random exploration or certain heuristics to choose where and how to fuzz, which limits the efficiency of fuzzing. In this paper, we propose a novel solution CAMFuzz to guide the fuzzing process with explainable decisions in explainable artificial intelligence (XAI). First, we propose a dynamic weight adjustment algorithm, which considers both the difficulty of reaching a block and the number of unvisited blocks nearby, to find code blocks worthy to explore first. Second, we utilize a widely used local interpretation technique, i.e., class activation mapping (CAM), to recognize which part of an input seed should be mutated to reach a given target code block. Therefore, CAMFuzz can distinguish which part of code in the program is more important and which positions in the input file should be mutated first, in order to achieve a better code coverage and bug finding efficiency. Third, to further help the fuzzer increase fuzzing efficiency, we leverage a lightweight static program analysis to help the fuzzer identify magic values. We implement a prototype of CAMFuzz and evaluate it on 13 real-world programs (including 11 open source targets, 2 closed-source commercial products including a Microsoft component and Hancom Office) Results show that CAMFuzz outperforms state-of-the-art fuzzers in both code coverage and bug finding. To detail, CAMFuzz on average achieves 2.07× more bugs and 1.17× coverage improvements. In total, it found 19 previously unknown vulnerabilities, of which 6 have been assigned by CVE so far. © 2022, The Author(s).},
	author_keywords = {Explainable artificial intelligence; Fuzzing; Grey-box fuzzing},
	keywords = {Codes (symbols); Efficiency; Open source software; Open systems; Bug finding; Code blocks; Code coverage; Explainable artificial intelligence; Fuzzing; Gray-box fuzzing; Grey-box; Local interpretation; Software bug; Target codes; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access}
}

@ARTICLE{Wilson202212889,
	author = {Wilson, Shyama I. and Goonetillake, Jeevani S. and Ginige, Athula and Walisadeera, Anusha I.},
	title = {Towards a Usable Ontology: The Identification of Quality Characteristics for an Ontology-Driven Decision Support System},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {12889 – 12912},
	doi = {10.1109/ACCESS.2022.3146331},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123722398&doi=10.1109%2fACCESS.2022.3146331&partnerID=40&md5=f6f7212e385a5792bda5362de39da3c0},
	abstract = {Achieving quality in use (i.e., the higher-level quality objectives) is now widely accepted for building a usable system and software product. However, ontology engineering is a discipline for which the quality theories are not yet well developed and adapted and thus, ontology engineering still does not have an agreed methodology and standards for ontology evaluation. As a result, an ontology-driven system may consist of a badly engineered ontology that is not usable. This will in turn cause an adverse effect on the quality in use of the corresponding system. It is necessary to alleviate this problem by formulating an evaluation methodology and standards towards producing a usable ontology particularly targeting an ontology-driven system. It is evident through the literature as well as our practical experience through an agricultural ontology-driven Decision Support System (DSS) that quality in use of the system is tightly coupled with the quality of the ontology. As the first step towards this, we explored the well-established quality theories in system and software engineering to adapt and enhance the quality concepts defined so far in the ontology engineering domain. In the light of this study, we devised an ontology quality approach that guides developers to produce ontologies by avoiding quality issues to make ontology-driven DSSs usable. The proposed approach was exemplified using a use case from the agriculture domain. This research could be a foundation to inspire and assist ontology engineers to rethink about ontology quality from a broader view in developing a usable ontology.  © 2013 IEEE.},
	author_keywords = {Agriculture; context of use; ontology quality; quality assessment; quality in use; software quality},
	keywords = {Artificial intelligence; Computer software selection and evaluation; Decision support systems; Ontology; Quality assurance; Quality control; Context of use; Hardware; Ontology engineering; Ontology qualities; Ontology's; Quality assessment; Quality in use; Software; Software Quality; Agriculture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access}
}

@ARTICLE{Barriga20221135,
	author = {Barriga, Angela and Rutle, Adrian and Heldal, Rogardt},
	title = {AI-powered model repair: an experience report—lessons learned, challenges, and opportunities},
	year = {2022},
	journal = {Software and Systems Modeling},
	volume = {21},
	number = {3},
	pages = {1135 – 1157},
	doi = {10.1007/s10270-022-00983-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124985776&doi=10.1007%2fs10270-022-00983-5&partnerID=40&md5=f3ddec9ca15934c82f63a722471ece60},
	abstract = {Artificial intelligence has already proven to be a powerful tool to automate and improve how we deal with software development processes. The application of artificial intelligence to model-driven engineering projects is becoming more and more popular; however, within the model repair field, the use of this technique remains mostly an open challenge. In this paper, we explore some existing approaches in the field of AI-powered model repair. From the existing approaches in this field, we identify a series of challenges which the community needs to overcome. In addition, we present a number of research opportunities by taking inspiration from other fields which have successfully used artificial intelligence, such as code repair. Moreover, we discuss the connection between the existing approaches and the opportunities with the identified challenges. Finally, we present the outcomes of our experience of applying artificial intelligence to model repair. © 2022, The Author(s).},
	author_keywords = {Artificial intelligence; Challenges; Model repair; Opportunities},
	keywords = {Artificial intelligence; Repair; Challenge; Engineering program; Experience report; Model repair; Model-driven Engineering; Opportunity; Research opportunities; Software development process; Software design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Li2021149043,
	author = {Li, Yanjun and Huang, Huan and Guo, Xinwei and Yuan, Yuyu},
	title = {An empirical study on group fairness metrics of judicial data},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {149043 – 149049},
	doi = {10.1109/ACCESS.2021.3122443},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118535350&doi=10.1109%2fACCESS.2021.3122443&partnerID=40&md5=930770d8fefd7d3133326d61200ff517},
	abstract = {Group fairness means that different groups have an equal probability of being predicted for one aspect. It is a significant fairness definition, which is conducive to maintaining social harmony and stability. Fairness is a vital issue when an artificial intelligence software system is used to make judicial decisions. Either data or algorithm alone may lead to unfair results. Determining the fairness of the dataset is a prerequisite for studying the fairness of algorithms. This paper focuses on the dataset to research group fairness from both micro and macro views. We propose a framework to determine the sensitive attributes of a dataset and metrics to measure the fair degree of sensitive attributes. We conducted experiments and statistical analysis of the judicial data to demonstrate the framework and metric approach better. The framework and metric approach can be applied to datasets of other domains, providing persuasive evidence for the effectiveness and availability of algorithmic fairness research. It opens up a new way for the research of the fairness of the dataset. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author_keywords = {Data fairness; Judicial data; Metric; Sensitive attributes},
	keywords = {Artificial intelligence; Gaussian distribution; Learning algorithms; Software testing; Benchmark testing; Data fairness; Empirical studies; Equal probability; Judicial data; Machine learning algorithms; Metric; Sensitive attribute; Software algorithms; Software-systems; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Aslam20201519,
	author = {Aslam, Kousar and Cleophas, Loek and Schiffelers, Ramon and van den Brand, Mark},
	title = {Interface protocol inference to aid understanding legacy software components},
	year = {2020},
	journal = {Software and Systems Modeling},
	volume = {19},
	number = {6},
	pages = {1519 – 1540},
	doi = {10.1007/s10270-020-00809-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087287848&doi=10.1007%2fs10270-020-00809-2&partnerID=40&md5=8cb4e3cf2d76aa910db8b229f83eaaf1},
	abstract = {High-tech companies are struggling today with the maintenance of legacy software. Legacy software is vital to many organizations as it contains the important business logic. To facilitate maintenance of legacy software, a comprehensive understanding of the software’s behavior is essential. In terms of component-based software engineering, it is necessary to completely understand the behavior of components in relation to their interfaces, i.e., their interface protocols, and to preserve this behavior during the maintenance activities of the components. For this purpose, we present an approach to infer the interface protocols of software components from the behavioral models of those components, learned by a blackbox technique called active (automata) learning. To validate the learned results, we applied our approach to the software components developed with model-based engineering so that equivalence can be checked between the learned models and the reference models, ensuring the behavioral relations are preserved. Experimenting with components having reference models and performing equivalence checking builds confidence that applying active learning technique to reverse engineer legacy software components, for which no reference models are available, will also yield correct results. To apply our approach in practice, we present an automated framework for conducting active learning on a large set of components and deriving their interface protocols. Using the framework, we validated our methodology by applying active learning on 202 industrial software components, out of which, interface protocols could be successfully derived for 156 components within our given time bound of 1 h for each component. © 2020, The Author(s).},
	author_keywords = {Active automata learning; Equivalence oracles; Interface protocols; Learning framework},
	keywords = {Artificial intelligence; Learning systems; Legacy systems; Component-based software engineering; Equivalence checking; High tech companies; Industrial software; Interface protocol; Maintenance activity; Model-based engineering; Software component; Computer software maintenance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Megerian2022,
	author = {Megerian, Jonathan T. and Dey, Sangeeta and Melmed, Raun D. and Coury, Daniel L. and Lerner, Marc and Nicholls, Christopher J. and Sohl, Kristin and Rouhbakhsh, Rambod and Narasimhan, Anandhi and Romain, Jonathan and Golla, Sailaja and Shareef, Safiullah and Ostrovsky, Andrey and Shannon, Jennifer and Kraft, Colleen and Liu-Mayo, Stuart and Abbas, Halim and Gal-Szabo, Diana E. and Wall, Dennis P. and Taraman, Sharief},
	title = {Evaluation of an artificial intelligence-based medical device for diagnosis of autism spectrum disorder},
	year = {2022},
	journal = {npj Digital Medicine},
	volume = {5},
	number = {1},
	doi = {10.1038/s41746-022-00598-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129593842&doi=10.1038%2fs41746-022-00598-6&partnerID=40&md5=3fea439b0fe2a4b719e58bd33360c270},
	abstract = {Autism spectrum disorder (ASD) can be reliably diagnosed at 18 months, yet significant diagnostic delays persist in the United States. This double-blinded, multi-site, prospective, active comparator cohort study tested the accuracy of an artificial intelligence-based Software as a Medical Device designed to aid primary care healthcare providers (HCPs) in diagnosing ASD. The Device combines behavioral features from three distinct inputs (a caregiver questionnaire, analysis of two short home videos, and an HCP questionnaire) in a gradient boosted decision tree machine learning algorithm to produce either an ASD positive, ASD negative, or indeterminate output. This study compared Device outputs to diagnostic agreement by two or more independent specialists in a cohort of 18–72-month-olds with developmental delay concerns (425 study completers, 36% female, 29% ASD prevalence). Device output PPV for all study completers was 80.8% (95% confidence intervals (CI), 70.3%–88.8%) and NPV was 98.3% (90.6%–100%). For the 31.8% of participants who received a determinate output (ASD positive or negative) Device sensitivity was 98.4% (91.6%–100%) and specificity was 78.9% (67.6%–87.7%). The Device’s indeterminate output acts as a risk control measure when inputs are insufficiently granular to make a determinate recommendation with confidence. If this risk control measure were removed, the sensitivity for all study completers would fall to 51.6% (63/122) (95% CI 42.4%, 60.8%), and specificity would fall to 18.5% (56/303) (95% CI 14.3%, 23.3%). Among participants for whom the Device abstained from providing a result, specialists identified that 91% had one or more complex neurodevelopmental disorders. No significant differences in Device performance were found across participants’ sex, race/ethnicity, income, or education level. For nearly a third of this primary care sample, the Device enabled timely diagnostic evaluation with a high degree of accuracy. The Device shows promise to significantly increase the number of children able to be diagnosed with ASD in a primary care setting, potentially facilitating earlier intervention and more efficient use of specialist resources. © 2022, The Author(s).},
	keywords = {Decision trees; Diagnosis; Diseases; Learning algorithms; Machine learning; Software testing; Surveys; Autism spectrum disorders; Cohort studies; Confidence interval; Control measures; Health care providers; Medical Devices; Multi-site; Primary care; Prospectives; Risks controls; anxiety disorder; Article; artificial intelligence; attention deficit hyperactivity disorder; autism; caregiver; child; cohort analysis; communication disorder; compulsion; controlled study; developmental delay; developmental disorder; diagnostic accuracy; double blind procedure; education; ethnicity; evaluation study; female; gender; health care personnel; human; income; intellectual impairment; language disability; learning disorder; major clinical study; male; medical specialist; motor dysfunction; multicenter study; oppositional defiant disorder; preschool child; prevalence; primary medical care; prospective study; questionnaire; race; sensitivity and specificity; speech sound disorder; videorecording; Risk assessment},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 53; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Joshi202323634,
	author = {Joshi, Gargi and Srivastava, Ananya and Yagnik, Bhargav and Hasan, Mohammed and Saiyed, Zainuddin and Gabralla, Lubna A. and Abraham, Ajith and Walambe, Rahee and Kotecha, Ketan},
	title = {Explainable Misinformation Detection Across Multiple Social Media Platforms},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {23634 – 23646},
	doi = {10.1109/ACCESS.2023.3251892},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149838685&doi=10.1109%2fACCESS.2023.3251892&partnerID=40&md5=e88950d73b65f9cc4d686e372ebe9363},
	abstract = {Web Information Processing (W.I.P.) has enormously impacted modern society since a huge percentage of the population relies on the internet to acquire information. Social Media platforms provide a channel for disseminating information and a breeding ground for spreading misinformation, creating confusion and fear among the population. One of the techniques for the detection of misinformation is machine learning-based models. However, due to the availability of multiple social media platforms, developing and training AI-based models has become a tedious job. Despite multiple efforts to develop machine learning-based methods for identifying misinformation, more work must be done on developing an explainable generalized detector capable of robust detection and generating explanations beyond black-box outcomes. Knowing the reasoning behind the outcomes is essential to make the detector trustworthy. Hence employing explainable A.I. techniques is of utmost importance. In this work, the integration of two machine learning approaches, namely domain adaptation and explainable A.I., is proposed to address these two issues of generalized detection and explainability. Firstly the Domain Adversarial Neural Network (DANN) develops a generalized misinformation detector across multiple social media platforms. DANN generates the classification results for test domains with relevant but unseen data. The DANN-based, traditional black-box model cannot justify and explain its outcome, i.e., the labels for the target domain. Hence a Local Interpretable Model-Agnostic Explanations (LIME) explainable A.I. model is applied to explain the outcome of the DANN model. To demonstrate these two approaches and their integration for effective explainable generalized detection, COVID-19 misinformation is considered a case study. We experimented with two datasets and compared results with and without DANN implementation. It is observed that using DANN significantly improves the F1 score of classification and increases the accuracy by 3% and A.U.C. by 9%. The results show that the proposed framework performs well in the case of domain shift and can learn domain-invariant features while explaining the target labels with LIME implementation. This can enable trustworthy information processing and extraction to combat misinformation effectively.  © 2013 IEEE.},
	author_keywords = {Covid 19; DANN; lime; misinformation detection; social media; text processing; web information processing; XAI},
	keywords = {Artificial intelligence; Information dissemination; Learning systems; Social networking (online); Text processing; Covid 19; Domain adversarial neural network; Misinformation detection; Neural-networks; Social media; Social media platforms; Text-processing; Web information; Web information processing; XAI; Lime},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Liu2022,
	author = {Liu, Zheng and Li, Rong},
	title = {Attention Analysis and Information Dissemination Effect of Young People Based on Multisource Sensors under the Background of Wireless Communication and Artificial Intelligence},
	year = {2022},
	journal = {Scientific Programming},
	volume = {2022},
	doi = {10.1155/2022/5673435},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126583002&doi=10.1155%2f2022%2f5673435&partnerID=40&md5=667ba2801b537e80080e1c83cb3caaf3},
	abstract = {In recent years, the development of wireless communication and artificial intelligence technology has brought tremendous convenience to human life. Especially in the fields of education and medical treatment, the functions of wireless communication are the most representative. In order to enhance the attention of young people and the effect of information dissemination, scholars have conducted many analyses on its influencing factors. Studies have found that strong noise environments have obvious negative effects on attention and information dissemination effects. Therefore, this paper proposes a multisource sensor-based method for young people's attention analysis and information dissemination effect enhancement in a strong noise environment. Multisource sensors are selected to perform young people's attention analysis on speech in a strong noise environment, combined with the information dissemination effect to strengthen the model obtained by combining attention analysis and information dissemination effect strengthened speech. Through the multisource sensor system, it is demonstrated that the method in this paper is superior to the traditional method in a strong noise environment. And through the comparative analysis of multisource sensors, the corresponding improvement plan for the strong noise environment was formulated, and the numerical simulation experiment of young people's attention analysis and information dissemination effect was carried out to verify the optimization results. This research, based on wireless communication and artificial intelligence decision-making technology, proposes to evaluate and optimize the attention analysis and information dissemination effect of young people in a strong noisy environment based on multisource sensors and gives useful ideas to ensure the system design method in a strong noisy environment and the environmental quality after implementation. It also provides support and help for people to more specifically prevent the adverse effects on people's attention analysis and information dissemination effects in a strong noise environment. This paper designs a model based on multisource sensors to study the effect of young people's attention analysis and information dissemination in a strong noise environment, which is of great significance to the development of people's high-quality life. © 2022 Zheng Liu and Rong Li.},
	keywords = {Artificial intelligence; Behavioral research; Decision making; Environmental technology; Quality control; Artificial intelligence technologies; Comparative analyzes; Human lives; Medical treatment; Multi-Sources; Noise environments; Noisy environment; Sensor systems; Wireless communications; Young peoples; Information dissemination},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Kumar2022531,
	author = {Kumar, Gagan and Chopra, Vinay},
	title = {Hybrid Approach for Automated Test Data Generation},
	year = {2022},
	journal = {Journal of ICT Standardization},
	volume = {10},
	number = {4},
	pages = {531 – 562},
	doi = {10.13052/jicts2245-800X.1043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145436055&doi=10.13052%2fjicts2245-800X.1043&partnerID=40&md5=7e27da072a80ae54f8540ebd3e8911ea},
	abstract = {Software testing has long been thought to be a good technique to improve the software quality and reliability. Path testing is the most reliable software testing technique and the key method for improving software quality among all testing approaches. On the other hand, test data quality has a big impact on the software testing activity's ability to detect errors or defects. To solving testing problem, one must locate the entire search space for the relevant input data to encompass the different paths in the testable program. To satisfy path coverage, it is vital test to look at the accumulated test data across the thorough search area. A new approach based on ant colony optimization and negative selection algorithm (HACO-NSA) is presented in this research which overcome the flaws associated with search-based test data by generated automated test data. The optimum path testing objective is to generate appropriate test data to maximise coverage and to enhance the test data's efficacy, as a result, the test data's adequacy is validated using a path-based fitness function. In the NSA generation stage, the suggested method alters the new detectors creation using ACO. The proposed approach is evaluated for metrics such as average coverage, average generation, average time, and success rate and comparison has been done with random testing, ant colony optimization and negative selection algorithm Different benchmark programs have been used for object-oriented system. The findings show that the hybrid methodology escalates the coverage percentage and curtail test data size, reduces the redundancy in data and enhances the efficiency. The proposed approach is follows IEEE 829-2008 test documentation in entire testing process. © 2022 River Publishers.},
	author_keywords = {ant colony optimization; artificial immune search; metaheuristic search; negative selection algorithm; path coverage; Test data generation},
	keywords = {Ability testing; Artificial intelligence; Computer software selection and evaluation; Object oriented programming; Software reliability; Software testing; Artificial immune; Artificial immune search; Meta-heuristic search; Negative-selection algorithms; Path coverage; Path testing; Software Quality; Software testings; Test data; Test data generation; Ant colony optimization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Sarabi2022,
	author = {Sarabi, Shahryar and Han, Qi and de Vries, Bauke and Romme, A. Georges L. and Almassy, Dora},
	title = {The Nature-Based Solutions Case-Based System: A hybrid expert system},
	year = {2022},
	journal = {Journal of Environmental Management},
	volume = {324},
	doi = {10.1016/j.jenvman.2022.116413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139324359&doi=10.1016%2fj.jenvman.2022.116413&partnerID=40&md5=fe452fafb6d2c5fd5a441fcf046578aa},
	abstract = {Deriving knowledge and learning from past experiences is essential for the successful adoption of Nature-Based Solutions (NBS) as novel integrative solutions that involve many uncertainties. Past experiences in implementing NBS have been collected in a number of repositories; however, it is a major challenge to derive knowledge from the huge amount of information provided by these repositories. This calls for information systems that can facilitate the knowledge extraction process. This paper introduces the NBS Case-Based System (NBS-CBS), an expert system that uses a hybrid architecture to derive information and recommendations from an NBS experience repository. The NBS-CBS combines a ‘black-box’ artificial neural networks model with a ‘white-box’ case-based reasoning model to deliver an intelligent, adaptive, and explainable system. Experts have tested this system to assess its functionality and accuracy. Accordingly, the NBS-CBS appears to provide inspirational recommendations and information for the NBS planning and design process. © 2022 The Authors},
	author_keywords = {Artificial intelligence; Case-based reasoning; Expert system; Knowledge extraction; Nature-based solutions; NBS},
	keywords = {Expert Systems; Neural Networks, Computer; Electric circuit breakers; Expert systems; Extraction; Neural networks; Amount of information; Case based systems; Casebased reasonings (CBR); Extraction process; Hybrid architectures; Hybrid expert system; Knowledge extraction; Nature-based solution; Uncertainty; artificial intelligence; design; expert system; information system; knowledge; learning; article; artificial intelligence; artificial neural network; case based reasoning; expert system; extraction; human; Case based reasoning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Dinh2022,
	author = {Dinh, Nghia and Ogiela, Lidia},
	title = {Human-artificial intelligence approaches for secure analysis in CAPTCHA codes},
	year = {2022},
	journal = {Eurasip Journal on Information Security},
	volume = {2022},
	number = {1},
	doi = {10.1186/s13635-022-00134-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143727730&doi=10.1186%2fs13635-022-00134-9&partnerID=40&md5=f19489bd120175214486e1a43bae4551},
	abstract = {CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) has long been used to keep automated bots from misusing web services by leveraging human-artificial intelligence (HAI) interactions to distinguish whether the user is a human or a computer program. Various CAPTCHA schemes have been proposed over the years, principally to increase usability and security against emerging bots and hackers performing malicious operations. However, automated attacks have effectively cracked all common conventional schemes, and the majority of present CAPTCHA methods are also vulnerable to human-assisted relay attacks. Invisible reCAPTCHA and some approaches have not yet been cracked. However, with the introduction of fourth-generation bots accurately mimicking human behavior, a secure CAPTCHA would be hardly designed without additional special devices. Almost all cognitive-based CAPTCHAs with sensor support have not yet been compromised by automated attacks. However, they are still compromised to human-assisted relay attacks due to having a limited number of challenges and can be only solved using trusted devices. Obviously, cognitive-based CAPTCHA schemes have an advantage over other schemes in the race against security attacks. In this study, as a strong starting point for creating future secure and usable CAPTCHA schemes, we have offered an overview analysis of HAI between computer users and computers under the security aspects of open problems, difficulties, and opportunities of current CAPTCHA schemes. © 2022, The Author(s).},
	author_keywords = {CAPTCHA codes; Human-artificial intelligence; Secure analysis},
	keywords = {Artificial intelligence; Automation; Behavioral research; Botnet; Codes (symbols); Electronic mail filters; Human computer interaction; Network security; Personal computing; Web services; Automated attacks; Completely automated public turing test to tell computer and human apart code; Conventional schemes; Fourth generation; Human-artificial intelligence; Relay attack; Secure analyse; Turing tests; Usability and security; Webs services; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access}
}

@ARTICLE{Ma2022,
	author = {Ma, Mingrui and Han, Lansheng and Qian, Yekui},
	title = {CVDF DYNAMIC—A Dynamic Fuzzy Testing Sample Generation Framework Based on BI-LSTM and Genetic Algorithm},
	year = {2022},
	journal = {Sensors},
	volume = {22},
	number = {3},
	doi = {10.3390/s22031265},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124077480&doi=10.3390%2fs22031265&partnerID=40&md5=26832127151a5d4832f1d7a1ddb7acb6},
	abstract = {As one of the most effective methods of vulnerability mining, fuzzy testing has scalability and complex path detection ability. Fuzzy testing sample generation is the key step of fuzzy testing, and the quality of sample directly determines the vulnerability mining ability of fuzzy tester. At present, the known sample generation methods focus on code coverage or seed mutation under a critical execution path, so it is difficult to take both into account. Therefore, based on the idea of ensemble learning in artificial intelligence, we propose a fuzzy testing sample generation framework named CVDF DYNAMIC, which is based on genetic algorithm and BI-LSTM neural network. The main purpose of CVDF DYNAMIC is to generate fuzzy testing samples with both code coverage and path depth detection ability. CVDF DYNAMIC generates its own test case sets through BILSTM neural network and genetic algorithm. Then, we integrate the two sample sets through the idea of ensemble learning to obtain a sample set with both code coverage and vulnerability mining ability for a critical execution path of the program. In order to improve the efficiency of fuzzy testing, we use heuristic genetic algorithm to simplify the integrated sample set. We also innovatively put forward the evaluation index of path depth detection ability (pdda), which can effectively measure the vulnerability mining ability of the generated test case set under the critical execution path of the program. Finally, we compare CVDF DYNAMIC with some existing fuzzy testing tools and scientific research results and further propose the future improvement ideas of CVDF DYNAMIC. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Bi-LSTM neural network; Deep learning; Fuzzy testing sample generation; Genetic algorithm},
	keywords = {Algorithms; Artificial Intelligence; Forecasting; Fuzzy Logic; Learning; Neural Networks, Computer; Codes (symbols); Fuzzy inference; Fuzzy neural networks; Learning algorithms; Long short-term memory; Software testing; Bi-LSTM neural network; Code coverage; Deep learning; Detection ability; Execution paths; Fuzzy testing sample generation; Neural-networks; Sample generations; Testing samples; Vulnerabilities minings; algorithm; artificial intelligence; forecasting; fuzzy logic; learning; Genetic algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Loeb2021,
	author = {Loeb, Gerald E.},
	title = {A new approach to medical diagnostic decision support},
	year = {2021},
	journal = {Journal of Biomedical Informatics},
	volume = {116},
	doi = {10.1016/j.jbi.2021.103723},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102626468&doi=10.1016%2fj.jbi.2021.103723&partnerID=40&md5=7b071a30201dee00026f79c47328d672},
	abstract = {Data mining is a powerful tool to reduce costs and mitigate errors in the diagnostic analysis and repair of complex engineered system, but it has yet to be applied systematically to the most complex and socially expensive system – the human body. The currently available approaches of knowledge-based and pattern-based artificial intelligence are unsuited to the iterative and often subjective nature of clinician-patient interactions. Furthermore, current electronic health records generally have poor design and low quality for such data mining. Bayesian methods have been developed to suggest multiple possible diagnoses given a set of clinical findings, but the larger problem is advising the physician on useful next steps. A new approach based on inverting Bayesian inference allows identification of the diagnostic actions that are most likely to disambiguate a differential diagnosis at each point in a patient's work-up. This can be combined with personalized cost information to suggest a cost-effective path to the clinician. Because the software is tracking the clinician's decision-making process, it can provide salient suggestions for both diagnoses and diagnostic tests in standard, coded formats that need only to be selected. This would reduce the need to type in free text, which is prone to ambiguities, omissions and errors. As the database of high-quality records grows, the scope, utility and acceptance of the system should also grow automatically, without requiring expert updating or correction. © 2021},
	author_keywords = {Artificial intelligence; Bayesian; Cost-effectiveness; Diagnostic tests; Electronic health records; Learning health system},
	keywords = {Artificial Intelligence; Bayes Theorem; Electronic Health Records; Humans; Knowledge Bases; Software; Artificial intelligence; Bayesian networks; Cost benefit analysis; Cost effectiveness; Decision making; Decision support systems; Diagnosis; Inference engines; Iterative methods; Software testing; Complex engineered systems; Cost effective paths; Decision making process; Diagnostic analysis; Differential diagnosis; Electronic health record; Medical diagnostics; Patient interaction; adult; article; artificial intelligence; cost effectiveness analysis; data mining; decision support system; differential diagnosis; electronic health record; human; learning health system; physician; software; Bayes theorem; electronic health record; knowledge base; software; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Bronze Open Access}
}

@ARTICLE{Myllyaho2021,
	author = {Myllyaho, Lalli and Raatikainen, Mikko and Männistö, Tomi and Mikkonen, Tommi and Nurminen, Jukka K.},
	title = {Systematic literature review of validation methods for AI systems},
	year = {2021},
	journal = {Journal of Systems and Software},
	volume = {181},
	doi = {10.1016/j.jss.2021.111050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112415717&doi=10.1016%2fj.jss.2021.111050&partnerID=40&md5=5effdf8b47fc5b128e3b499b5557b7d2},
	abstract = {Context: Artificial intelligence (AI) has made its way into everyday activities, particularly through new techniques such as machine learning (ML). These techniques are implementable with little domain knowledge. This, combined with the difficulty of testing AI systems with traditional methods, has made system trustworthiness a pressing issue. Objective: This paper studies the methods used to validate practical AI systems reported in the literature. Our goal is to classify and describe the methods that are used in realistic settings to ensure the dependability of AI systems. Method: A systematic literature review resulted in 90 papers. Systems presented in the papers were analysed based on their domain, task, complexity, and applied validation methods. Results: The validation methods were synthesized into a taxonomy consisting of trial, simulation, model-centred validation, and expert opinion. Failure monitors, safety channels, redundancy, voting, and input and output restrictions are methods used to continuously validate the systems after deployment. Conclusions: Our results clarify existing strategies applied to validation. They form a basis for the synthesization, assessment, and refinement of AI system validation in research and guidelines for validating individual systems in practice. While various validation strategies have all been relatively widely applied, only few studies report on continuous validation. © 2021 The Author(s)},
	author_keywords = {Artificial intelligence; Machine learning; Systematic literature review; Testing; V&V; Validation},
	keywords = {Software engineering; AI systems; Domain knowledge; Expert opinion; Individual systems; Input and outputs; Synthesization; Systematic literature review; Validation strategies; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Akrami202378879,
	author = {Akrami, Nouhaila El and Hanine, Mohamed and Flores, Emmanuel Soriano and Aray, Daniel Gavilanes and Ashraf, Imran},
	title = {Unleashing the Potential of Blockchain and Machine Learning: Insights and Emerging Trends From Bibliometric Analysis},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {78879 – 78903},
	doi = {10.1109/ACCESS.2023.3298371},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165866619&doi=10.1109%2fACCESS.2023.3298371&partnerID=40&md5=332ca071cf8a184801210e62466ab29c},
	abstract = {Blockchain and machine learning (ML) has garnered growing interest as cutting-edge technologies that have witnessed tremendous strides in their respective domains. Blockchain technology provides a decentralized and immutable ledger, enabling secure and transparent transactions without intermediaries. Alternatively, ML is a sub-field of artificial intelligence (AI) that empowers systems to enhance their performance by learning from data. The integration of these data-driven paradigms holds the potential to reinforce data privacy and security, improve data analysis accuracy, and automate complex processes. The confluence of blockchain and ML has sparked increasing interest among scholars and researchers. Therefore, a bibliometric analysis is carried out to investigate the key focus areas, hotspots, potential prospects, and dynamical aspects of the field. This paper evaluates 700 manuscripts drawn from the Web of Science (WoS) core collection database, spanning from 2017 to 2022. The analysis is conducted using advanced bibliometric tools (e.g., Bibliometrix R, VOSviewer, and CiteSpace) to assess various aspects of the research area regarding publication productivity, influential articles, prolific authors, the productivity of academic countries and institutions, as well as the intellectual structure in terms of hot topics and emerging trends. The findings suggest that upcoming research should focus on blockchain technology, AI-powered 5G networks, industrial cyber-physical systems, IoT environments, and autonomous vehicles. This paper provides a valuable foundation for both academic scholars and practitioners as they contemplate future projects on the integration of blockchain and ML.  © 2013 IEEE.},
	author_keywords = {bibliometric analysis; Blockchain; machine learning; network visualization},
	keywords = {5G mobile communication systems; Artificial intelligence; Blockchain; Data privacy; Embedded systems; Hydrogenation; Industrial research; Learning systems; Bibliometric; Bibliometrics analysis; Block-chain; Emerging trends; Machine-learning; Market researches; Network visualization; Systematic; Visualization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; All Open Access, Gold Open Access}
}

@ARTICLE{Liu2022,
	author = {Liu, Xiaofeng and Han, Li},
	title = {Artificial Intelligence Enterprise Management Using Deep Learning},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/2422434},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133101467&doi=10.1155%2f2022%2f2422434&partnerID=40&md5=0c3dad18be5baff4fa28b4088857ed17},
	abstract = {In this paper, we explore the application status of deep learning (DL) in enterprise management, with China Merchants Bank as an example, and the role of DL in bank enterprise management. We analysed the application status of AI in marketing, risk control, investment, and other fields of CMB and identified five types of problems encountered in the current practical application of AI. We proposed five countermeasures: strengthening the AI organisation system's construction, enhancing the financial data guarantee mechanism, concentrating on customer-oriented, tightly managing the danger of AI technology, and building a full AI talent system. Recent data are used to assess the impact of DL in marketing, risk management, and investment consulting. According to the data, by the end of 2019, the number of clients of CMB's two APP platforms had reached 114 million and 91.2643 million, respectively. In 2019, CMB's personal savings balance climbed by roughly 53% compared to 2016, and its personal loan amount increased by approximately 61%. These findings indicate that the use of AI improves consumer happiness and trust in businesses. © 2022 Xiaofeng Liu and Li Han.},
	keywords = {Artificial Intelligence; China; Deep Learning; Humans; Commerce; Deep learning; Risk assessment; Risk management; 'current; AI Technologies; Application status; Applications of AI; Customers oriented; Enterprise management; Financial data; Organization system; Risks controls; System construction; artificial intelligence; China; human; Marketing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Fisher2021,
	author = {Fisher, Michael and Mascardi, Viviana and Rozier, Kristin Yvonne and Schlingloff, Bernd-Holger and Winikoff, Michael and Yorke-Smith, Neil},
	title = {Towards a framework for certification of reliable autonomous systems},
	year = {2021},
	journal = {Autonomous Agents and Multi-Agent Systems},
	volume = {35},
	number = {1},
	doi = {10.1007/s10458-020-09487-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105747603&doi=10.1007%2fs10458-020-09487-2&partnerID=40&md5=a5502fcaa489f62e59c2cc00308270da},
	abstract = {A computational system is called autonomous if it is able to make its own decisions, or take its own actions, without human supervision or control. The capability and spread of such systems have reached the point where they are beginning to touch much of everyday life. However, regulators grapple with how to deal with autonomous systems, for example how could we certify an Unmanned Aerial System for autonomous use in civilian airspace? We here analyse what is needed in order to provide verified reliable behaviour of an autonomous system, analyse what can be done as the state-of-the-art in automated verification, and propose a roadmap towards developing regulatory guidelines, including articulating challenges to researchers, to engineers, and to regulators. Case studies in seven distinct domains illustrate the article. © 2020, The Author(s).},
	author_keywords = {Artificial intelligence; Autonomous systems; Certification; Verification},
	keywords = {Artificial intelligence; Automated verification; Autonomous system; Certification; Civilian airspace; Computational system; Human control; Human supervision; Roadmap; State of the art; Unmanned aerial systems; Antennas},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 63; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Sonobe2022,
	author = {Sonobe, Tomohiro},
	title = {An Experimental Survey of Extended Resolution Effects for SAT Solvers on the Pigeonhole Principle},
	year = {2022},
	journal = {Algorithms},
	volume = {15},
	number = {12},
	doi = {10.3390/a15120479},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144595823&doi=10.3390%2fa15120479&partnerID=40&md5=5c65646e360a300fb28fc9d0d2334fac},
	abstract = {It has been proven that extended resolution (ER) has more powerful reasoning than general resolution for the pigeonhole principle in Cook’s paper. This fact indicates the possibility that a solver based on extended resolution can exceed Boolean satisfiability problem solvers (SAT solvers for short) based on general resolution. However, few studies have provided practical evidence of this assumption. This paper explores how extended resolution can improve SAT solvers by using the pigeonhole principle, which was the first problem solved by ER in polynomial steps. In fact, although Cook’s paper introduced how to add auxiliary variables, there is no evidence that these variables are really useful for practical solvers. We try to answer the question: If the SAT solver can add appropriate auxiliary variables as proposed in Cook’s paper, can the solver enhance its performance by utilizing these variables? Experimental results show that if the solver properly prioritizes the extended variables in the search, the solver can end the search in a shorter time. © 2022 by the author.},
	author_keywords = {extended resolution; SAT solver; search},
	keywords = {Artificial intelligence; Auxiliary variables; Boolean satisfiability problems; Experimental survey; Extended resolution; Pigeonhole principle; Problem solvers; Resolution effects; SAT solv; SAT solvers; Search; Model checking},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Abraham2021,
	author = {Abraham, Joanna and Meng, Alicia and Holzer, Katherine J. and Brawer, Luke and Casarella, Aparna and Avidan, Michael and Politi, Mary C.},
	title = {Exploring patient perspectives on telemedicine monitoring within the operating room},
	year = {2021},
	journal = {International Journal of Medical Informatics},
	volume = {156},
	doi = {10.1016/j.ijmedinf.2021.104595},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118751066&doi=10.1016%2fj.ijmedinf.2021.104595&partnerID=40&md5=54f7c2199ab238189b56415dbc2268ce},
	abstract = {Background: Clinical decision support systems and telemedicine for remote monitoring can together support clinicians' intraoperative decision-making and management of surgical patients' care. However, there has been limited investigation on patient perspectives about advanced health information technology use in intraoperative settings, especially an electronic OR (eOR) for remote monitoring and management of surgical patients. Purpose: Our study objectives were: (1) to identify participant-rated items contributing to patient attitudes, beliefs, and level of comfort with eOR monitoring; and (2) to highlight barriers and facilitators to eOR use. Methods: We surveyed 324 individuals representing surgical patients across the United States using Amazon Mechanical Turk, an online platform supporting internet-based work. The structured survey questions examined the level of agreement and comfort with eOR for remote patient monitoring. We calculated descriptive statistics for demographic variables and performed a Wilcoxon matched-pairs signed-rank test to assess whether participants were more comfortable with familiar clinicians from local hospitals or health systems monitoring their health and safety status during surgery than clinicians from hospitals or health systems in other regions or countries. We also analyzed open-ended survey responses using a thematic approach informed by an eight-dimensional socio-technical model. Results: Participants’ average age was 34.07 (SD = 10.11). Most were white (80.9%), male (57.1%), and had a high school degree or more (88.3%). Participants reported a higher level of comfort with clinicians they knew monitoring their health and safety than clinicians they did not know, even within the same healthcare system (z = -4.012, p <.001). They reported significantly higher comfort levels with clinicians within the same hospital or health system in the United States than those in a different country (z = -10.230, p <.001). Facilitators and barriers to eOR remote monitoring were prevalent across four socio-technical dimensions: 1) organizational policies, procedures, environment, and culture; 2) people; 3) workflow and communication; and 4) hardware and software. Facilitators to eOR use included perceptions of improved patient safety through a safeguard system and perceptions of streamlined care. Barriers included fears of incorrect eOR patient assessments, decision-making conflicts between care teams, and technological malfunctions. Conclusions: Participants expressed significant support for intraoperative telemedicine use and greater comfort with local telemedicine systems instead of long-distance telemedicine systems. Reservations centered on organizational policies, procedures, environment, culture; people; workflow and communication; and hardware and software. To improve the buy-in and acceptability of remote monitoring by an eOR team, we offer a few evidence-based guidelines applicable to telemedicine use within the context of OR workflow. Guidelines include backup plans for technical challenges, rigid care, and privacy standards, and patient education to increase understanding of telemedicine's potential to improve patient care. © 2021 Elsevier B.V.},
	author_keywords = {Operating room; Remote monitoring; Surgery; Telemedicine},
	keywords = {Adult; Communication; Hospitals; Humans; Male; Monitoring, Physiologic; Operating Rooms; Telemedicine; United States; Artificial intelligence; Behavioral research; Decision making; Decision support systems; Operating rooms; Software testing; Surgery; Surveys; Telemedicine; Decisions makings; Health and safety; Health systems; Intra-operative; Organizational policies; Patient care; Remote monitoring; Sociotechnical; Surgical patients; Work-flows; adult; Amazon Mechanical Turk; Article; clinical decision support system; clinician; controlled study; cultural factor; demography; female; health care personnel; health care system; health survey; high school; human; Internet; major clinical study; male; medical informatics; online social network; organizational policy; patient assessment; patient attitude; patient comfort; patient education; patient safety; practice guideline; remote sensing; software; surgical patient; telemedicine; telemonitoring; United States; Wilcoxon signed ranks test; workflow; hospital; interpersonal communication; operating room; physiologic monitoring; Remote control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Green Open Access}
}

@ARTICLE{Baumont De Oliveira202134,
	author = {Baumont De Oliveira, Francis J. and Ferson, Scott and Dyer, Ronald},
	title = {A Collaborative Decision Support System Framework for Vertical Farming Business Developments},
	year = {2021},
	journal = {International Journal of Decision Support System Technology},
	volume = {13},
	number = {1},
	pages = {34 – 66},
	doi = {10.4018/IJDSST.2021010103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096803309&doi=10.4018%2fIJDSST.2021010103&partnerID=40&md5=1a2b02019fc3fe88b88e0cf20f748573},
	abstract = {The emerging industry of vertical farming (VF) faces three key challenges: standardisation, environmental sustainability, and profitability. High failure rates are costly and can stem from premature business decisions about location choice, pricing strategy, system design, and other critical issues. Improving knowledge transfer and developing adaptable economic analysis for VF is necessary for profitable business models to satisfy investors and policy makers. A review of current horticultural software identifies a need for a decision support system (DSS) that facilitates risk-empowered business planning for vertical farmers. Data from the literature alongside lessons learned from industry practitioners are centralised in the proposed DSS, using imprecise data techniques to accommodate for partial information. The DSS evaluates business sustainability using financial risk assessment. This is necessary for complex/new sectors such as VF with scarce data. Copyright © 2021, IGI Global.},
	author_keywords = {Artificial Intelligence; Business Sustainability; Decision Support; Imprecise Data; Risk Assessment; Vertical Farming},
	keywords = {Economic analysis; Failure analysis; Knowledge management; Planning; Profitability; Risk assessment; Sustainable development; Business decisions; Business development; Business sustainability; Collaborative decisions; Decision support system (dss); Environmental sustainability; Knowledge transfer; Partial information; Decision support systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Vázquez-Cano2021,
	author = {Vázquez-Cano, Esteban and Mengual-Andrés, Santiago and López-Meneses, Eloy},
	title = {Chatbot to improve learning punctuation in Spanish and to enhance open and flexible learning environments},
	year = {2021},
	journal = {International Journal of Educational Technology in Higher Education},
	volume = {18},
	number = {1},
	doi = {10.1186/s41239-021-00269-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108941717&doi=10.1186%2fs41239-021-00269-8&partnerID=40&md5=f46dce8f3e3ce4d05884cd9d600cc3b1},
	abstract = {The objective of this article is to analyze the didactic functionality of a chatbot to improve the results of the students of the National University of Distance Education (UNED / Spain) in accessing the university in the subject of Spanish Language. For this, a quasi-experimental experiment was designed, and a quantitative methodology was used through pretest and posttest in a control and experimental group in which the effectiveness of two teaching models was compared, one more traditional based on exercises written on paper and another based on interaction with a chatbot. Subsequently, the perception of the experimental group in an academic forum about the educational use of the chatbot was analyzed through text mining with tests of Latent Dirichlet Allocation (LDA), pairwise distance matrix and bigrams. The quantitative results showed that the students in the experimental group substantially improved the results compared to the students with a more traditional methodology (experimental group / mean: 32.1346 / control group / mean: 28.4706). Punctuation correctness has been improved mainly in the usage of comma, colon and periods in different syntactic patterns. Furthermore, the perception of the students in the experimental group showed that they positively value chatbots in their teaching–learning process in three dimensions: greater “support” and companionship in the learning process, as they perceive greater interactivity due to their conversational nature; greater “feedback” and interaction compared to the more traditional methodology and, lastly, they especially value the ease of use and the possibility of interacting and learning anywhere and anytime. © 2021, The Author(s).},
	author_keywords = {Artificial intelligence; Chatbot; Communication; Education; Mobile learning; Virtual learning environments},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 126; All Open Access, Gold Open Access}
}

@ARTICLE{Wang202193,
	author = {Wang, Bin and Yuan, Jianjun and Ghafoor, Kayhan Zrar},
	title = {Research on Construction Cost Estimation Based on Artificial Intelligence Technology},
	year = {2021},
	journal = {Scalable Computing},
	volume = {22},
	number = {2},
	pages = {93 – 104},
	doi = {10.12694/scpe.v22i2.1868},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127012155&doi=10.12694%2fscpe.v22i2.1868&partnerID=40&md5=31522f48ebe4e51055d2d3186c75f60c},
	abstract = {For the prediction of economic expenses involved in construction industry, cost estimation has become an important aspect of construction management for the prediction of economic expenses and successful completion of the construction work. Cost analysis is crucial and require expertise for accurate and comprehensive estimation. In order to effectively improve the accuracy of construction project cost, this paper establishes an estimation model based on gray BP neural network. It combines the MATLAB toolbox for program design, and learns and tests the input and output of training samples. This article determines the application of grey system theory to optimize the estimation model of Back Propagation (BP) neural network. The viability of the method established in this article, is tested by collecting the engineering cost data in Zhengzhou city and comparing between the standard BP neural network and the gray BP neural network methods. The results show that the average error of the gray system theory optimized BP neural network model designed in this paper is 2.33%. The gray BP neural network model studied in this paper can not only quickly estimate the project cost, but also has high accuracy rate. The outcomes obtained establishes a model with scientific and reasonable construction project cost estimation © 2021 SCPE.},
	author_keywords = {Artificial intelligence; Bp neural network; Construction management; Engineering cost; Zhengzhou city},
	keywords = {Backpropagation; Construction industry; Cost benefit analysis; Cost engineering; Cost estimating; MATLAB; Project management; Software testing; System theory; Back-propagation neural networks; BP neural networks; Construction management; Construction project costs; Cost estimations; Engineering costs; Estimation models; Gray system theory; Neural network model; Zhengzhou city; Neural network models},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Khatibsyarbini2021166262,
	author = {Khatibsyarbini, Muhammad and Isa, Mohd Adham and Jawawi, Dayang N. A. and Shafie, Muhammad Luqman Mohd and Wan-Kadir, Wan Mohd Nasir and Hamed, Haza Nuzly Abdull and Suffian, Muhammad Dhiauddin Mohamed},
	title = {Trend Application of Machine Learning in Test Case Prioritization: A Review on Techniques},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {166262 – 166282},
	doi = {10.1109/ACCESS.2021.3135508},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121830569&doi=10.1109%2fACCESS.2021.3135508&partnerID=40&md5=60835d1547bb0261a6d092b1e35c36c4},
	abstract = {Software quality can be assured by passing the process of software testing. However, software testing process involve many phases which lead to more resources and time consumption. To reduce these downsides, one of the approaches is to adopt test case prioritization (TCP) where numerous works has indicated that TCP do improve the overall software testing performance. TCP does have several kinds of techniques which have their own strengths and weaknesses. As for this review paper, the main objective of this paper is to examine deeper on machine learning (ML) techniques based on research questions created. The research method for this paper was designed in parallel with the research questions. Consequently, 110 primary studies were selected where, 58 were journal articles, 50 were conference papers and 2 considered as others articles. For overall result, it can be said that ML techniques in TCP has trending in recent years yet some improvements are certainly welcomed. There are multiple ML techniques available, in which each technique has specified potential values, advantages, and limitation. It is notable that ML techniques has been considerably discussed in TCP approach for software testing.  © 2013 IEEE.},
	author_keywords = {Machine learning; Software engineering; Software testing; Systematic literature review; Test case prioritization},
	keywords = {Application programs; Artificial intelligence; Computer software selection and evaluation; Learning systems; Market Research; Transmission control protocol; License; Machine learning techniques; Machine-learning; Market researches; Software; Software testings; Systematic; Systematic literature review; Test case prioritization; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access}
}

@ARTICLE{Phung202330562,
	author = {Phung, Khoa and Ogunshile, Emmanuel and Aydin, Mehmet},
	title = {Error-Type - A Novel Set of Software Metrics for Software Fault Prediction},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {30562 – 30574},
	doi = {10.1109/ACCESS.2023.3262411},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151551046&doi=10.1109%2fACCESS.2023.3262411&partnerID=40&md5=0c3556037519ebe7585b5c7bd9b829a0},
	abstract = {In software development, identifying software faults is an important task. The presence of faults not only reduces the quality of the software, but also increases the cost of development life cycle. Fault identification can be performed by analysing the characteristics of the buggy source codes from the past and predict the present ones based on the same characteristics using statistical or machine learning models. Many studies have been conducted to predict the fault proneness of software systems. However, most of them provide either inadequate or insufficient information and thus make the fault prediction task difficult. In this paper, we present a novel set of software metrics called Error-type software metrics, which provides prediction models with information about patterns of different types of Java runtime error. Particular, in this study, the ESM values consist of information of three common Java runtime errors which are Index Out Of Bounds Exception, Null Pointer Exception, and Class Cast Exception. Also, we propose a methodology for modelling, extracting, and evaluating error patterns from software modules using Stream X-Machine (a formal modelling method) and machine learning techniques. The experimental results showed that the proposed Error-type software metrics could significantly improve the performances of machine learning models in fault-proneness prediction.  © 2013 IEEE.},
	author_keywords = {Error type prediction; machine learning; software fault prediction; software metrics; stream X-machine},
	keywords = {Artificial intelligence; Computer software; Errors; Forecasting; Java programming language; Learning systems; Software design; Error type prediction; Error types; Machine-learning; Predictive models; Runtimes; Software; Software fault prediction; Software metrics; Source-coding; Stream X-machines; Type predictions; Life cycle},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Gold Open Access}
}

@ARTICLE{Hance2022,
	author = {Hance, Jack and Milbrath, Jordan and Ross, Noah and Straub, Jeremy},
	title = {Distributed Attack Deployment Capability for Modern Automated Penetration Testing},
	year = {2022},
	journal = {Computers},
	volume = {11},
	number = {3},
	doi = {10.3390/computers11030033},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125487170&doi=10.3390%2fcomputers11030033&partnerID=40&md5=be3533eadc600510c41b3d6b9a7801ed},
	abstract = {Cybersecurity is an ever-changing landscape. The threats of the future are hard to predict and even harder to prepare for. This paper presents work designed to prepare for the cybersecurity landscape of tomorrow by creating a key support capability for an autonomous cybersecurity testing system. This system is designed to test and prepare critical infrastructure for what the future of cyberattacks looks like. It proposes a new type of attack framework that provides precise and granular attack control and higher perception within a set of infected infrastructure. The proposed attack framework is intelligent, supports the fetching and execution of arbitrary attacks, and has a small memory and network footprint. This framework facilitates autonomous rapid penetration testing as well as the evaluation of where detection systems and procedures are underdeveloped and require further improvement in preparation for rapid autonomous cyber-attacks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Artificial intelligence; Attack automation; Automate penetration testing; Blackboard Architecture; Cybersecurity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Bruneliere2022,
	author = {Bruneliere, Hugo and Muttillo, Vittoriano and Eramo, Romina and Berardinelli, Luca and Gómez, Abel and Bagnato, Alessandra and Sadovykh, Andrey and Cicchetti, Antonio},
	title = {AIDOaRt: AI-augmented Automation for DevOps, a model-based framework for continuous development in Cyber–Physical Systems},
	year = {2022},
	journal = {Microprocessors and Microsystems},
	volume = {94},
	doi = {10.1016/j.micpro.2022.104672},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138071342&doi=10.1016%2fj.micpro.2022.104672&partnerID=40&md5=e0e1ce282b8b3929d9c739c230de17cf},
	abstract = {The advent of complex Cyber–Physical Systems (CPSs) creates the need for more efficient engineering processes. Recently, DevOps promoted the idea of considering a closer continuous integration between system development (including its design) and operational deployment. Despite their use being still currently limited, Artificial Intelligence (AI) techniques are suitable candidates for improving such system engineering activities (cf. AIOps). In this context, AIDOaRT is a large European collaborative project that aims at providing AI-augmented automation capabilities to better support the modeling, coding, testing, monitoring, and continuous development of CPSs. The project proposes to combine Model Driven Engineering principles and techniques with AI-enhanced methods and tools for engineering more trustable CPSs. The resulting framework will (1) enable the dynamic observation and analysis of system data collected at both runtime and design time and (2) provide dedicated AI-augmented solutions that will then be validated in concrete industrial cases. This paper describes the main research objectives and underlying paradigms of the AIDOaRt project. It also introduces the conceptual architecture and proposed approach of the AIDOaRt overall solution. Finally, it reports on the actual project practices and discusses the current results and future plans. © 2022 Elsevier B.V.},
	author_keywords = {AIOps; Artificial Intelligence; Continuous development; Cyber–Physical Systems; DevOps; Model Driven Engineering; Software engineering; System engineering},
	keywords = {Artificial intelligence; Software engineering; AIOp; Continuous development; Continuous integrations; Cybe-physical systems; Cyber-physical systems; Engineering process; Model-based OPC; Model-driven Engineering; Operational deployments; System development; Systems engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Zhang20214,
	author = {Zhang, Jingjing},
	title = {Computer Assisted Instruction System Under Artificial Intelligence Technology},
	year = {2021},
	journal = {International Journal of Emerging Technologies in Learning},
	volume = {16},
	number = {5},
	pages = {4 – 16},
	doi = {10.3991/ijet.v16i05.20307},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103309718&doi=10.3991%2fijet.v16i05.20307&partnerID=40&md5=abc8784410516db54094aa7aaefed548},
	abstract = {In order to promote the development of intelligent teaching system, artificial intelligence technology is combined with computer assisted instruction (CAI) system to produce intelligent CAI (ICAI), and the design of ICAI is studied in order to facilitate the application and implementation of intelligent teaching system. First, there is an introduction of the basic concepts, related theories, and implementation principles of ICAI. Then, the requirements of ICAI are analyzed in detail on the basis of studying the relevant technologies required by ICAI, including the functional and non-functional requirements. The subsystem is set based on the different needs of the target users (administrator, teacher, and student users). Finally, the general and functional structure of ICAI are designed, and there are tests for the system functions. The results show that the system takes Java Server Pages (JSP) as the development language, Struts2 as the technical framework, and SQL Server 2008 as the back-end database, which realizes a teaching platform mainly for teachers, students, and administrators. After testing, the overall operation of the system is good, and the functional test results of the login module, homework release, and learning resource upload meet the software design requirements. The response time of the system prolongs, the number of users who can’t successfully log in grows, and the utilization of server central procession unit (CPU) and memory utilization both increase, which are caused by the increasing number of concurrent users. The results indicate that the combination of CAI and artificial intelligence (AI) technology can meet students’ online teaching demands, break the time-space limitation of traditional teaching, and provide important reference value for the reform of teaching mode and the improvement of teaching quality. © 2021},
	author_keywords = {Artificial intelligence; computer assisted instruction; intelligent computer assisted instruction; Java Server Pages; teaching mode},
	keywords = {Computer aided instruction; E-learning; Software design; Software testing; Students; Artificial intelligence technologies; Back-end database; Computer Assisted Instruction; Functional structure; Intelligent teaching systems; Memory utilization; Non-functional requirements; Technical frameworks; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@ARTICLE{Rehman202119395,
	author = {Rehman, Mujeeb Ur and Najam, Shaheryar and Khalid, Sohail and Shafique, Arslan and Alqahtani, Fehaid and Baothman, Fatmah and Shah, Syed Yaseen and Abbasi, Qammer H. and Imran, Muhammad Ali and Ahmad, Jawad},
	title = {Infrared Sensing Based Non-Invasive Initial Diagnosis of Chronic Liver Disease Using Ensemble Learning},
	year = {2021},
	journal = {IEEE Sensors Journal},
	volume = {21},
	number = {17},
	pages = {19395 – 19406},
	doi = {10.1109/JSEN.2021.3091471},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112409203&doi=10.1109%2fJSEN.2021.3091471&partnerID=40&md5=e4bbb35b3eef90634b3601f3041fc3dc},
	abstract = {The liver is a vital human body organ and its functionality can be degraded by several diseases such as hepatitis, fatty liver disease, and liver cancer and so forth. Hence, the early diagnosis of liver diseases is extremely crucial for saving human lives. With the rapid development of multimedia technology, it is now possible to design and implement a non-invasive system that can chronic liver diseases. For this purpose, machine learning and Artificial Intelligence (AI) have been used within the past few years. In this regard, digital image processing supported by AI methods has been implemented in the diagnosis of diseases that also showed high reliability. Therefore, in this paper, an iris feature-based non-invasive technique is proposed by incorporating a novel machine-learning algorithm. The experimental setup involved data set for the models' training included 879 subjects from Pakistan, of which 453 subjects have chronic liver disease and 426 are healthy. The iris images were collected using an infrared camera that consists of a lens, a thermal sensor and digital electronics processing. The lens focuses on the infrared energy on the sensor, using distinctive forms of features twenty-two physiological and thirty-three iris features. The designed classification model for a non-invasive system combined eleven different classifiers and used cross-validation techniques for comparing the results. The overall performance of the model was analyzed using five parameters: accuracy, precision, F-score, specificity, and sensitivity. The results confirmed that the proposed non-invasive model is capable of predicting chronic liver diseases with 98% of accuracy. © 2001-2012 IEEE.},
	author_keywords = {Artificial intelligence; chronic liver disease; complementary medicine technique; computer-aided diagnosis; ensemble classification; iridology; machine learning; stack learning; thermal sensor},
	keywords = {Diagnosis; Diseases; Image processing; Machine learning; Multimedia systems; Chronic liver disease; Classification models; Cross-validation technique; Design and implements; Digital electronics; Fatty liver disease; Multimedia technologies; Noninvasive technique; Learning algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access, Green Open Access}
}

@ARTICLE{Plucar202118,
	author = {Plucar, Jan and Frank, Jiří and Walter, Daniel and Zelinka, Ivan},
	title = {Intelligent malware – trends and possibilities},
	year = {2021},
	journal = {Mendel},
	volume = {27},
	number = {1},
	pages = {18 – 22},
	doi = {10.13164/mendel.2021.1.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109849910&doi=10.13164%2fmendel.2021.1.018&partnerID=40&md5=127215defb67cdac64f90ee46bba7df1},
	abstract = {In recent months and years, with more and more computers and computer systems becoming the target of cyberattacks. These attacks are gaining strength and the sophistication of the approach in terms of how to attack. Attackers and Defenders are increasingly using artificial intelligence methods to maximize the success of their actions. For a successful defence, we must be able to anticipate future threats that may come. For these reasons, our research group is engaged in creating experimental software with artificial intelligence to test the possibilities and capabilities of such malware in the event of its deployment. This software has not only malware capabilities but also antimalware and can be used on both sides. This article introduces the reader to the main principles of our design, which can serve as a future platform for cyber defence systems. © 2021, Brno University of Technology. All rights reserved.},
	author_keywords = {Artificial intelligence; Artificial neural network; Malware; Swarm},
	keywords = {Malware; Network security; Software testing; Swarm intelligence; Anti-malware; Artificial intelligence methods; Cyber-attacks; Cyber-defense; Defence systems; Future threats; Research groups; Swarm; Neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Kazimov202142,
	author = {Kazimov, T.H. and Bayramova, T.A. and Malikova, N.J.},
	title = {RESEARCH OF INTELLIGENT METHODS OF SOFTWARE TESTING},
	year = {2021},
	journal = {System Research and Information Technologies},
	volume = {2021},
	number = {4},
	pages = {42 – 52},
	doi = {10.20535/SRIT.2308-8893.2021.4.03},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124206542&doi=10.20535%2fSRIT.2308-8893.2021.4.03&partnerID=40&md5=16b7e9dd1262f5663d0dbdf31aed93ec},
	abstract = {This article presents the examination of several techniques and tools used in the automated software testing process. Considering the ever-growing importance of software testing, several possible implications of implementation of artificial intelligence into this area are also discussed. The main objective of this study is to examine the field of test automation by categorising related test activities, to which artificial intelligence tools can be applied for increased efficiency, and evalu-ate the impact of the application. The main software testing methods are white-box, black-box, and grey-box methods; an effort has been made to determine a connec-tion between the given testing methods and artificial intelligence methods. A brief summary of several artificial intelligence engine tools used to automate testing was also provided. Lastly, the possible future benefits from usage of AI in software testing was investigated.. © T.H. Kazimov, T.A. Bayramova, N.J. Malikova, 2021.},
	author_keywords = {Artificial intelligence; Automated testing; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Pan202249508,
	author = {Pan, Zhixin and Mishra, Prabhat},
	title = {A Survey on Hardware Vulnerability Analysis Using Machine Learning},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {49508 – 49527},
	doi = {10.1109/ACCESS.2022.3173287},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130075660&doi=10.1109%2fACCESS.2022.3173287&partnerID=40&md5=3381dde2737f937d920d51b959d73394},
	abstract = {Electronic systems rely on efficient hardware, popularly known as system-on-chip (SoC), to support its core functionalities. A typical SoC consists of diverse components gathered from third-party vendors to reduce SoC design cost and meet time-to-market constraints. Unfortunately, the participation of third-party companies in global supply chain introduces potential security vulnerabilities. There is a critical need to efficiently detect and mitigate hardware vulnerabilities. Machine learning has been successfully used in hardware security verification as well as development of effective countermeasures. There are recent surveys on hardware Trojan detection using machine learning. To the best of our knowledge, there are no comprehensive surveys on utilization of machine learning techniques for detection and mitigation of a wide variety of hardware vulnerabilities including malicious implants (e.g., hardware Trojans), side-channel leakage, reverse engineering, and supply-chain vulnerabilities (e.g., counterfeiting, overbuilding and recycling). In this paper, we provide a comprehensive survey of hardware vulnerability analysis using machine learning techniques. Specifically, we discuss how existing approaches effectively utilize machine learning algorithms for hardware security verification using simulation-based validation, formal verification as well as side-channel analysis.  © 2013 IEEE.},
	author_keywords = {embedded system; Hardware security; machine learning; vulnerability analysis},
	keywords = {Artificial intelligence; Embedded systems; Formal verification; Hardware security; Learning algorithms; Learning systems; Malware; Programmable logic controllers; Reverse engineering; Side channel attack; Supply chains; Surveys; System-on-chip; Classification algorithm; Core functionality; Electronics system; Embedded-system; Hardware; Machine learning techniques; Machine-learning; Security; Security verification; Vulnerability analysis; Decision trees},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access}
}

@ARTICLE{Tan2021,
	author = {Tan, Bin},
	title = {Soccer-assisted training robot based on image recognition omnidirectional movement},
	year = {2021},
	journal = {Wireless Communications and Mobile Computing},
	volume = {2021},
	doi = {10.1155/2021/5532210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114090768&doi=10.1155%2f2021%2f5532210&partnerID=40&md5=ec437e6945ff2ecb2eab8ab777f81402},
	abstract = {With the continuous emergence and innovation of computer technology, mobile robots are a relatively hot topic in the field of artificial intelligence. It is an important research area of more and more scholars. The core of mobile robots is to be able to realize real-time perception of the surrounding environment and self-positioning and to conduct self-navigation through this information. It is the key to the robot's autonomous movement and has strategic research significance. Among them, the goal recognition ability of the soccer robot vision system is the basis of robot path planning, motion control, and collaborative task completion. The main recognition task in the vision system is the omnidirectional vision system. Therefore, how to improve the accuracy of target recognition and the light adaptive ability of the robot omnidirectional vision system is the key issue of this paper. Completed the system construction and program debugging of the omnidirectional mobile robot platform, and tested its omnidirectional mobile function, positioning and map construction capabilities in the corridor and indoor environment, global navigation function in the indoor environment, and local obstacle avoidance function. How to use the local visual information of the robot more perfectly to obtain more available information, so that the "eyes"of the robot can be greatly improved by relying on image recognition technology, so that the robot can obtain more accurate environmental information by itself has always been domestic and foreign one of the goals of the joint efforts of scholars. Research shows that the standard error of the experimental group's shooting and dribbling test scores before and the experimental group's shooting and dribbling test results after the standard error level is 0.004, which is less than 0.05, which proves the use of soccer-assisted robot-assisted training. On the one hand, we tested the positioning and navigation functions of the omnidirectional mobile robot, and on the other hand, we verified the feasibility of positioning and navigation algorithms and multisensor fusion algorithms. © 2021 Bin Tan.},
	keywords = {Artificial intelligence; Computer vision; Educational robots; Environmental technology; Football; Image enhancement; Image recognition; Indoor positioning systems; Mobile robots; Motion estimation; Navigation; Program debugging; Robot applications; Software testing; Visual servoing; Environmental information; Image recognition technology; Local obstacle avoidances; Multisensor fusion algorithm; Omni-directional mobiles; Omnidirectional mobile robot; Omnidirectional vision system; Surrounding environment; Robot programming},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Lee2020,
	author = {Lee, Dong-Gun and Seo, Yeong-Seok},
	title = {Improving bug report triage performance using artificial intelligence based document generation model},
	year = {2020},
	journal = {Human-centric Computing and Information Sciences},
	volume = {10},
	number = {1},
	doi = {10.1186/s13673-020-00229-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086592271&doi=10.1186%2fs13673-020-00229-7&partnerID=40&md5=cbb5f71ff33b4a84f1c224e44ee00395},
	abstract = {Artificial intelligence is one of the key technologies for progression to the fourth industrial revolution. This technology also has a significant impact on software professionals who are continuously striving to achieve high-quality software development by fixing various types of software bugs. During the software development and maintenance stages, software bugs are the major factor that can affect the cost and time of software delivery. To efficiently fix a software bug, open bug repositories are used for identifying bug reports and for classifying and prioritizing the reports for assignment to the most appropriate software developers based on their level of interest and expertise. Owing to a lack of resources such as time and manpower, this bug report triage process is extremely important in software development. To improve the bug report triage performance, numerous studies have focused on a latent Dirichlet allocation (LDA) using the k-nearest neighbors or a support vector machine. Although the existing approaches have improved the accuracy of a bug triage, they often cause conflicts between the combined techniques and generate incorrect triage results. In this study, we propose a method for improving the bug report triage performance using multiple LDA-based topic sets by improving the LDA. The proposed method improves the existing topic sets of the LDA by building two adjunct topic sets. In our experiment, we collected bug reports from a popular bug tracking system, Bugzilla, as well as Android bug reports, to evaluate the proposed method and demonstrate the achievement of the following two goals: increase the bug report triage accuracy, and satisfy the compatibility with other state-of-the-art approaches. © 2020, The Author(s).},
	author_keywords = {Artificial intelligence; Bug report triage; Latent Dirichlet Allocation; Machine learning; Software defect prediction; Software engineering},
	keywords = {Nearest neighbor search; Software design; Statistics; Support vector machines; Combined techniques; High-quality software; Industrial revolutions; K-nearest neighbors; Latent dirichlet allocations; Open bug repositories; Software development and maintenances; State-of-the-art approach; Program debugging},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Williams20213470,
	author = {Williams, Helena and Cattani, Laura and Van Schoubroeck, Dominique and Yaqub, Mohammad and Sudre, Carole and Vercauteren, Tom and D'Hooge, Jan and Deprest, Jan},
	title = {Automatic Extraction of Hiatal Dimensions in 3-D Transperineal Pelvic Ultrasound Recordings},
	year = {2021},
	journal = {Ultrasound in Medicine and Biology},
	volume = {47},
	number = {12},
	pages = {3470 – 3479},
	doi = {10.1016/j.ultrasmedbio.2021.08.009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114993476&doi=10.1016%2fj.ultrasmedbio.2021.08.009&partnerID=40&md5=2c94f2e7fbd8b2d3fcf7457565b9cd7e},
	abstract = {The aims of this work were to create a robust automatic software tool for measurement of the levator hiatal area on transperineal ultrasound (TPUS) volumes and to measure the potential reduction in variability and time taken for analysis in a clinical setting. The proposed tool automatically detects the C-plane (i.e., the plane of minimal hiatal dimensions) from a 3-D TPUS volume and subsequently uses the extracted plane to automatically segment the levator hiatus, using a convolutional neural network. The automatic pipeline was tested using 73 representative TPUS volumes. Reference hiatal outlines were obtained manually by two experts and compared with the pipeline's automated outlines. The Hausdorff distance, area, a clinical quality score, C-plane angle and C-plane Euclidean distance were used to evaluate C-plane detection and quantify levator hiatus segmentation accuracy. A visual Turing test was created to compare the performance of the software with that of the expert, based on the visual assessment of C-plane and hiatal segmentation quality. The overall time taken to extract the hiatal area with both measurement methods (i.e., manual and automatic) was measured. Each metric was calculated both for computer–observer differences and for inter-and intra-observer differences. The automatic method gave results similar to those of the expert when determining the hiatal outline from a TPUS volume. Indeed, the hiatal area measured by the algorithm and by an expert were within the intra-observer variability. Similarly, the method identified the C-plane with an accuracy of 5.76 ± 5.06° and 6.46 ± 5.18 mm in comparison to the inter-observer variability of 9.39 ± 6.21° and 8.48 ± 6.62 mm. The visual Turing test suggested that the automatic method identified the C-plane position within the TPUS volume visually as well as the expert. The average time taken to identify the C-plane and segment the hiatal area manually was 2 min and 35 ± 17 s, compared with 35 ± 4 s for the automatic result. This study presents a method for automatically measuring the levator hiatal area using artificial intelligence-based methodologies whereby the C-plane within a TPUS volume is detected and subsequently traced for the levator hiatal outline. The proposed solution was determined to be accurate, relatively quick, robust and reliable and, importantly, to reduce time and expertise required for pelvic floor disorder assessment. © 2021 World Federation for Ultrasound in Medicine & Biology},
	author_keywords = {Automatic clinical workflow; Deep learning; Levator hiatus; Segmentation; Transperineal ultrasound; Ultrasound},
	keywords = {Artificial Intelligence; Humans; Imaging, Three-Dimensional; Pelvic Floor; Ultrasonography; Valsalva Maneuver; C (programming language); Convolutional neural networks; Deep learning; Geometry; Quality control; Software testing; Ultrasonic applications; Automatic clinical workflow; Automatic extraction; Automatic method; Clinical workflow; Deep learning; Levator hiatus; Segmentation; Transperineal ultrasound; Turing tests; Ultrasound volume; Article; artificial intelligence; automation; clinical evaluation; controlled study; convolutional neural network; female; human; image quality; image segmentation; mathematical analysis; measurement accuracy; practice guideline; quantitative analysis; software; three-dimensional imaging; transperineal pelvic ultrasound; Turing test; ultrasound; workflow; diagnostic imaging; echography; pelvis floor; three-dimensional imaging; Valsalva maneuver; Pipelines},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Green Open Access}
}

@ARTICLE{Bratić202293,
	author = {Bratić, Diana and Loknar, Nikolina Stanić and Ivančević, Tajana Koren},
	title = {Fuzzy Logic-Based Evaluation Model of Handwritten Font Sizes Readability on Extra Small Devices},
	year = {2022},
	journal = {International Journal of Interactive Mobile Technologies},
	volume = {16},
	number = {9},
	pages = {93 – 106},
	doi = {10.3991/ijim.v16i09.31135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129924115&doi=10.3991%2fijim.v16i09.31135&partnerID=40&md5=26b550307654e98fc9bcbe1ef18f009e},
	abstract = {Handwritten fonts are appealing to designers, but their application in web design can cause a readability problems. Different handwritten letter cuts in different font sizes are not equally readable on all screen types. The problem often occurs on extra small devices such as mobile phones. Therefore, it is necessary to choose the combination of the font type and appropriate font size that will ensure responsiveness and be readable on different devices, especially the small ones. For this purpose, a study of readability of monoline handwritten font in seven letter cuts (thin, ultra-light, light, regular, semi-bold, bold, and ultra-bold) in several font sizes was conducted. Variable font was used because it contains all style versions of one typeface family, as opposed to standard font families that use different files for each style version. Also, variable font is suitable for use on web because one file with all the necessary typeface styles is significantly smaller in size than classic families with multiple files which shortens the font loading time. Furthermore, model of readability evaluation using the fuzzy logic based postprocessing method for segmentation values related to evaluation criteria is proposed. Prototype of a variable handwritten fonts are tested in responsive web environment, using CSS technology. The results show that handwritten font size readability evaluation has measurable output because the score combine various numeral factors affecting the readability of particular font size in several letter cuts. Using of proposed model in short time can show readability level of some font type in some font size on a new responsive web suitable for different screen sizes, including extra small devices © 2022. International Journal of Interactive Mobile Technologies.All Rights Reserved.},
	author_keywords = {Artificial intelligence; Letter cut; Mobile phones; Responsive web; Variable font},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Paiva202195,
	author = {Paiva, Rui and Bedregal, Benjamín and Santiago, Regivan and Vieira, Thiago},
	title = {Residuated implications derived from quasi-overlap functions on lattices},
	year = {2021},
	journal = {International Journal of Approximate Reasoning},
	volume = {134},
	pages = {95 – 110},
	doi = {10.1016/j.ijar.2021.04.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105316591&doi=10.1016%2fj.ijar.2021.04.008&partnerID=40&md5=2e2c8b01af1dd7f2a88706b702d85314},
	abstract = {Recently, Paiva et al. generalized the notion of overlap functions in the context of lattices and introduced a weaker definition, called quasi-overlap, that originates from the removal of the continuity condition. In this paper, we introduce the concept of residuated implications related to quasi-overlap functions on lattices and prove some related properties. We also show that the class of quasi-overlap functions that fulfill the residuation principle is the same class of continuous functions according to a Scott topology on lattices. Scott continuity and the notion of densely ordered posets are used to generalize a classification theorem for residuated quasi-overlap functions on lattices. Conjugated quasi-overlaps are also considered. © 2021 Elsevier Inc.},
	author_keywords = {Lattices; Quasi-overlap functions; Residuated implications; Residuation principle; Scott topology},
	keywords = {Artificial intelligence; Continuity conditions; Continuous functions; Overlap functions; Residuated; Residuated implications; Residuation; Scott topology; Software engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Green Open Access}
}

@ARTICLE{Hijji20217152,
	author = {Hijji, Mohammad and Alam, Gulzar},
	title = {A Multivocal Literature Review on Growing Social Engineering Based Cyber-Attacks/Threats during the COVID-19 Pandemic: Challenges and Prospective Solutions},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {7152 – 7169},
	doi = {10.1109/ACCESS.2020.3048839},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099095693&doi=10.1109%2fACCESS.2020.3048839&partnerID=40&md5=75f139bdedd7bf1cd4532e5e1eb49877},
	abstract = {The novel coronavirus (COVID-19) pandemic has caused a considerable and long-lasting social and economic impact on the world. Along with other potential challenges across different domains, it has brought numerous cybersecurity challenges that must be tackled timely to protect victims and critical infrastructure. Social engineering-based cyber-attacks/threats are one of the major methods for creating turmoil, especially by targeting critical infrastructure, such as hospitals and healthcare services. Social engineering-based cyber-attacks are based on the use of psychological and systematic techniques to manipulate the target. The objective of this research study is to explore the state-of-the-art and state-of-the-practice social engineering-based techniques, attack methods, and platforms used for conducting such cybersecurity attacks and threats. We undertake a systematically directed Multivocal Literature Review (MLR) related to the recent upsurge in social engineering-based cyber-attacks/threats since the emergence of the COVID-19 pandemic. A total of 52 primary studies were selected from both formal and grey literature based on the established quality assessment criteria. As an outcome of this research study; we discovered that the major social engineering-based techniques used during the COVID-19 pandemic are phishing, scamming, spamming, smishing, and vishing, in combination with the most used socio-technical method: fake emails, websites, and mobile apps used as weapon platforms for conducting successful cyber-attacks. Three types of malicious software were frequently used for system and resource exploitation are; ransomware, trojans, and bots. We also emphasized the economic impact of cyber-attacks performed on different organizations and critical infrastructure in which hospitals and healthcare were on the top targeted infrastructures during the COVID-19 pandemic. Lastly, we identified the open challenges, general recommendations, and prospective solutions for future work from the researcher and practitioner communities by using the latest technology, such as artificial intelligence, blockchain, and big data analytics. © 2013 IEEE.},
	author_keywords = {COVID-19; cyber-attacks and threats; Multivocal literature review; prospective solutions; security and privacy; social engineering},
	keywords = {Advanced Analytics; Artificial intelligence; Critical infrastructures; Data Analytics; Economic and social effects; Health care; Hospitals; Infrastructure as a service (IaaS); Network security; Public works; Different domains; Healthcare services; Latest technology; Literature reviews; Quality assessment; Social and economic impacts; Social engineering; State of the practice; Malware},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 85; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ahakonye2021154892,
	author = {Ahakonye, Love Allen Chijioke and Nwakanma, Cosmas Ifeanyi and Lee, Jae-Min and Kim, Dong-Seong},
	title = {Efficient Classification of Enciphered SCADA Network Traffic in Smart Factory Using Decision Tree Algorithm},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {154892 – 154901},
	doi = {10.1109/ACCESS.2021.3127560},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119452276&doi=10.1109%2fACCESS.2021.3127560&partnerID=40&md5=c56705e0493d4ffdf34e0f1f9df69f32},
	abstract = {Vulnerability detection in Supervisory Control and Data Acquisition (SCADA) network of a Smart Factory (SF) is a high-priority research area in the cyber-security domain. Choosing an efficient Machine Learning (ML) algorithm for intrusion detection is a huge challenge. This study performed an investigative analysis into the classification ability of various ML models leveraging public cyber-security datasets to determine the best model. Based on the performance evaluation, all adaptions of Decision Tree (DT) and KNN in terms of accuracy, training time, MCE, and prediction speed are the most suitable ML for resolving security issues in the SCADA system. © 2013 IEEE.},
	author_keywords = {Algorithms; artificial intelligence; machine learning; SCADA systems},
	keywords = {Artificial intelligence; Classification (of information); Data mining; Decision trees; Intrusion detection; Learning algorithms; Learning systems; Network security; Software testing; Computational modelling; Cyber security; Data acquisition networks; Decision-tree algorithm; Machine-learning; Network traffic; Security; Smart manufacturing; Software algorithms; Supervisory control and data acquisition; SCADA systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 36; All Open Access, Gold Open Access}
}

@ARTICLE{He2023,
	author = {He, Shan},
	title = {A novel travel route planning method based on an ant colony optimization algorithm},
	year = {2023},
	journal = {Open Geosciences},
	volume = {15},
	number = {1},
	doi = {10.1515/geo-2022-0541},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175793758&doi=10.1515%2fgeo-2022-0541&partnerID=40&md5=156b9e6901268369019c1c8936f5ae9f},
	abstract = {As people's living standards improve, tourism has become an important way for people to spend their time on leisure and entertainment. The growing number of tourists in recent years has given rise to the creation of tourism-related ancillary services. Travelers need to choose a travel route that suits their needs and expectations and do it in a way that does not cause a waste of time, whether it is an emerging self-driving tour or a traditional tour group. Therefore, the optimization of tourist routes is of great significance to the majority of tourists. Given the planning requirements of tourist attractions in the post-epidemic era, an ant colony-based optimization algorithm is proposed to resolve the planning problem of optimal tourist routes. An optimized pheromone update strategy is also proposed based on the basic ant colony optimization algorithm. The optimized ant colony algorithm tries to balance two conflicting concepts, namely, flows into tourist attractions and the carrying capacity of destinations. To analyze the performance of the proposed optimization algorithm, the effects of different optimization algorithms on the route planning of tourist attractions were compared in the experiment, and the acceleration ratio of the optimized ant colony algorithm was tested using the graphics processing unit parallel computing program. The results show that the proposed algorithm provides certain advantages and has certain potential in parallel computing. To sum up, this study provides a better scientific basis for optimal tourist route planning and has a good reference value. © 2023 the author(s), published by De Gruyter.},
	author_keywords = {ant colony algorithm; parallel computing; pheromone; tourist route planning},
	keywords = {Ant colony optimization; Artificial intelligence; Computer graphics; Computer graphics equipment; Leisure; Program processors; Software testing; Ant colonies algorithm; Ant Colony Optimization algorithms; Optimization algorithms; Optimized ant colony algorithms; Parallel com- puting; Pheromone; Route planning; Tourist attractions; Tourist route planning; Travel route planning; algorithm; optimization; parallel computing; planning practice; tourist behavior; Graphics processing unit},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Gold Open Access}
}

@ARTICLE{Mori2022118236,
	author = {Mori, Ken T. and Liang, Xu and Elster, Lukas and Peters, Steven},
	title = {The Inadequacy of Discrete Scenarios in Assessing Deep Neural Networks},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {118236 – 118242},
	doi = {10.1109/ACCESS.2022.3220904},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141593838&doi=10.1109%2fACCESS.2022.3220904&partnerID=40&md5=c048510de2ed4356d76a83ffb78796d0},
	abstract = {Many recent approaches for automated driving (AD) functions currently include components relying on deep neural networks (DNNs). One approach in order to test AD functions is the scenario-based approach. This work formalizes and evaluates the parameter discretization process required in order to yield concrete scenarios for which an AD function can be tested. Using a common perception algorithm for camera images, a simulation case study is conducted for a simple static scenario containing one other vehicle. The results are analyzed with methods akin to those applied in the domain of computational fluid dynamics (CFD). The performance of the perception algorithm shows strong fluctuations even for small input changes and displays unpredictable outliers even at very small discretization steps. The convergence criteria as known from CFD fail, meaning that no parametrization is found which is sufficient for the validation of the perception component. Indeed, the results do not indicate consistent improvement with a finer discretization. These results agree well with theoretical attributes known for existing neural networks. However, the impact appears to be large even for the most basic scenario without malicious input. This indicates the necessity of directing more attention towards the parameter discretization process of the scenario-based testing approach to enable the safety argumentation of AD functions.  © 2013 IEEE.},
	author_keywords = {Artificial intelligence; autonomous vehicles; concrete scenarios; deep learning; error testing; intelligent vehicles; logical scenarios; machine learning; neural networks; software testing},
	keywords = {Computational fluid dynamics; Concretes; Forecasting; Intelligent vehicle highway systems; Interpolation; Job analysis; Safety testing; Vehicle safety; Autonomous Vehicles; Concrete scenario; Deep learning; Error testing; Logical scenario; Machine-learning; Neural-networks; Software testings; Task analysis; Deep neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}

@ARTICLE{Razdan202328343,
	author = {Razdan, Rahul and Akbas, Mustafa Ilhan and Sell, Raivo and Bellone, Mauro and Menase, Mahesh and Malayjerdi, Mohsen},
	title = {PolyVerif: An Open-Source Environment for Autonomous Vehicle Validation and Verification Research Acceleration},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {28343 – 28354},
	doi = {10.1109/ACCESS.2023.3258681},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151508125&doi=10.1109%2fACCESS.2023.3258681&partnerID=40&md5=ce1ecb5200dcdaaff822c0a31a5daca3},
	abstract = {Validation and Verification (V&V) of Artificial Intelligence (AI) based cyber physical systems such as Autonomous Vehicles (AVs) is currently a vexing and unsolved problem. AVs integrate subsystems in areas such as detection, sensor fusion, localization, perception, and path planning. Each of these subsystems contains significant AI content integrated with traditional hardware and software components. The complexity for validating even a subsystem is daunting and the task of validating the whole system is nearly impossible. Fundamental research in advancing the state-of-the-art for AV V&V is required. However, for V&V researchers, it is exceedingly difficult to make progress because of the massive infrastructure requirements to demonstrate the viability of any solution. This paper presents PolyVerif, the world's first open-source solution focused on V&V researchers with the objective of accelerating the state-of-the-art for AV V&V research. PolyVerif provides an AI design and verification framework consisting of a digital twin creation process, an open-source AV engine, access to several open-source physics based simulators, and open-source symbolic test generation engines. PolyVerif's objective is to arm V&V researchers with a framework which extends the state-of-the-art on any one of the many major axes of interest and use the remainder of the infrastructure to quickly demonstrate the viability of their solution. Given its open-source nature, researchers can also contribute their innovations to the project. Using this critical property of open-source environments, the innovation rate of the whole research community to solve these vexing issues can be greatly accelerated. Finally, the paper also presents results from several projects which have used PolyVerif.  © 2013 IEEE.},
	author_keywords = {artificial intelligence; Autonomous vehicles; modeling and simulation; validation and verification},
	keywords = {Embedded systems; Engines; Inference engines; Motion planning; Open systems; Software testing; Vehicles; Verification; Autonomous Vehicles; Hardware; Inference algorithm; Model and simulation; Open-source; Software; Software algorithms; State of the art; System analysis and design; Validation and verification; Open source software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access, Gold Open Access}
}

@ARTICLE{John-Mathews2022,
	author = {John-Mathews, Jean-Marie},
	title = {Some critical and ethical perspectives on the empirical turn of AI interpretability},
	year = {2022},
	journal = {Technological Forecasting and Social Change},
	volume = {174},
	doi = {10.1016/j.techfore.2021.121209},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115387930&doi=10.1016%2fj.techfore.2021.121209&partnerID=40&md5=7242a3895d51dc71c1561b66b2629f03},
	abstract = {We consider two fundamental and related issues currently facing the development of Artificial Intelligence (AI): the lack of ethics, and the interpretability of AI decisions. Can interpretable AI decisions help to address the issue of ethics in AI? Using a randomized study, we experimentally show that the empirical and liberal turn of the production of explanations tends to select AI explanations with a low denunciatory power. Under certain conditions, interpretability tools are therefore not means but, paradoxically, obstacles to the production of ethical AI since they can give the illusion of being sensitive to ethical incidents. We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education of the person for whom the explication is intended. AI ethics tools are therefore sometimes too flexible and self-regulation through the liberal production of explanations does not seem to be enough to resolve ethical issues. By following an STS pragmatist program, we highlight the role of non-human actors (such as computational paradigms, testing environments, etc.) in the formation of structural power relations, such as sexism. We then propose two scenarios for the future development of ethical AI: more external regulation, or more liberalization of AI explanations. These two opposite paths will play a major role in the future development of ethical AI. © 2021},
	author_keywords = {Artificial intelligence; Ethics; Experimentation; Interpretability; Self-regulation},
	keywords = {Artificial intelligence; Philosophical aspects; Software testing; Condition; Critical perspectives; Ethical issues; Ethical perspectives; Human actor; Intelligence decision; Interpretability; Or educations; Power; Self regulation; artificial intelligence; critical analysis; data interpretation; ethics; experimental study; gender discrimination; liberalization; power relations; regulatory approach; Deregulation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Martínez-Fernández2022,
	author = {Martínez-Fernández, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
	title = {Software Engineering for AI-Based Systems: A Survey},
	year = {2022},
	journal = {ACM Transactions on Software Engineering and Methodology},
	volume = {31},
	number = {2},
	doi = {10.1145/3487043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727000&doi=10.1145%2f3487043&partnerID=40&md5=fbf7ac61842908f81cd945ae6a1009e4},
	abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.  © 2022 Copyright held by the owner/author(s).},
	author_keywords = {AI-based systems; artificial intelligence; Software engineering; systematic mapping study},
	keywords = {Application programs; Artificial intelligence; Computer software selection and evaluation; Curricula; Software testing; Speech recognition; AI-based system; Autonomous driving; Classifieds; Learn+; Property; Research areas; Software-systems; State of the art; Synthesised; Systematic mapping studies; Mapping},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 137; All Open Access, Green Open Access}
}

@ARTICLE{Maksur2021753,
	author = {Maksur, Imam Al and Muhajir, Muhammad},
	title = {MyBotS Prototype on Social Media Discord with NLP},
	year = {2021},
	journal = {Baghdad Science Journal},
	volume = {18},
	number = {1},
	pages = {753 – 763},
	doi = {10.21123/BSJ.2021.18.1(SUPPL.).0753},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104499677&doi=10.21123%2fBSJ.2021.18.1%28SUPPL.%29.0753&partnerID=40&md5=6ad11101b340b04851c7bf7b046c330d},
	abstract = {The continuous growth in technology and technological devices has led to the development of machines to help ease various human-related activities. For instance, irrespective of the importance of information on the Steam platform, buyers or players still get little information related to the application. This is not encouraging despite the importance of information in this current globalization era. Therefore, it is necessary to develop an attractive and interactive application that allows users to ask questions and get answers, such as a chatbot, which can be implemented on Discord social media. Artificial Intelligence is a technique that allows machines to think and be able to make their own decisions. This research showed that the discord chatbot prototype provides various services based on the results of classification testing using the SVM method with three kernels, namely Linear, Polynomial, and RBF. The test data and accuracy values prediction are the largest Liniear Kernel SVM with accuracy and error prediction values of 94% and 6%. © 2021 University of Baghdad. All rights reserved.},
	author_keywords = {Artificial Intelligence; Chatbot; Discord; NLP; SVM},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Sobczak202224985,
	author = {Sobczak, Szymon and Kapela, Rafal},
	title = {Hybrid Restricted Boltzmann Machine- Convolutional Neural Network Model for Image Recognition},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {24985 – 24994},
	doi = {10.1109/ACCESS.2022.3155873},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125751806&doi=10.1109%2fACCESS.2022.3155873&partnerID=40&md5=449fca49c57f80093fc07e1a1d36c7e7},
	abstract = {Convolutional Neural Networks (CNNs) have become a standard approach to many image processing dilemmas. Consequently, most of the proposed CNN architectures tend to increase the model deepness or layer complexity. Thus, they are composed of many parameters and need considerable computing resources and training examples. However, some recent works show that either shallow neural networks or architectures without convolutions can achieve similar results with these models often being used in systems with limited resources. Consideration of these aspects led us to a relatively simple preprocessing layer that increases the accuracy of CNN or may reduce its complexity. The layer is composed of two parts: the first is used to transform RGB data to binary representation, the second is a neural network that transforms the binary data into a multi-channel, real-value matrix and is trained in a fully unsupervised manner. Our proposal also includes a metric that may be used for measuring the similarity of training data, with the latter proving useful when performing transfer learning. Our experiments show that the resulting architecture not only helps to improve accuracy but is also more robust to image noise, including adversarial attacks, when compared to state-of-the-art models. © 2013 IEEE.},
	author_keywords = {Artificial intelligence; binary patterns; deep learning; image recognition; local binary pattern; restricted Boltzmann machine},
	keywords = {Complex networks; Convolution; Deep learning; Image enhancement; Image recognition; Network architecture; Neural networks; Binary patterns; Complexity theory; Computational modelling; Convolutional neural network; Deep learning; Image color analysis; Local binary patterns; Neural-networks; Restricted boltzmann machine; Task analysis; Computer architecture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Holtzen2020,
	author = {Holtzen, Steven and Van Den Broeck, Guy and Millstein, Todd},
	title = {Scaling exact inference for discrete probabilistic programs},
	year = {2020},
	journal = {Proceedings of the ACM on Programming Languages},
	volume = {4},
	number = {OOPSLA},
	doi = {10.1145/3428208},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097578044&doi=10.1145%2f3428208&partnerID=40&md5=9f618a586bb693357b11c431b60c4a45},
	abstract = {Probabilistic programming languages (PPLs) are an expressive means of representing and reasoning about probabilistic models. The computational challenge of probabilistic inference remains the primary roadblock for applying PPLs in practice. Inference is fundamentally hard, so there is no one-size-fits all solution. In this work, we target scalable inference for an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis, but they prove to be challenging for existing PPLs. We develop a domain-specific probabilistic programming language called Dice that features a new approach to exact discrete probabilistic program inference. Dice exploits program structure in order to factorize inference, enabling us to perform exact inference on probabilistic programs with hundreds of thousands of random variables. Our key technical contribution is a new reduction from discrete probabilistic programs to weighted model counting (WMC). This reduction separates the structure of the distribution from its parameters, enabling logical reasoning tools to exploit that structure for probabilistic inference. We (1) show how to compositionally reduce Dice inference to WMC, (2) prove this compilation correct with respect to a denotational semantics, (3) empirically demonstrate the performance benefits over prior approaches, and (4) analyze the types of structure that allow Dice to scale to large probabilistic programs. © 2020 Owner/Author.},
	author_keywords = {Probabilistic programming},
	keywords = {Artificial intelligence; Computer programming languages; Semantics; Computational challenges; Denotational semantics; Discrete distribution; Probabilistic inference; Probabilistic models; Probabilistic programming language; Probabilistic programs; Technical contribution; Probability distributions},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 57; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Abdel Hameed2022,
	author = {Abdel Hameed, Mohamed and Hassaballah, M. and Hosney, Mosa E. and Alqahtani, Abdullah},
	title = {An AI-Enabled Internet of Things Based Autism Care System for Improving Cognitive Ability of Children with Autism Spectrum Disorders},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/2247675},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131337474&doi=10.1155%2f2022%2f2247675&partnerID=40&md5=10c46f5f97af1bc7dfd36d2ea16e0dab},
	abstract = {Smart monitoring and assisted living systems for cognitive health assessment play a central role in assessment of individuals' health conditions. Autistic children suffer from some difficulties including social skills, repetitive behaviors, speech and nonverbal communication, and accommodating to the environment around them. Thus, dealing with autistic children is a serious public health problem as it is hard to determine what they feel with a lack of emotional cognitive ability. Currently, no medical treatments have been shown to cure autistic children, with most of the social assistive research to date focusing on Autism Spectrum Disorder (ASD) without suggesting a real treatment. In this paper, we focus on improving cognitive ability and daily living skills and maximizing the ability of the autistic child to function and participate positively in the community. Through utilizing intelligent systems based Artificial Intelligence (AI) and Internet of Things (IoT) technologies, we facilitate the process of adaptation to the world around the autistic children. To this end, we propose an AI-enabled IoT system embodied in a sensor for measuring the heart rate to predict the state of the child and then sending the state to the guardian with feeling and expected behavior of the child via a mobile application. Further, the system can provide a new virtual environment to help the child to be capable of improving eye contact with other people. This way is represented in pictures of these persons in 3D models that break this child's fear barrier. The system follows strategies that have focused on social communication skill development particularly at young ages to be more interactive with others.  © 2022 Mohamed Abdel Hameed et al.},
	keywords = {Artificial Intelligence; Autism Spectrum Disorder; Autistic Disorder; Child; Cognition; Humans; Internet of Things; Cognitive systems; Diseases; Intelligent systems; Public health; Speech communication; Virtual reality; Autism spectrum disorders; Autistic children; Children with autisms; Cognitive ability; Health assessments; Health condition; Living systems; Non-verbal communications; Smart monitoring; Social skills; artificial intelligence; autism; child; cognition; human; psychology; Internet of things},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Jana2020322,
	author = {Jana, Rajib Lochan and Dey, Soumyajit and Mondal, Arijit and Dasgupta, Pallab},
	title = {Automated planning for finding alternative bug traces},
	year = {2020},
	journal = {IET Computers and Digital Techniques},
	volume = {14},
	number = {6},
	pages = {322 – 335},
	doi = {10.1049/iet-cdt.2019.0283},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093692273&doi=10.1049%2fiet-cdt.2019.0283&partnerID=40&md5=8e745f43da6e41edb4fb217cbd647790},
	abstract = {Bug traces serve as references for patching a microprocessor design after a bug has been found. Unless the root cause of a bug has been detected and patched, variants of the bug may return through alternative bug traces, following a different sequence of micro-architectural events. To avoid such a situation, the verification engineer must think of every possible way in which the bug may return, which is a complex problem for a modern microprocessor. This study proposes a methodology which gleans high-level descriptions of the micro-architectural steps and uses them in an artificial Intelligence planning framework to find alternative pathways through which a bug may return. The plans are then translated to simulation test cases which explore these potential bug scenarios. The planning tool essentially automates the task of the verification engineer towards exploring possible alternative sequences of micro-architectural steps that may allow a bug to return. The proposed methodology is demonstrated in three case studies. © The Institution of Engineering and Technology 2020},
	keywords = {Artificial intelligence; Artificial intelligence planning; Automated planning; Complex problems; High level description; Microprocessor designs; Modern microprocessor; Simulation tests; Verification engineers; Architecture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Bronze Open Access}
}

@ARTICLE{Galli2021,
	author = {Galli, Tamas and Chiclana, Francisco and Siewe, Francois},
	title = {Genetic algorithm-based fuzzy inference system for describing execution tracing quality},
	year = {2021},
	journal = {Mathematics},
	volume = {9},
	number = {21},
	doi = {10.3390/math9212822},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119142384&doi=10.3390%2fmath9212822&partnerID=40&md5=76e2181e5894eda78f3d58276e25a967},
	abstract = {Execution tracing is a tool used in the course of software development and software maintenance to identify the internal routes of execution and state changes while the software operates. Its quality has a high influence on the duration of the analysis required to locate software faults. Nevertheless, execution tracing quality has not been described by a quality model, which is an impediment while measuring software product quality. In addition, such a model needs to consider uncertainty, as the underlying factors involve human analysis and assessment. The goal of this study is to address both issues and to fill the gap by defining a quality model for execution tracing. The data collection was conducted on a defined study population with the inclusion of software professionals to consider their accumulated experiences; moreover, the data were processed by genetic algorithms to identify the linguistic rules of a fuzzy inference system. The linguistic rules constitute a human-interpretable rule set that offers further insights into the problem domain. The study found that the quality properties accuracy, design and implementation have the strongest impact on the quality of execution tracing, while the property legibility is necessary but not completely inevitable. Furthermore, the quality property security shows adverse effects on the quality of execution tracing, but its presence is required to some extent to avoid leaking information and to satisfy legal expectations. The created model is able to describe execution tracing quality appropriately. In future work, the researchers plan to link the constructed quality model to overall software product quality frameworks to consider execution tracing quality with regard to software product quality as a whole. In addition, the simplification of the mathematically complex model is also planned to ensure an easy-to-tailor approach to specific application domains. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Artificial intelligence; Execution tracing; Execution tracing quality; Fuzzy logic; Logging; Logging quality; Quality assessment; Software product quality model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Rajabli20214797,
	author = {Rajabli, Nijat and Flammini, Francesco and Nardone, Roberto and Vittorini, Valeria},
	title = {Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {4797 – 4819},
	doi = {10.1109/ACCESS.2020.3048047},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117089711&doi=10.1109%2fACCESS.2020.3048047&partnerID=40&md5=5a3ea215bae4dc35b5e38e398d889b3a},
	abstract = {Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and traffic congestion. However, those benefits come with great challenges in the verification and validation (V&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artificial Intelligence (AI), its use in autonomous cars creates concerns that need to be addressed using appropriate V&V processes that can address trustworthy AI and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classified in order to investigate the state-of-the-art in the software V&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The first part of the review addresses certification issues against reference standards, challenges in assessing machine learning, as well as general V&V methodologies. The second part investigates more specific approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author_keywords = {Advanced driver assistance systems; automotive engineering; autonomous vehicles; cyber-physical systems; formal verification; intelligent vehicles; machine learning; system testing; system validation; vehicle safety},
	keywords = {Accidents; Advanced driver assistance systems; Artificial intelligence; Automobile drivers; Cyber Physical System; Formal verification; Learning systems; Safety testing; Software testing; Vehicle safety; Autonomous automobile; Autonomous Vehicles; Cybe-physical systems; Cyber-physical systems; Machine-learning; Road; Software; System testing; System validation; Systematic; Vehicle safety; Embedded systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 58; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Zhou20211,
	author = {Zhou, Huisi and Ouyang, Dantong and Zhang, Liming},
	title = {Efficient static compaction of test patterns using partial maximum satisfiability},
	year = {2021},
	journal = {Tsinghua Science and Technology},
	volume = {26},
	number = {1},
	pages = {1 – 8},
	doi = {10.26599/TST.2019.9010046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092026115&doi=10.26599%2fTST.2019.9010046&partnerID=40&md5=66caf7e37e662d9b58e2b46d3e102622},
	abstract = {Static compaction methods aim at finding unnecessary test patterns to reduce the size of the test set as a post-process of test generation. Techniques based on partial maximum satisfiability are often used to track many hard problems in various domains, including artificial intelligence, computational biology, data mining, and machine learning. We observe that part of the test patterns generated by the commercial Automatic Test Pattern Generation (ATPG) tool is redundant, and the relationship between test patterns and faults, as a significant information, can effectively induce the test patterns reduction process. Considering a test pattern can detect one or more faults, we map the problem of static test compaction to a partial maximum satisfiability problem. Experiments on ISCAS89, ISCAS85, and ITC99 benchmarks show that this approach can reduce the initial test set size generated by TetraMAX18 while maintaining fault coverage. © 2021 Tsinghua University Press. All rights reserved.},
	author_keywords = {Automatic Test Pattern Generation (ATPG); partial maximum satisfiability; test compaction},
	keywords = {Artificial intelligence; Compaction; Data mining; Formal logic; Automatic test pattern generation tools; Computational biology; Fault coverages; Partial maximum; Reduction process; Static compaction; Static test compaction; Test generations; Automatic test pattern generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Bronze Open Access}
}

@ARTICLE{Casal202223,
	author = {Casal, Filipe and Mordido, Andreia and Vasconcelos, Vasco T.},
	title = {Mixed sessions},
	year = {2022},
	journal = {Theoretical Computer Science},
	volume = {897},
	pages = {23 – 48},
	doi = {10.1016/j.tcs.2021.08.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114304411&doi=10.1016%2fj.tcs.2021.08.005&partnerID=40&md5=dd36339b74d00a225e74ec092c2d9df4},
	abstract = {Session types describe patterns of interaction on communicating channels. Traditional session types include a form of choice whereby servers offer a collection of options, of which each client selects exactly one. Mixed choices blur the distinction between servers and clients (that is, external and internal choice) by allowing options to be both offered and selected in the same choice. We introduce mixed choices in the context of session types and argue that they increase the flexibility of program development at the same time that they reduce the number of synchronisation primitives down to exactly one. We present a type system incorporating subtyping and prove preservation and absence of runtime errors for well-typed processes. We further show that classical (conventional) sessions can be faithfully and tightly embedded in mixed choices, and conversely that there is a minimal encoding from mixed choices to classical sessions. Finally, we discuss algorithmic type checking and a runtime system built on top of a conventional (choice-less) message-passing architecture. © 2021 The Authors},
	author_keywords = {Mixed choice; Session types; Type systems},
	keywords = {Artificial intelligence; Algorithmics; Encodings; Mixed choice; Program development; Run-time errors; Session types; Subtypings; Synchronization primitive; Typechecking; Typesystems; Message passing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Baharuddin2023174,
	author = {Baharuddin and Mendoza, Muhammad Dominique and Hutajulu, Olnes Yosefa and Fibriasari, Hesti},
	title = {The Utilization of Artificial Intelligence Based Chatbot in Interactive Learning Media},
	year = {2023},
	journal = {Journal of Engineering Education Transformations},
	volume = {37},
	number = {2},
	pages = {174 – 188},
	doi = {10.16920/jeet/2023/v37i2/23159},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180524522&doi=10.16920%2fjeet%2f2023%2fv37i2%2f23159&partnerID=40&md5=d0f7f40dcdc66877c5d3c0244ea10cde},
	abstract = {After the spread of the COVID-19 virus, the world of learning is no longer the same as it was before the pandemic, especially at the university level, which requires students to be more active in finding lots of learning references via the internet. Active learning has a close relationship with the use of artificial intelligence technology, in this study researchers designed an interactive learning media that applied Artificial Intelligence (AI) Assisted Chatbot. The multimedia development method used in this research is the Multimedia Development Lifecycle (MDLC) method. Interactive learning media is built using Android Studio, and the retrieval-based chatbot is built with Python, Tensorflow, Hard (Extension for Tensorflow), NumPy, and Matplotlib. The researcher applies the White box and Black box testing methods to test whether the learning media that have been made have worked according to the user's needs. The chatbot model that has been applied to this learning media when tested with the BLEU metric gets a result of 0.1117, this shows the chatbot produces good answers and provi des credible learni ng references. There is a difference in student learning outcomes after using chatbot-based learning media in basic electronics subjects, the utilization of chatbot-based learning media is able to improve student learning outcomes, especiall y i n the st udent weaknesses section, the chatbot can provide detailed explanations and guide students in solving linear equation problems. © 2023, Rajarambapu Institute Of Technology. All rights reserved.},
	author_keywords = {Artificial Intelligence; Chat bot; Interactive Learning Media; Retreival-based Chatbot},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Danciu2022,
	author = {Danciu, Gabriel Mihail and Dinu, Alexandru},
	title = {Coverage Fulfillment Automation in Hardware Functional Verification Using Genetic Algorithms},
	year = {2022},
	journal = {Applied Sciences (Switzerland)},
	volume = {12},
	number = {3},
	doi = {10.3390/app12031559},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123786322&doi=10.3390%2fapp12031559&partnerID=40&md5=5c9820cf7b2d3d9687116bcf53bb1d42},
	abstract = {The functional verification process is one of the most expensive steps in integrated circuit manufacturing. Functional coverage is the most important metric in the entire verification process. By running multiple simulations, different situations of DUT functionality can be encountered, and in this way, functional coverage fulfillment can be improved. However, in many cases it is difficult to reach specific functional situations because it is not easy to correlate the required input stimuli with the expected behavior of the digital design. Therefore, both industry and academia seek solutions to automate the generation of stimuli to reach all the functionalities of interest with less human effort and in less time. In this paper, several approaches inspired by genetic algorithms were developed and tested using three different designs. In all situations, the percentage of stimulus sets generated using well‐performing genetic algorithms approaches was higher than the values that resulted when random simulations were employed. In addition, in most cases the genetic algorithm approach reached a higher coverage value per test compared to the random simulation outcome. The results confirmed that in many cases genetic algorithms can outperform constrained random generation of stimuli, that is employed in the classical way of doing verification, considering coverage fulfillment level per verification test. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Artificial intelligence; Automation; Functional coverage; Genetic algorithms; Hardware verification},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Shafeek20215771,
	author = {Shafeek, H. and Soltan, H.A. and Abdel-Aziz, M.H.},
	title = {Corrosion monitoring in pipelines with a computerized system},
	year = {2021},
	journal = {Alexandria Engineering Journal},
	volume = {60},
	number = {6},
	pages = {5771 – 5778},
	doi = {10.1016/j.aej.2021.04.006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107615287&doi=10.1016%2fj.aej.2021.04.006&partnerID=40&md5=e4043beb5cb1b7ea4784057dbaaf37f7},
	abstract = {This study aims to combine the smart pigs as a non-destructive test (NDT) inspection technique with software developed for the assessment of pipeline corrosion defects to ensure fitness for the surface. The software uses decision support systems, connected through the correlated linkage technique, which is coded using Microsoft Access and Visual C#. This software measures general internal pipeline corrosion forms to identify locations with potential corrosion features and predict corrosion conditions in the future. Computer-aided corrosion management program (CACM) examined maximum corroded depth of internal corrosion, maximum allowable axial corrosion defect length, failure pressure, the corrosion rate, and the remaining pipeline life. This work introduces a wide-ranging review of computer-aided corrosion management programs. The proposed method of assisting and detecting corrosion internal defects and defects data should be available. This software is easy to use without complicated analysis. It helps to reduce unplanned shutdowns in the oil and gas production industry. © 2021 THE AUTHORS},
	author_keywords = {Corroded pipelines; Corrosion rate; Defect level; Failure pressure; Remaining life; Smart pig},
	keywords = {Artificial intelligence; Decision support systems; Internal corrosion; Mammals; Nondestructive examination; Pipelines; Software testing; Computer-aided; Corroded pipelines; Corrosion defect; Corrosion management; Defect levels; Failure pressure; Management programs; Pipeline corrosion; Remaining life; Smart pigs; Corrosion rate},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access}
}

@ARTICLE{Batarseh2021,
	author = {Batarseh, Feras A. and Freeman, Laura and Huang, Chih-Hao},
	title = {A survey on artificial intelligence assurance},
	year = {2021},
	journal = {Journal of Big Data},
	volume = {8},
	number = {1},
	doi = {10.1186/s40537-021-00445-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104988842&doi=10.1186%2fs40537-021-00445-7&partnerID=40&md5=133ef6a606fa4249eda696ab1158f466},
	abstract = {Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing) library of algorithms that could be applied for different problems. One important notion for the adoption of AI algorithms into operational decision processes is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 and 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented, and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance. © 2021, The Author(s).},
	author_keywords = {AI assurance; Data Engineering; Explainable AI (XAI); Validation and verification},
	keywords = {Decision making; Artificial intelligence algorithms; Artificial intelligence assurance; Data engineering; Decision process; Decisions makings; Explainable artificial intelligence (XAI); Multiple domains; Operational decisions; Systematic Review; Validation and verification; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 80; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Al-Johany202360668,
	author = {Al-Johany, Norah Abdullah and Eassa, Fathy Elbouraey and Sharaf, Sanaa Abdullah and Noaman, Amin Y. and Ahmed, Asaad},
	title = {Prediction and Correction of Software Defects in Message-Passing Interfaces Using a Static Analysis Tool and Machine Learning},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {60668 – 60680},
	doi = {10.1109/ACCESS.2023.3285598},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162715212&doi=10.1109%2fACCESS.2023.3285598&partnerID=40&md5=7815e345aa181536afea1d9686aedefc},
	abstract = {The Software Defect Prediction (SDP) method forecasts the occurrence of defects at the beginning of the software development process. Early fault detection will decrease the overall cost of software and improve its dependability. However, no effort has been made in high-performance software to address it. The contribution of this paper is predicting and correcting software defects in the Message Passing Interface (MPI) based on machine learning (ML). This system predicts defects including deadlock, race conditions, and mismatch, by dividing the model into three stages: training, testing, and prediction. The training phase extracts and combines the features as well as the label and then trains on classification. During the testing phase, these features are extracted and classified. The prediction phase inputs the MPI code and determines whether it includes defects. If it discovers a defect, the correction subsystem corrects it. We collected 40 MPI codes in C++, including all MPI communication. Results show the NB classifiers have high accuracy, precision, and recall, which are about 1.  © 2013 IEEE.},
	author_keywords = {High-performance computing; message passing interface; parallel programming; semantic features; software defect prediction},
	keywords = {Artificial intelligence; C++ (programming language); Classification (of information); Defects; Fault detection; Forecasting; Learning systems; Object oriented programming; Parallel programming; Program debugging; Semantics; Software design; Software testing; Static analysis; Computer bugs; Features extraction; High-performance computing; Message passing interface; Message systems; Message-passing; Performance computing; Predictive models; Semantic features; Software; Software defect prediction; System recovery; Message passing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access}
}

@ARTICLE{Zhu2022,
	author = {Zhu, Ye and Wang, Chong},
	title = {Study on Virtual Experience Marketing Model Based on Augmented Reality: Museum Marketing (Example)},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/2485460},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131223989&doi=10.1155%2f2022%2f2485460&partnerID=40&md5=638f155ea2e97e54ae0504d439d605c7},
	abstract = {With the development of emerging digital technologies such as Augmented Reality and Artificial Intelligence, Augmented Reality (AR) technology-enabled experience marketing model can bring brand new virtual experience to the users, improve the brand attitudes of users, and increase the use and purchase intention of users. Based on the theoretical basis of experience marketing and AR, the AR application of Guilin Museum was designed and developed by using Unity as the software development tool and using AR Foundation as the AR development framework. The implementation of this application was mainly based on face detection and tracking, image detection, and tracking in the underlying API of AR Foundation. Subsequently, an AR virtual experience marketing model was constructed based on the Schmitt strategic experience module, and the usage data of AR applications were collected. Furthermore, the collected data were analyzed and evaluated using SPSS and AMOS software, and the relationships and influences of sensory experience, emotional experience, thinking experience, action experience, and association experience on the brand attitudes of users and use intention and purchase intention in AR application were tested.  © 2022 Ye Zhu and Chong Wang.},
	keywords = {Artificial Intelligence; Augmented Reality; Marketing; Museums; User-Computer Interface; Application programming interfaces (API); Application programs; Augmented reality; Commerce; Face recognition; Marketing; Purchasing; Software design; Augmented reality applications; Augmented reality technology; Development frameworks; Digital technologies; Face detection and tracking; Marketing models; Model-based OPC; Purchase intention; Software development tools; Use intentions; artificial intelligence; computer interface; information center; marketing; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Asyrofi20225087,
	author = {Asyrofi, Muhammad Hilmi and Yang, Zhou and Yusuf, Imam Nur Bani and Kang, Hong Jin and Thung, Ferdian and Lo, David},
	title = {BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems},
	year = {2022},
	journal = {IEEE Transactions on Software Engineering},
	volume = {48},
	number = {12},
	pages = {5087 – 5101},
	doi = {10.1109/TSE.2021.3136169},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121800826&doi=10.1109%2fTSE.2021.3136169&partnerID=40&md5=88013af518d8f30d176d0ebc4b718cfd},
	abstract = {Artificial intelligence systems, such as Sentiment Analysis (SA) systems, typically learn from large amounts of data that may reflect human bias. Consequently, such systems may exhibit unintended demographic bias against specific characteristics (e.g., gender, occupation, country-of-origin, etc.). Such bias manifests in an SA system when it predicts different sentiments for similar texts that differ only in the characteristic of individuals described. To automatically uncover bias in SA systems, this paper presents BiasFinder, an approach that can discover biased predictions in SA systems via metamorphic testing. A key feature of BiasFinder is the automatic curation of suitable templates from any given text inputs, using various Natural Language Processing (NLP) techniques to identify words that describe demographic characteristics. Next, BiasFinder generates new texts from these templates by mutating words associated with a class of a characteristic (e.g., gender-specific words such as female names, 'she', 'her'). These texts are then used to tease out bias in an SA system. BiasFinder identifies a bias-uncovering test case (BTC) when an SA system predicts different sentiments for texts that differ only in words associated with a different class (e.g., male vs. female) of a target characteristic (e.g., gender). We evaluate BiasFinder on 10 SA systems and 2 large scale datasets, and the results show that BiasFinder can create more BTCs than two popular baselines. We also conduct an annotation study and find that human annotators consistently think that test cases generated by BiasFinder are more fluent than the two baselines.  © 1976-2012 IEEE.},
	author_keywords = {bias; fairness bug; metamorphic testing; Sentiment analysis; test case generation},
	keywords = {Artificial intelligence; Large dataset; Online systems; Population statistics; Sentiment analysis; Social networking (online); Annotation; Bias; Fairness bug; Metamorphic testing; Sentiment analysis; Shaft; Social networking (online); Tagging; Test case generation; Blogs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Tapado2023193,
	author = {Tapado, Belen M. and Bola, John Gregory M. and Salazar, Erickson T. and Tablizo, Zcel T.},
	title = {University’s Service Delivery Improvement Through a DSS-enabled Client Feedback System},
	year = {2023},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {14},
	number = {7},
	pages = {193 – 199},
	doi = {10.14569/IJACSA.2023.0140721},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168794088&doi=10.14569%2fIJACSA.2023.0140721&partnerID=40&md5=13ac3455645ab7d3c20bb68ea9fad9af},
	abstract = {The expansion of products and services on a global scale demands the improvement of an organization’s performance. In addition to addressing the challenges of improving product and service delivery, companies must focus not only on meeting customer expectations but also on surpassing them. Consequently, valuing the opinions of clients, giving the best client experience, and measuring client satisfaction are deemed vital not only for the company’s survival but also for gaining a competitive edge for the organizations in the wired communities. It is because of these premises that the Client Feedback System was developed in this study for the university’s service delivery improvement. This system captured the results of the Client Satisfaction Survey for School Year 2015-2016 to School Year 2020-2021. Interpretation of these captured data were made and action for the improvement of service delivery for each department in this university was recommended using the Decision Support System (DSS) technique. The system was created using the Rapid Application Development (RAD) method and utilized various software and technologies such as HTML, CSS, and JavaScript for the front-end development, MySQL and PHP for the back-end, and Apache as the local server of the system during its development and pilot testing. © 2023, Science and Information Organization. All Rights Reserved.},
	author_keywords = {client feedback system; client satisfaction; Decision support system; rapid application development; service delivery improvement},
	keywords = {Application programs; Artificial intelligence; Software testing; Client feedback system; Client satisfaction; Feedback systems; Global scale; Performance; Product and services; Product delivery; Rapid application development; Service delivery; Service delivery improvement; Decision support systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access, Gold Open Access}
}

@ARTICLE{Alkahtani2022,
	author = {Alkahtani, Hasan and Aldhyani, Theyazn H. H.},
	title = {Artificial Intelligence Algorithms for Malware Detection in Android‐Operated Mobile Devices},
	year = {2022},
	journal = {Sensors},
	volume = {22},
	number = {6},
	doi = {10.3390/s22062268},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126304459&doi=10.3390%2fs22062268&partnerID=40&md5=c9014891ae0cc9ea0c6a7fe77c6d24cb},
	abstract = {With the rapid expansion of the use of smartphone devices, malicious attacks against Android mobile devices have increased. The Android system adopted a wide range of sensitive applications such as banking applications; therefore, it is becoming the target of malware that exploits the vulnerabilities of the security system. A few studies proposed models for the detection of mobile malware. Nevertheless, improvements are required to achieve maximum efficiency and performance. Hence, we implemented machine learning and deep learning approaches to detect Android‐directed malicious attacks. The support vector machine (SVM), k‐nearest neighbors (KNN), linear discriminant analysis (LDA), long short‐term memory (LSTM), convolution neural network‐long short‐term memory (CNN‐LSTM), and autoencoder algorithms were applied to identify malware in mobile environments. The cybersecurity system was tested with two Android mobile benchmark datasets. The correlation was calculated to find the high‐percentage significant features of these systems in the protection against attacks. The machine learning and deep learning algorithms successfully detected the malware on Android applications. The SVM algorithm achieved the highest accuracy (100%) using the CICAndMal2017 dataset. The LSTM model also achieved a high percentage accuracy (99.40%) using the Drebin dataset. Additionally, by calculating the mean error, mean square error, root mean square error, and Pearson correlation, we found a strong relationship between the predicted values and the target values in the validation phase. The correlation coefficient for the SVM method was R2 = 100% using the CICAndMal2017 dataset, and LSTM achieved R2 = 97.39% in the Drebin dataset. Our results were compared with existing security systems, showing that the SVM, LSTM, and CNN‐LSTM algorithms are of high efficiency in the detection of malware in the Android environment. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Android applications; Cybersecurity; Deep learning; Machine learning; Malware},
	keywords = {Algorithms; Artificial Intelligence; Machine Learning; Neural Networks, Computer; Smartphone; Android (operating system); Correlation methods; Cybersecurity; Errors; Learning algorithms; Long short-term memory; Mean square error; Mobile security; Network security; Security systems; Signal encoding; Support vector machines; Android applications; Artificial intelligence algorithms; Convolution neural network; Cyber security; Deep learning; Malicious attack; Malware detection; Rapid expansion; Smart phones; Support vectors machine; algorithm; artificial intelligence; machine learning; smartphone; Malware},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 57; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Yu2022,
	author = {Yu, Zhen},
	title = {Study on Ergonomic Design of Artificial Intelligence Lower Limb Assist Brace for the Elderly},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/3304513},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136040900&doi=10.1155%2f2022%2f3304513&partnerID=40&md5=3794e8e1ea78d394f99d0438b3630e0d},
	abstract = {The ergonomic design study of artificial intelligence lower limb-assisted brace for the elderly is a new design standard of lower limb-assisted brace for the elderly with mobility problems. Based on human factors engineering, this study tested and analyzed the advantages and disadvantages of human lower limb motion mechanics, human gait motion law, and existing lower limb assisted brace design cases at home and abroad and concluded that the common external assisted method is less man-machine efficient than the internal assisted method. Therefore, a new brace joint rotation curvature, component parameters, and other key information were designed based on the structure of the medial assistance method. With the help of the engineering and scientific analysis methods in human factors engineering, the designed machines and systems are made more adaptable to the physiological and psychological characteristics of human beings. This study explores the interaction between humans and machines and the rationality of their mutual integration, which can effectively avoid repetitive strain injuries and other muscle diseases over time for users in the process of assistance and achieve efficiency, health, and safety. Subsequently, Rhino software was used for digital modeling, physical prototyping, experimental testing, and analysis of the design solution and continuous optimization of the design. At the same time, the perceptual engineering design method was utilized to meet the humanized aesthetic design requirements. The prototype of the design study was finally completed, which is more in line with the evaluation criteria of "human-machine-environment system"than the existing market design in terms of functional rationality, human-machine performance, and human experience. This demonstrates the validity of the design method and is an important reference for the design standard of the lower limb support for the elderly.  © 2022 Zhen Yu.},
	keywords = {Aged; Artificial Intelligence; Biomechanical Phenomena; Braces; Ergonomics; Humans; Lower Extremity; Design; Ergonomics; Optimization; Software prototyping; Software testing; Design standard; Design studies; Ergonomic design; Gait motions; Human factor engineerings; Human gait; Human lower limbs; Human-machine; Lower limb; Lower-limb motion; aged; artificial intelligence; biomechanics; brace; ergonomics; human; lower limb; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Eldaya2022770,
	author = {Eldaya, Rami W. and Kansagra, Akash P. and Zei, Markus and Mason, Evan and Holder, Derek and Heitsch, Laura and Vo, Katie D. and Goyal, Manu S.},
	title = {Performance of Automated RAPID Intracranial Hemorrhage Detection in Real-World Practice: A Single-Institution Experience},
	year = {2022},
	journal = {Journal of Computer Assisted Tomography},
	volume = {46},
	number = {5},
	pages = {770 – 774},
	doi = {10.1097/RCT.0000000000001335},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138445858&doi=10.1097%2fRCT.0000000000001335&partnerID=40&md5=ad08611c9be485ac6549661b8854ed93},
	abstract = {Background and Purpose Intracranial hemorrhage (ICH) is a common finding in patients presenting to the emergency department with acute neurological symptoms. Noncontrast head computed tomography (NCCT) is the primary modality for assessment and detection of ICH in the acute setting. RAPID ICH software aims to automatically detect ICH on NCCT and was previously shown to have high accuracy when applied to a curated test data set. Here, we measured the test performance characteristics of RAPID ICH software in detecting ICH on NCCT performed in patients undergoing emergency stroke evaluation at a tertiary academic comprehensive stroke center. Materials and Methods This retrospective study assessed consecutive patients over a 6-month period who presented with acute neurological symptoms suspicious for stroke and underwent NCCT with RAPID ICH postprocessing. RAPID ICH detection was compared with the interpretation of a reference standard comprising a board-certified or board-eligible neuroradiologist, or in cases of discrepancy, adjudicated by a consensus panel of 3 neuroradiologists. Accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of RAPID ICH for ICH detection were determined. Results Three hundred seven NCCT scans were included in the study. RAPID ICH correctly identified 34 of 37 cases with ICH and 228 of 270 without ICH. RAPID ICH had a sensitivity of 91.9% (78.1%-98.3%), specificity of 84.4% (79.6%-88.6%), NPV of 98.7% (96.3%-99.6%), PPV of 44.7% (37.6%-52.1%), and overall accuracy of 85.3% (80.9%-89.1%). Conclusions In a real-world scenario, RAPID ICH software demonstrated high NPV but low PPV for the presence of ICH when evaluating possible stroke patients. © Wolters Kluwer Health, Inc. All rights reserved.},
	author_keywords = {AI; artificial intelligence; convolutional neuronal network; effectiveness study; ICH; intracranial hemorrhage; NCCT; pragmatic study; RAPID},
	keywords = {Humans; Intracranial Hemorrhages; Predictive Value of Tests; Retrospective Studies; Stroke; Tomography, X-Ray Computed; Computerized tomography; Neural networks; Neurons; Software testing; Convolutional neuronal network; Effectiveness study; Hemorrhage detection; Intracranial hemorrhages; Neuronal networks; Noncontrast head computed tomography; Positive predictive values; Pragmatic study; RAPID; accuracy; adult; Article; artificial intelligence; bleeding; brain; brain hemorrhage; brain tumor; cavernous hemangioma; cerebrovascular accident; computed tomographic angiography; computer assisted tomography; consensus; controlled study; diagnostic test accuracy study; electronic medical record; emergency physician; human; major clinical study; middle aged; neurologic disease; neuroradiologist; nuclear magnetic resonance imaging; predictive value; prevalence; retrospective study; sensitivity and specificity; standard; stroke patient; Sylvian fissure; task performance; cerebrovascular accident; diagnostic imaging; procedures; x-ray computed tomography; Statistical tests},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Bronze Open Access}
}

@ARTICLE{Dean20213259,
	author = {Dean, Cazes and Meir, Kalech},
	title = {Model-based diagnosis with uncertain observations},
	year = {2021},
	journal = {International Journal of Intelligent Systems},
	volume = {36},
	number = {7},
	pages = {3259 – 3292},
	doi = {10.1002/int.22416},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103646418&doi=10.1002%2fint.22416&partnerID=40&md5=23a11d14ce9e0c3d47192cd4f34848d9},
	abstract = {Classical model-based diagnosis uses a model of the system to infer diagnoses—explanations—of a given abnormal observation. In this study we address the case where there is uncertainty over a given observation and explore it. This can happen, for example, when the observation outputs are collected by sensors with some noise, that are known to return incorrect value with some probability. We formally define this problem for abductive and consistency-based forms domains. Furthermore, we propose two complete and sound algorithms for finding and ranking all diagnoses and analyze their complexity. We dive deeper and improve the first algorithm's efficiency even more by exploiting past results. Finally, we propose a third algorithm that returns the most likely diagnosis without finding all possible diagnoses, assuming the uncertainty over the observations and the components behavior modes are independents. Experimental evaluation shows that the last algorithm can be very efficient in cases where the diagnosis cardinality, as well as the uncertainty likelihood over the observation, are expected to be small. If, however, all possible diagnoses are desired, then the choice between the first two algorithms depends on whether the domain's diagnosis form is abductive or consistent. © 2021 Wiley Periodicals LLC},
	author_keywords = {MBD; model-based diagnosis; software diagnosis; uncertain observations; uncertainty},
	keywords = {Artificial intelligence; Software engineering; Cardinalities; Classical model; Experimental evaluation; Model based diagnosis; Most likely; Uncertain observations; Petroleum reservoir evaluation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Ban20232215,
	author = {Ban, Younghoon and Yi, Jeong Hyun and Cho, Haehyun},
	title = {Augmenting Android Malware Using Conditional Variational Autoencoder for the Malware Family Classification},
	year = {2023},
	journal = {Computer Systems Science and Engineering},
	volume = {46},
	number = {2},
	pages = {2215 – 2230},
	doi = {10.32604/csse.2023.036555},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148242186&doi=10.32604%2fcsse.2023.036555&partnerID=40&md5=b55a2947b5da7a7f5db8ef333a8b0160},
	abstract = {Android malware has evolved in various forms such as adware that continuously exposes advertisements, banking malware designed to access users’ online banking accounts, and Short Message Service (SMS) malware that uses a Command & Control (C&C) server to send malicious SMS, intercept SMS, and steal data. By using many malicious strategies, the number of malware is steadily increasing. Increasing Android malware threats numerous users, and thus, it is necessary to detect malware quickly and accurately. Each malware has distinguishable characteristics based on its actions. Therefore, security researchers have tried to categorize malware based on their behaviors by conducting the familial analysis which can help analysists to reduce the time and cost for analyzing malware. However, those studies algorithms typically used imbalanced, well-labeled open-source dataset, and thus, it is very difficult to classify some malware families which only have a few number of malware. To overcome this challenge, previous data augmentation studies augmented data by visualizing malicious codes and used them for malware analysis. However, visualization of malware can result in misclassifications because the behavior information of the malware could be compromised. In this study, we propose an android malware familial analysis system based on a data augmentation method that preserves malware behaviors to create an effective multi-class classifier for malware family analysis. To this end, we analyze malware and use Application Programming Interface (APIs) and permissions that can reflect the behavior of malware as features. By using these features, we augment malware dataset to enable effective malware detection while preserving original malicious behaviors. Our evaluation results demonstrate that, when a model is created by using only the augmented data, a macro-F1 score of 0.65 and accuracy of 0.63%. On the other hand, when the augmented data and original malware are used together, the evaluation results show that a macroF1 score of 0.91 and an accuracy of 0.99%. © 2023 CRL Publishing. All rights reserved.},
	author_keywords = {Android; artificial intelligence; cybersecurity; data augmentation},
	keywords = {Android (operating system); Application programming interfaces (API); Classification (of information); Cybersecurity; Learning systems; Mobile security; Open source software; Text messaging; Android; Android malware; Auto encoders; Cyber security; Data augmentation; Evaluation results; Malware families; Malwares; On-line banking; Short message services; Android malware},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Freeman202282,
	author = {Freeman, Laura and Batarseh, Feras A. and Kuhn, D. Richard and Raunak, M S and Kacker, Raghu N and Lin, Hsiao-Ying},
	title = {The Path to a Consensus on Artificial Intelligence Assurance},
	year = {2022},
	journal = {Computer},
	volume = {55},
	number = {3},
	pages = {82 – 86},
	doi = {10.1109/MC.2021.3129027},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127614808&doi=10.1109%2fMC.2021.3129027&partnerID=40&md5=37ac43564cab7215fe9cb776aae38d1f},
	abstract = {Wide-scale adoption of intelligent algorithms requires artificial intelligence (AI) engineers to provide assurances that an algorithm will perform as intended. In this article, we discuss the formalization of important aspects of AI assurance, including its key components. © 2022 IEEE.},
	keywords = {Formalisation; Intelligent Algorithms; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access, Bronze Open Access}
}

@ARTICLE{Zhang2022,
	author = {Zhang, Yu-Dong and Satapathy, Suresh Chandra and Wang, Shui-Hua},
	title = {Fruit category classification by fractional Fourier entropy with rotation angle vector grid and stacked sparse autoencoder},
	year = {2022},
	journal = {Expert Systems},
	volume = {39},
	number = {3},
	doi = {10.1111/exsy.12701},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104007872&doi=10.1111%2fexsy.12701&partnerID=40&md5=f087b86b918daff1c529fb2c41460557},
	abstract = {Aim: Fruit category classification is important in factory packing and transportation, price prediction, dietary intake, and so forth. Methods: This study proposed a novel artificial intelligence system to classify fruit categories. First, 2D fractional Fourier entropy with rotation angle vector grid was used to extract features from fruit images. Afterwards, a five-layer stacked sparse autoencoder was used as the classifier. Results: Ten runs on the test set showed our method achieved a micro-averaged F1 score of 95.08% for an 18-category fruit dataset. Conclusion: Our method gives better micro-averaged F1 score than 10 state-of-the-art approaches. © 2021 John Wiley & Sons Ltd.},
	author_keywords = {autoencoder; deep learning; fractional Fourier entropy; rotational angle vector grid},
	keywords = {Artificial intelligence; Entropy; Learning systems; Statistical tests; Artificial intelligence systems; Auto encoders; Category Classification; Dietary intakes; Fractional fourier; Price prediction; Rotation angles; State-of-the-art approach; Fruits},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access, Green Open Access}
}

@ARTICLE{Labzioui2023997,
	author = {Labzioui, Redouane and Letrache, Khadija and Ramdani, Mohammed},
	title = {New Approach based on Association Rules for Building and Optimizing OLAP Cubes on Graphs},
	year = {2023},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {14},
	number = {7},
	pages = {997 – 1008},
	doi = {10.14569/IJACSA.2023.01407108},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168798636&doi=10.14569%2fIJACSA.2023.01407108&partnerID=40&md5=59c1d97ed2148756f5c671f16b2a97cd},
	abstract = {The expansion of data has prompted the creation of various NoSQL (Not only SQL) databases, including graphoriented databases, which provide an understandable abstraction for modeling complex domains and managing highly connected data. However, to add graph data to existing decision support systems, new data warehouse systems that consider the special characteristics of graphs need to be developed. This work proposes a novel method for creating a data warehouse under a graph database and demonstrates how OLAP (Online Analytical Processing) structures created for reporting can be handled by graph databases. Additionally, the paper suggests using aggregation algorithms based association rules techniques to improve the efficiency of reporting and data analysis within a graph-based data warehouse. Finally, we provide a Cypher language implementation of the suggested approach to evaluate and validate our approach. © 2023, Science and Information Organization. All Rights Reserved.},
	author_keywords = {aggregation algorithms; association rules; cypher language; data warehouse; graph-oriented databases; NoSQL; OLAP},
	keywords = {Artificial intelligence; Association rules; Decision support systems; Graph Databases; Graphic methods; Aggregation algorithms; Cipher language; Complex domains; Graph database; Graph-oriented database; Model complexes; New approaches; Not only SQL; Online analytical processing; SQL database; Data warehouses},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Chen2021,
	author = {Chen, Yaping},
	title = {Framework of the Smart Finance and Accounting Management Model under the Artificial Intelligence Perspective},
	year = {2021},
	journal = {Mobile Information Systems},
	volume = {2021},
	doi = {10.1155/2021/4295191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118131452&doi=10.1155%2f2021%2f4295191&partnerID=40&md5=62937379c6baecbe06f0053126684dfc},
	abstract = {With the establishment of modern enterprise systems and the emergence of some large-scale enterprise groups formed through asset reorganization, industry alliances, and cross-industry mergers, new requirements have been put forward for the financial management mode of enterprise groups. This article mainly studies the development of the smart accounting management model architecture under the artificial intelligence perspective. On the accounting sharing service platform, the accounting sharing center does not belong to the logistics branch of any region. Through the accounting sharing center, it provides unified and standardized accounting, asset management, currency revenue and expenditure, etc., for the branches of the logistics company. At the same time, in the network environment, the financial and accounting sharing platform is not unilaterally closed. As each branch under unified management, it has the power to feedback, suggest, and supervise the financial and accounting sharing platform. When testing, one only needs to focus on the external properties of the program, without considering the internal logical structure and internal characteristics of the program at all. The system under test is regarded as a sealed black box, and the boundary value analysis method, the equivalence class division method, and causality are used. Technical methods such as the graph method and error speculation method are tested at the system interface to detect whether the function of each test can receive and output the results correctly. The implementation of the accounting management model of accounting shared services must ensure that the evaluation results of related work are objective and accurate. It is necessary to ensure that the evaluation index system has a high degree of quantification. In the specific scoring, the full score system can be used to score points by experts to ensure the objectivity of the evaluation results and accuracy while improving the operability and enforceability of the evaluation work. The average value of the operating net profit margin is 0.088731, and the median value is 0.082263. The results show that artificial intelligence technology has greatly promoted the development of the smart accounting management model architecture.  © 2021 Yaping Chen.},
	keywords = {Artificial intelligence; Economics; Equivalence classes; Mergers and acquisitions; Network architecture; Software testing; Cross industry; Enterprise groups; Enterprise system; Evaluation results; Industry alliances; Large-scales; Management Model; Modeling architecture; Reorganisation; Sharing platforms; Finance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Sofian202251021,
	author = {Sofian, Hazrina and Yunus, Nur Arzilawati Md and Ahmad, Rodina},
	title = {Systematic Mapping: Artificial Intelligence Techniques in Software Engineering},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {51021 – 51040},
	doi = {10.1109/ACCESS.2022.3174115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130778732&doi=10.1109%2fACCESS.2022.3174115&partnerID=40&md5=e36e8ee7999d57a6b18554a0dc5d434f},
	abstract = {Artificial Intelligence (AI) has become a core feature of today's real-world applications, making it a trending topic within the software engineering (SE) community. The rise in the availability of AI techniques encompasses the capability to make rapid, automated, impactful decisions and predictions, leading to the adoption of AI techniques in SE. With industry revolution 4.0, the role of software engineering has become critical for developing productive, efficient, and quality software. Thus, there is a major need for AI techniques to be applied to enhance and improve the critical activities within the software engineering phases. Software is developed through intelligent software engineering phases. This paper concerns a systematic mapping study that aimed to characterize the publication landscape of AI techniques in software engineering. Gaps are identified and discussed by mapping these AI techniques against the SE phases to which they contributed. Many systematic mapping review papers have been produced only for a specific AI technique or a specific SE phase or activity. Hence, to our best of knowledge within the last decade, there is no systematic mapping review that has fully explored the overall trends in AI techniques and their application to all SE phases. © 2013 IEEE.},
	author_keywords = {Analysis and design; Artificial intelligence; Data mining; Deep learning; Machine learning; Requirements engineering; Software deployment; Software development; Software engineering; Software maintenance; Software testing},
	keywords = {Application programs; Computer software maintenance; Deep learning; Mapping; Requirements engineering; Software design; Software testing; Analyse and design; Deep learning; Machine-learning; Predictive models; Requirement engineering; Software; Software deployment; Software development; Software testings; Systematic; Data mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22; All Open Access, Gold Open Access}
}

@ARTICLE{Job2021594,
	author = {Job, Minimol Anil},
	title = {Automating and Optimizing Software Testing using Artificial Intelligence Techniques},
	year = {2021},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {12},
	number = {5},
	pages = {594 – 602},
	doi = {10.14569/IJACSA.2021.0120571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107536835&doi=10.14569%2fIJACSA.2021.0120571&partnerID=40&md5=480c4348e6115756c2bbbec33d7c5f47},
	abstract = {The final product of software development process is a software system and testing is one of the important stages in this process. The success of this process can be determined by how well it accomplishes its goal. Due to the advancement of technology, various software testing tools have been introduced in the software engineering discipline. The use of software is increasing day-by-day and complexity of software functions are challenging and there is need to release the software within the short quality evaluation period, there is a high demand in adopting automation in software testing. Emergence of automatic software testing tools and techniques helps in quality enhancement and reducing time and cost in the software development activity. Artificial Intelligence (AI) techniques are widely applied in different areas of Software engineering (SE). Application of AI techniques can help in achieving good performance in software Testing and increase the productivity of the software development firms. This paper briefly presents the state of the art in the field of software testing by applying AI techniques in software testing. © 2021. All Rights Reserved.},
	author_keywords = {artificial intelligence; software engineering; software quality; Software testing; testing automation},
	keywords = {Application programs; Artificial intelligence; Computer software selection and evaluation; Software design; Artificial intelligence techniques; Engineering disciplines; High demand; Quality evaluation; Software development process; Software functions; Software testings; Software-systems; Testing automation; Testing tools; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Bohm202216,
	author = {Bohm, A. and Jajcay, N.},
	title = {Technical and practical aspects of artificial intelligence in cardiology},
	year = {2022},
	journal = {Bratislava Medical Journal},
	volume = {123},
	number = {1},
	pages = {16 – 21},
	doi = {10.4149/BLL_2022_003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123036914&doi=10.4149%2fBLL_2022_003&partnerID=40&md5=0681cba17cf06ca3cf66374f56778f53},
	abstract = {Artificial intelligence (AI) is here to stay. It is not a future anymore, and there are many particular problems in cardiology that are already being solved via machine learning (ML), and many more are to come. AI cannot solve complex tasks yet, and probably this will not change in the upcoming years. Therefore, cardiologists do not have to be afraid that computers will replace them. However, cardiologists who will not be able to use ML algorithms in their clinical practice will be replaced by those who will. (Fig. 2, Ref. 50). Text in PDF www.elis.sk © 2022. All Rights Reserved.},
	author_keywords = {artificial intelligence; automated analysis of various imaging examinations; cardiology; classification algorithms; computer vision; ECG interpretation; pathophysiological mechanisms; phenotype clustering; potential machine learning; survival models},
	keywords = {Algorithms; Artificial Intelligence; Cardiology; Machine Learning; Phenotype; algorithm; artificial intelligence; cardiology; machine learning; phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access, Gold Open Access}
}

@ARTICLE{Dong2022,
	author = {Dong, Qizheng},
	title = {Leakage Prediction in Machine Learning Models When Using Data from Sports Wearable Sensors},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	doi = {10.1155/2022/5314671},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130936999&doi=10.1155%2f2022%2f5314671&partnerID=40&md5=89762904ba5837a9539d88317db79712},
	abstract = {One of the major problems in machine learning is data leakage, which can be directly related to adversarial type attacks, raising serious concerns about the validity and reliability of artificial intelligence. Data leakage occurs when the independent variables used to teach the machine learning algorithm include either the dependent variable itself or a variable that contains clear information that the model is trying to predict. This data leakage results in unreliable and poor predictive results after the development and use of the model. It prevents the model from generalizing, which is required in a machine learning problem and thus causes false assumptions about its performance. To have a solid and generalized forecasting model, which will be able to produce remarkable forecasting results, we must pay great attention to detecting and preventing data leakage. This study presents an innovative system of leakage prediction in machine learning models, which is based on Bayesian inference to produce a thorough approach to calculating the reverse probability of unseen variables in order to make statistical conclusions about the relevant correlated variables and to calculate accordingly a lower limit on the marginal likelihood of the observed variables being derived from some coupling method. The main notion is that a higher marginal probability for a set of variables suggests a better fit of the data and thus a greater likelihood of a data leak in the model. The methodology is evaluated in a specialized dataset derived from sports wearable sensors.  © 2022 Qizheng Dong.},
	keywords = {Artificial Intelligence; Bayes Theorem; Machine Learning; Reproducibility of Results; Wearable Electronic Devices; Bayesian networks; Inference engines; Learning algorithms; Machine learning; Sports; Wearable sensors; Bayesian inference; Data leakage; Dependent variables; Forecasting models; Independent variables; Innovative systems; Machine learning algorithms; Machine learning models; Machine learning problem; Performance; artificial intelligence; Bayes theorem; electronic device; machine learning; reproducibility; Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Alghassab202212139,
	author = {Alghassab, Mohammed},
	title = {Quantitative assessment of sustainable renewable energy through soft computing: Fuzzy AHP-TOPSIS method},
	year = {2022},
	journal = {Energy Reports},
	volume = {8},
	pages = {12139 – 12152},
	doi = {10.1016/j.egyr.2022.09.049},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138482465&doi=10.1016%2fj.egyr.2022.09.049&partnerID=40&md5=50e91d55e83409475049357366ca4afe},
	abstract = {Green energy resources need to be mobilized at an imminent pace to fulfill the world's energy requirements that are both sustainable and cost-effective. Utilizing soft computing for generating sustainable power effectively eliminates additional requirements and costs. The selection procedure of Fuzzy Analytic Hierarchy Process (FAHP) and Fuzzy Technique for Order of Preferences by Similarity to Ideal Solution (FTOPSIS) plays a focal part in sustainable power and soft computing. We assess and analyze to create a long-term renewable energy control system with enhanced system protection against cyber-attacks. The factors affecting the sustainable renewable energy resources selected in primary level further every initial factors have five dependent secondary factors are selected for the assessment. The different approaches of generation of renewable energy are taken as alternatives. FAHP technique is required due to the nature of the characteristics involved in evaluating the performance assessment of the renewable energy control system and the weight of the factors. The FTOPSIS-based technique looks to be a good strategy for selecting the best option from a large number of options. The finding shows that Wind energy is fully sustainable most preferable and leading renewable energy source. The proposed work's novelty will stem from the use of hybrid soft computing techniques and the application of the issue to the energy industry. © 2022 The Author(s)},
	author_keywords = {Artificial Intelligence; FAHP; FTOPSIS; Machine Learning; Renewable energy; Sustainability},
	keywords = {Control systems; Machine learning; Network security; Soft computing; Sustainable development; Wind power; Fuzzy analytic hierarchy; Fuzzy analytic hierarchy process; Fuzzy technique for order of preference by similarity to ideal solution; Fuzzy techniques; Ideal solutions; Machine-learning; Process techniques; Renewable energies; Soft-Computing; Sustainable power; Cost effectiveness},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; All Open Access, Gold Open Access}
}

@ARTICLE{Kusharki2022,
	author = {Kusharki, Muhammad Bello and Misra, Sanjay and Muhammad-Bello, Bilkisu and Salihu, Ibrahim Anka and Suri, Bharti},
	title = {Automatic Classification of Equivalent Mutants in Mutation Testing of Android Applications},
	year = {2022},
	journal = {Symmetry},
	volume = {14},
	number = {4},
	doi = {10.3390/sym14040820},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129172137&doi=10.3390%2fsym14040820&partnerID=40&md5=e853c7d5ea41840d24392c5e3c5ff665},
	abstract = {Software and symmetric testing methodologies are primarily used in detecting software defects, but these testing methodologies need to be optimized to mitigate the wasting of resources. As mobile applications are becoming more prevalent in recent times, the need to have mobile applications that satisfy software quality through testing cannot be overemphasized. Testing suites and software quality assurance techniques have also become prevalent, which underscores the need to evaluate the efficacy of these tools in the testing of the applications. Mutation testing is one such technique, which is the process of injecting small changes into the software under test (SUT), thereby creating mutants. These mutants are then tested using mutation testing techniques alongside the SUT to determine the effectiveness of test suites through mutation scoring. Although mutation testing is effective, the cost of implementing it, due to the problem of equivalent mutants, is very high. Many research works gave varying solutions to this problem, but none used a standardized dataset. In this research work, we employed a standard mutant dataset tool called MutantBench to generate our data. Subsequently, an Abstract Syntax Tree (AST) was used in conjunction with a tree-based convolutional neural network (TBCNN) as our deep learning model to automate the classification of the equivalent mutants to reduce the cost of mutation testing in software testing of android applications. The result shows that the proposed model produces a good accuracy rate of 94%, as well as other performance metrics such as recall (96%), precision (89%), F1-score (92%), and Matthew’s correlation coefficients (88%) with fewer False Negatives and False Positives during testing, which is significant as it implies that there is a decrease in the risk of misclassification. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {android applications; artificial intelligence; mutation testing; software testing; tree-based convolutional neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Amadini2020,
	author = {Amadini, Roberto and Gange, Graeme and Stuckey, Peter J.},
	title = {Dashed strings for string constraint solving},
	year = {2020},
	journal = {Artificial Intelligence},
	volume = {289},
	doi = {10.1016/j.artint.2020.103368},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090321988&doi=10.1016%2fj.artint.2020.103368&partnerID=40&md5=e3ae13283948e54aabd9ce28396c5ea2},
	abstract = {String processing is ubiquitous across computer science, and arguably more so in web programming — where it is also a critical part of security issues such as injection attacks. In recent years, a number of string solvers have been developed to solve combinatorial problems involving string variables and constraints. We examine the dashed string approach to string constraint solving, which represents an unknown string as a sequence of blocks of characters with bounds on their cardinalities. The solving approach relies on propagation of information about the blocks of characters that arise from reasoning about the constraints in which they occur. This approach shows promising performance on many benchmarks involving constraints like string length, equality, concatenation, and regular expression membership. In this paper, we formally review the definition, the properties and the use of dashed strings for string constraint solving, and we provide an empirical validation that confirms the effectiveness of this approach. © 2020 Elsevier B.V.},
	author_keywords = {Artificial intelligence; Constraint programming; String solving},
	keywords = {Benchmarking; Network security; Cardinalities; Combinatorial problem; Constraint Solving; Empirical validation; Regular expressions; Security issues; String processing; Web programming; Logic programming},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access, Bronze Open Access, Green Open Access}
}

@ARTICLE{Zhu2022,
	author = {Zhu, Hong and Bayley, Ian},
	title = {Discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing},
	year = {2022},
	journal = {Journal of Systems and Software},
	volume = {187},
	doi = {10.1016/j.jss.2022.111231},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124473834&doi=10.1016%2fj.jss.2022.111231&partnerID=40&md5=3b0c2334b3b7fd2a620e68ca8281767f},
	abstract = {Testing has been widely recognised as difficult for AI applications. This paper proposes a set of testing strategies for testing machine learning applications in the framework of the datamorphism testing methodology. In these strategies, testing aims at exploring the data space of a classification or clustering application to discover the boundaries between classes that the machine learning application defines. This enables the tester to understand precisely the behaviour and function of the software under test. In the paper, three variants of exploratory strategies are presented with the algorithms implemented in the automated datamorphic testing tool Morphy. The correctness of these algorithms are formally proved. Their capability and cost of discovering borders between classes are evaluated via a set of controlled experiments with manually designed subjects and a set of case studies with real machine learning models. © 2022 Elsevier Inc.},
	author_keywords = {Artificial intelligence; Automation of software test; Datamorphic testing; Exploratory testing; Software testing; Test strategies},
	keywords = {Classification (of information); Computer aided instruction; Machine learning; AI applications; Automation of software test; Boundary values; Datamorphic testing; Exploratory testing; Feature-based; Machine learning applications; Software testings; Test strategies; Testing strategies; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Green Open Access}
}

@ARTICLE{Pytka2021,
	author = {Pytka, Jarosław and Budzyński, Piotr and Tomiło, Paweł and Michałowska, Joanna and Gnapowski, Ernest and Błażejczak, Dariusz and Łukaszewicz, Andrzej},
	title = {IMUMETER—A convolution neural network-based sensor for measurement of aircraft ground performance},
	year = {2021},
	journal = {Sensors},
	volume = {21},
	number = {14},
	doi = {10.3390/s21144726},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109268329&doi=10.3390%2fs21144726&partnerID=40&md5=372a7955c37b7ece90c2063876f09073},
	abstract = {The paper presents the development of the IMUMETER sensor, designed to study the dynamics of aircraft movement, in particular, to measure the ground performance of the aircraft. A motivation of this study was to develop a sensor capable of airplane motion measurement, especially for airfield performance, takeoff and landing. The IMUMETER sensor was designed on the basis of the method of artificial neural networks. The use of a neural network is justified by the fact that the automation of the measurement of the airplane’s ground distance during landing based on acceleration data is possible thanks to the recognition of the touchdown and stopping points, using artificial intelligence. The hardware is based on a single-board computer that works with the inertial navigation platform and a satellite navigation sensor. In the development of the IMUMETER device, original software solutions were developed and tested. The paper describes the development of the Convolution Neural Network, including the learning process based on the measurement results during flight tests of the PZL 104 Wilga 35A aircraft. The ground distance of the test airplane during landing on a grass runway was calculated using the developed neural network model. Additionally included are exemplary measurements of the landing distance of the test airplane during landing on a grass runway. The results obtained in this study can be useful in the development of artificial intelligence-based sensors, especially those for the measurement and analysis of aircraft flight dynamics. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Aircraft; Aircraft motion; Airfield performance; Artificial neural network; Convolution neural network; GARFIELD project; Grass airfield; IMU/GNSS sensor},
	keywords = {Acceleration; Aircraft; Artificial Intelligence; Neural Networks, Computer; Aircraft; Computer hardware; Convolution; Flight dynamics; Landing; Software testing; Vehicle performance; Aircraft movements; Convolution neural network; Inertial navigation platform; Measurement and analysis; Motion measurements; Neural network model; Satellite navigation; Single board computers; acceleration; aircraft; artificial intelligence; Neural networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Peischl2022,
	author = {Peischl, Bernhard and Tazl, Oliver A. and Wotawa, Franz},
	title = {Testing anticipatory systems: A systematic mapping study on the state of the art},
	year = {2022},
	journal = {Journal of Systems and Software},
	volume = {192},
	doi = {10.1016/j.jss.2022.111387},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134641034&doi=10.1016%2fj.jss.2022.111387&partnerID=40&md5=77b86d05d7c000808ee660ed9ba08212},
	abstract = {Context: Systems exhibiting anticipatory behavior are controlling devices that are influencing decisions critical to business with increasing frequency, but testing such systems has received little attention from the artificial intelligence or software engineering communities. Goal: In this article, we describe research activities being carried out to test anticipatory systems and explore how this research contributes to the body of knowledge. In addition, we review the types of addressed anticipatory applications and point out open issues and trends. Method: This systematic mapping study was conducted to classify and analyze the literature on testing anticipatory systems, enabling us to highlight the most relevant topics and potential gaps in this field. Results: We identified 206 studies that contribute to the testing of systems that exhibit anticipatory behavior. The papers address testing at stages such as context sensing, inferring higher-level concepts from the sensed data, predicting the future context, and intelligent decision-making. We also identified agent testing as a trend, among others. Conclusion: The existing literature on testing anticipatory systems has originated from various research communities, such as those on autonomous agents and quality engineering. Although researchers have recently exhibited increasing interest in testing anticipatory systems, theoretical knowledge about testing such systems is lacking. © 2022 The Author(s)},
	author_keywords = {Anticipatory systems; Artificial intelligence; Mapping study; Software testing; Validation; Verification},
	keywords = {Artificial intelligence; Autonomous agents; Decision making; Mapping; Software testing; Anticipatory systems; Body of knowledge; Controlling devices; Engineering community; Mapping studies; Research activities; Software testings; State of the art; Systematic mapping studies; Validation; Verification},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Borsci202295,
	author = {Borsci, Simone and Malizia, Alessio and Schmettow, Martin and van der Velde, Frank and Tariverdiyeva, Gunay and Balaji, Divyaa and Chamberlain, Alan},
	title = {The Chatbot Usability Scale: the Design and Pilot of a Usability Scale for Interaction with AI-Based Conversational Agents},
	year = {2022},
	journal = {Personal and Ubiquitous Computing},
	volume = {26},
	number = {1},
	pages = {95 – 119},
	doi = {10.1007/s00779-021-01582-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110968998&doi=10.1007%2fs00779-021-01582-9&partnerID=40&md5=084b1ac7aaeb1f93700011c1e5b28e42},
	abstract = {Standardised tools to assess a user’s satisfaction with the experience of using chatbots and conversational agents are currently unavailable. This work describes four studies, including a systematic literature review, with an overall sample of 141 participants in the survey (experts and novices), focus group sessions and testing of chatbots to (i) define attributes to assess the quality of interaction with chatbots and (ii) the designing and piloting a new scale to measure satisfaction after the experience with chatbots. Two instruments were developed: (i) A diagnostic tool in the form of a checklist (BOT-Check). This tool is a development of previous works which can be used reliably to check the quality of a chatbots experience in line with commonplace principles. (ii) A 15-item questionnaire (BOT Usability Scale, BUS-15) with estimated reliability between.76 and.87 distributed in five factors. BUS-15 strongly correlates with UMUX-LITE by enabling designers to consider a broader range of aspects usually not considered in satisfaction tools for non-conversational agents, e.g. conversational efficiency and accessibility, quality of the chatbot’s functionality and so on. Despite the convincing psychometric properties, BUS-15 requires further testing and validation. Designers can use it as a tool to assess products, thus building independent databases for future evaluation of its reliability, validity and sensitivity. © 2021, The Author(s).},
	author_keywords = {AI; Artificial intelligence; Autonomy; Chatbots; Conversational agents; Design; Evaluation; Human-Computer interaction (HCI); Interaction satisfaction; Satisfaction; Trust; Usability; User experience},
	keywords = {Buses; Surveys; Chatbot; Chatbots; Conversational agents; Diagnostic tools; Focus groups; Psychometric properties; Quality of interaction; Systematic literature review; User experience},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 125; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Nawaz20213086,
	author = {Nawaz, M. Saqib and Fournier-Viger, Philippe and Shojaee, Abbas and Fujita, Hamido},
	title = {Using artificial intelligence techniques for COVID-19 genome analysis},
	year = {2021},
	journal = {Applied Intelligence},
	volume = {51},
	number = {5},
	pages = {3086 – 3103},
	doi = {10.1007/s10489-021-02193-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101144333&doi=10.1007%2fs10489-021-02193-w&partnerID=40&md5=d3f1f01012de40a4b22716ad68f7c2c4},
	abstract = {The genome of the novel coronavirus (COVID-19) disease was first sequenced in January 2020, approximately a month after its emergence in Wuhan, capital of Hubei province, China. COVID-19 genome sequencing is critical to understanding the virus behavior, its origin, how fast it mutates, and for the development of drugs/vaccines and effective preventive strategies. This paper investigates the use of artificial intelligence techniques to learn interesting information from COVID-19 genome sequences. Sequential pattern mining (SPM) is first applied on a computer-understandable corpus of COVID-19 genome sequences to see if interesting hidden patterns can be found, which reveal frequent patterns of nucleotide bases and their relationships with each other. Second, sequence prediction models are applied to the corpus to evaluate if nucleotide base(s) can be predicted from previous ones. Third, for mutation analysis in genome sequences, an algorithm is designed to find the locations in the genome sequences where the nucleotide bases are changed and to calculate the mutation rate. Obtained results suggest that SPM and mutation analysis techniques can reveal interesting information and patterns in COVID-19 genome sequences to examine the evolution and variations in COVID-19 strains respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
	author_keywords = {COVID-19; Genome sequence; Mutation; Nucleotide bases; Sequential pattern mining},
	keywords = {Genes; Genetic algorithms; Genetic engineering; Nucleotides; Predictive analytics; Software testing; Artificial intelligence techniques; Genome sequencing; Interesting information; Mutation analysis; Nucleotide basis; Preventive strategies; Sequence prediction; Sequential-pattern mining; Artificial intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 83; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Yuba2022,
	author = {Yuba, Mitsuru and Iwasaki, Kiyotaka},
	title = {Systematic analysis of the test design and performance of AI/ML-based medical devices approved for triage/detection/diagnosis in the USA and Japan},
	year = {2022},
	journal = {Scientific Reports},
	volume = {12},
	number = {1},
	doi = {10.1038/s41598-022-21426-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139514048&doi=10.1038%2fs41598-022-21426-7&partnerID=40&md5=99b42b95a27afd34d3268ea222085fd6},
	abstract = {The development of computer-aided detection (CAD) using artificial intelligence (AI) and machine learning (ML) is rapidly evolving. Submission of AI/ML-based CAD devices for regulatory approval requires information about clinical trial design and performance criteria, but the requirements vary between countries. This study compares the requirements for AI/ML-based CAD devices approved by the US Food and Drug Administration (FDA) and the Pharmaceuticals and Medical Devices Agency (PMDA) in Japan. A list of 45 FDA-approved and 12 PMDA-approved AI/ML-based CAD devices was compiled. In the USA, devices classified as computer-aided simple triage were approved based on standalone software testing, whereas devices classified as computer-aided detection/diagnosis were approved based on reader study testing. In Japan, however, there was no clear distinction between evaluation methods according to the category. In the USA, a prospective randomized controlled trial was conducted for AI/ML-based CAD devices used for the detection of colorectal polyps, whereas in Japan, such devices were approved based on standalone software testing. This study indicated that the different viewpoints of AI/ML-based CAD in the two countries influenced the selection of different evaluation methods. This study’s findings may be useful for defining a unified global development and approval standard for AI/ML-based CAD. © 2022, The Author(s).},
	keywords = {Artificial Intelligence; Japan; Machine Learning; Pharmaceutical Preparations; Prospective Studies; Triage; drug; artificial intelligence; controlled study; emergency health service; Japan; machine learning; prospective study; randomized controlled trial},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Herde2021166970,
	author = {Herde, Marek and Huseljic, Denis and Sick, Bernhard and Calma, Adrian},
	title = {A Survey on Cost Types, Interaction Schemes, and Annotator Performance Models in Selection Algorithms for Active Learning in Classification},
	year = {2021},
	journal = {IEEE Access},
	volume = {9},
	pages = {166970 – 166989},
	doi = {10.1109/ACCESS.2021.3135514},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121822862&doi=10.1109%2fACCESS.2021.3135514&partnerID=40&md5=ab21cdedffcdd0f6f1b5e1c5afab8949},
	abstract = {Pool-based active learning (AL) aims to optimize the annotation process (i.e., labeling) as the acquisition of annotations is often time-consuming and therefore expensive. For this purpose, an AL strategy queries annotations intelligently from annotators to train a high-performance classification model at a low annotation cost. Traditional AL strategies operate in an idealized framework. They assume a single, omniscient annotator who never gets tired and charges uniformly regardless of query difficulty. However, in real-world applications, we often face human annotators, e.g., crowd or in-house workers, who make annotation mistakes and can be reluctant to respond if tired or faced with complex queries. Recently, many novel AL strategies have been proposed to address these issues. They differ in at least one of the following three central aspects from traditional AL: 1) modeling of (multiple) human annotators whose performances can be affected by various factors, such as missing expertise; 2) generalization of the interaction with human annotators through different query and annotation types, such as asking an annotator for feedback on an inferred classification rule; 3) consideration of complex cost schemes regarding annotations and misclassifications. This survey provides an overview of these AL strategies and refers to them as real-world AL. Therefore, we introduce a general real-world AL strategy as part of a learning cycle and use its elements, e.g., the query and annotator selection algorithm, to categorize about 60 real-world AL strategies. Finally, we outline possible directions for future research in the field of AL.  © 2013 IEEE.},
	author_keywords = {Active learning; Classification; Error-prone annotators; Human-in-the-loop learning; Interactive learning; Machine learning},
	keywords = {Artificial intelligence; Crowdsourcing; Learning algorithms; Learning systems; Surveys; Active Learning; Annotation; Error prones; Error-prone annotator; Generator; Human-in-the-loop; Human-in-the-loop learning; Interactive learning; License; Low voltages; Costs},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 20; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Singhal20236755,
	author = {Singhal, Shweta and Jatana, Nishtha and Subahi, Ahmad F. and Gupta, Charu and Khalaf, Osamah Ibrahim and Alotaibi, Youseef},
	title = {Fault Coverage-Based Test Case Prioritization and Selection Using African Buffalo Optimization},
	year = {2023},
	journal = {Computers, Materials and Continua},
	volume = {74},
	number = {3},
	pages = {6755 – 6774},
	doi = {10.32604/cmc.2023.032308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145353381&doi=10.32604%2fcmc.2023.032308&partnerID=40&md5=3a52391f278f1e66c25aa2c08520e43c},
	abstract = {Software needs modifications and requires revisions regularly. Owing to these revisions, retesting software becomes essential to ensure that the enhancements made, have not affected its bug-free functioning. The time and cost incurred in this process, need to be reduced by the method of test case selection and prioritization. It is observed that many nature-inspired techniques are applied in this area. African Buffalo Optimization is one such approach, applied to regression test selection and prioritization. In this paper, the proposed work explains and proves the applicability of the African Buffalo Optimization approach to test case selection and prioritization. The proposed algorithm converges in polynomial time (O(n2)). In this paper, the empirical evaluation of applying African Buffalo Optimization for test case prioritization is done on sample data set with multiple iterations. An astounding 62.5% drop in size and a 48.57% drop in the runtime of the original test suite were recorded. The obtained results are compared with Ant Colony Optimization. The comparative analysis indicates that African Buffalo Optimization and Ant Colony Optimization exhibit similar fault detection capabilities (80%), and a reduction in the overall execution time and size of the resultant test suite. The results and analysis, hence, advocate and encourages the use of African Buffalo Optimization in the area of test case selection and prioritization. © 2023 Tech Science Press. All rights reserved.},
	author_keywords = {African buffalo optimization; meta-heuristic; nature-inspired; regression testing; Test case prioritization; test case selection},
	keywords = {Artificial intelligence; Biomimetics; Drops; Fault detection; Polynomial approximation; Statistical tests; African buffalo optimization; Bug-free; Fault coverages; Metaheuristic; Nature-inspired; Optimisations; Regression testing; Selection and prioritisation; Test case prioritization; Test case selection; Ant colony optimization},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 23; All Open Access, Gold Open Access}
}

@ARTICLE{Gomez-Perez2022,
	author = {Gomez-Perez, Sandra L. and Zhang, Yanyu and Byrne, Cecily and Wakefield, Connor and Geesey, Thomas and Sclamberg, Joy and Peterson, Sarah},
	title = {Concordance of Computed Tomography Regional Body Composition Analysis Using a Fully Automated Open-Source Neural Network versus a Reference Semi-Automated Program with Manual Correction},
	year = {2022},
	journal = {Sensors},
	volume = {22},
	number = {9},
	doi = {10.3390/s22093357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128767753&doi=10.3390%2fs22093357&partnerID=40&md5=70a2237ac8d7ef278b3cacde5cb41e95},
	abstract = {Quick, efficient, fully automated open-source programs to segment muscle and adipose tissues from computed tomography (CT) images would be a great contribution to body composition research. This study examined the concordance of cross-sectional areas (CSA) and densities for muscle, visceral adipose tissue (VAT), subcutaneous adipose tissue (SAT), and intramuscular adipose tissue (IMAT) from CT images at the third lumbar (L3) between an automated neural network (test method) and a semi-automatic human-based program (reference method). Concordance was further evaluated by disease status, sex, race/ethnicity, BMI categories. Agreement statistics applied included Lin’s Concordance (CCC), Spearman correlation coefficient (SCC), Sorensen dice-similarity coefficient (DSC), and Bland–Altman plots with limits of agreement (LOA) within 1.96 standard deviation. A total of 420 images from a diverse cohort of patients (60.35 ± 10.92 years; body mass index (BMI) of 28.77 ± 7.04 kg/m2; 55% female; 53% Black) were included in this study. About 30% of patients were healthy (i.e., received a CT scan for acute illness or pre-surgical donor work-up), while another 30% had a diagnosis of colorectal cancer. The CCC, SCC, and DSC estimates for muscle, VAT, SAT were all greater than 0.80 (>0.80 indicates good performance). Agreement analysis by diagnosis showed good performance for the test method except for critical illness (DSC 0.65–0.87). Bland–Altman plots revealed narrow LOA suggestive of good agreement despite minimal proportional bias around the zero-bias line for muscle, SAT, and IMAT CSA. The test method shows good performance and almost perfect concordance for L3 muscle, VAT, SAT, and IMAT per DSC estimates, and Bland–Altman plots even after stratification by sex, race/ethnicity, and BMI categories. Care must be taken to assess the density of the CT images from critically ill patients before applying the automated neural network (test method). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {adipose tissue; agreement; artificial intelligence; automated segmentation; body composition; computed tomography; muscle; validation},
	keywords = {Adipose Tissue; Body Composition; Body Mass Index; Female; Humans; Male; Neural Networks, Computer; Tomography, X-Ray Computed; Automation; Biochemistry; Diagnosis; Diseases; Image segmentation; Muscle; Software testing; Adipose tissue; Automated segmentation; Body composition; Computed tomography images; Intramuscular; Neural-networks; Similarity coefficients; Subcutaneous adipose tissues; Test method; Validation; adipose tissue; body composition; body mass; female; human; male; procedures; x-ray computed tomography; Computerized tomography},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Attaallah20233165,
	author = {Attaallah, Abdulaziz and al-Sulbi, Khalil and Alasiry, Areej and Marzougui, Mehrez and Khan, Mohd Waris and Faizan, Mohd and Agrawal, Alka and Pandey, Dhirendra},
	title = {Security Test Case Prioritization through Ant Colony Optimization Algorithm},
	year = {2023},
	journal = {Computer Systems Science and Engineering},
	volume = {47},
	number = {3},
	pages = {3165 – 3195},
	doi = {10.32604/csse.2023.040259},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176955093&doi=10.32604%2fcsse.2023.040259&partnerID=40&md5=3d1953f8a076ad2b5f7582c21ac67444},
	abstract = {Security testing is a critical concern for organizations worldwide due to the potential financial setbacks and damage to reputation caused by insecure software systems. One of the challenges in software security testing is test case prioritization, which aims to reduce redundancy in fault occurrences when executing test suites. By effectively applying test case prioritization, both the time and cost required for developing secure software can be reduced. This paper proposes a test case prioritization technique based on the Ant Colony Optimization (ACO) algorithm, a metaheuristic approach. The performance of the ACO-based technique is evaluated using the Average Percentage of Fault Detection (APFD) metric, comparing it with traditional techniques. It has been applied to a Mobile Payment Wallet application to validate the proposed approach. The results demonstrate that the proposed technique outperforms the traditional techniques in terms of the APFD metric. The ACO-based technique achieves an APFD of approximately 76%, two percent higher than the second-best optimal ordering technique. These findings suggest that metaheuristic-based prioritization techniques can effectively identify the best test cases, saving time and improving software security overall. © 2023 CRL Publishing. All rights reserved.},
	author_keywords = {Ant Colony Optimization algorithm; authentication; authorization; Confidentiality; integrity; non-repudiation; resilience},
	keywords = {Ant colony optimization; Artificial intelligence; Fault detection; Global system for mobile communications; Software testing; Ant Colony Optimization algorithms; Confidentiality; Faults detection; Integrity; Non repudiation; Optimization-based techniques; Prioritization techniques; Resilience; Test case prioritization; Traditional techniques; Authentication},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access, Hybrid Gold Open Access}
}@ARTICLE{Popchev2020116,
	author = {Popchev, Ivan P. and Orozova, Daniela A.},
	title = {Towards a multistep method for assessment in e-learning of emerging technologies},
	year = {2020},
	journal = {Cybernetics and Information Technologies},
	volume = {20},
	number = {3},
	pages = {116 – 129},
	doi = {10.2478/cait-2020-0032},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093080750&doi=10.2478%2fcait-2020-0032&partnerID=40&md5=801bf6408f319adfa4a6b4fefa68dcda},
	abstract = {In the Fourth Industrial Revolution some important leading technologies are identified as emerging technologies with unknown in advance potential risks. Emphasized is the need for new approaches and solutions for forming of increased information awareness, knowledge and competencies in the present and future generations to use the possibilities of Industry 4.0 for technological breakthroughs. A method for evaluation and prognosis of the knowledge, skills and competencies of the students in the virtual education space is proposed in the form of a five-step process. The method can be adapted to new technologies and applications. Research and analysis of the method are carried out in the academic course 'Artificial Intelligence' at the Burgas Free University with the application of the instruments of the Orange system for experimentation and inference. © 2020 Sciendo. All rights reserved.},
	author_keywords = {Artificial intelligence; E-Learning; Industry 4.0; Machine learning; Orange system; Risks; Virtual education space; Еmerging тechnologies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Gold Open Access}
}

@ARTICLE{Taylor2020,
	author = {Taylor, William and Shah, Syed Aziz and Dashtipour, Kia and Zahid, Adnan and Abbasi, Qammer H. and Imran, Muhammad Ali},
	title = {An intelligent non-invasive real-time human activity recognition system for next-generation healthcare},
	year = {2020},
	journal = {Sensors (Switzerland)},
	volume = {20},
	number = {9},
	doi = {10.3390/s20092653},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084352411&doi=10.3390%2fs20092653&partnerID=40&md5=8ea270edc76b7ed03c6e1e999e738f1b},
	abstract = {Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real-time monitoring by deploying equipment on a person’s body. However, putting devices on a person’s body all the time makes it uncomfortable and the elderly tend to forget to wear them, in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in a quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals present particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software-defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine-learning model was able to achieve 96.70% accuracy using the Random Forest algorithm using 10 fold cross-validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90%. The machine-learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Channel state information; Human motion detection; KNN; Machine learning; Neural networks; Random forest; Real-time classification; SVM; USRP},
	keywords = {Aged; Artificial Intelligence; Computer Systems; Delivery of Health Care; Human Activities; Humans; Wearable Electronic Devices; Classification (of information); Decision trees; Noninvasive medical procedures; Software radio; Software testing; Statistical tests; Wearable technology; 10-fold cross-validation; Developed applications; Human activity recognition systems; Human motion detection; Machine learning models; Random forest algorithm; Real time monitoring; Software-defined radios; aged; artificial intelligence; computer system; electronic device; health care delivery; human; human activities; Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 134; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Calderon-Gomez2020118340,
	author = {Calderon-Gomez, Huriviades and Mendoza-Pitti, Luis and Vargas-Lombardo, Miguel and Gomez-Pulido, Jose Manuel and Castillo-Sequera, Jose Luis and Sanz-Moreno, Jose and Sencion, Gloria},
	title = {Telemonitoring System for Infectious Disease Prediction in Elderly People Based on a Novel Microservice Architecture},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {118340 – 118354},
	doi = {10.1109/ACCESS.2020.3005638},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089557947&doi=10.1109%2fACCESS.2020.3005638&partnerID=40&md5=036539ea5677e57be6e1e7a8c960ba9e},
	abstract = {This article describes the design, development and implementation of a set of microservices based on an architecture that enables detection and assisted clinical diagnosis within the field of infectious diseases of elderly patients, via a telemonitoring system. The proposed system is designed to continuously update a medical database fed with vital signs from biosensor kits applied by nurses to elderly people on a daily basis. The database is hosted in the cloud and is managed by a flexible microservices software architecture. The computational paradigms of the edge and the cloud were used in the implementation of a hybrid cloud architecture in order to support versatile high-performance applications under the microservices pattern for the pre-diagnosis of infectious diseases in elderly patients. The results of an analysis of the usability of the equipment, the performance of the architecture and the service concept show that the proposed e-health system is feasible and innovative. The system components are also selected to give a cost-effective implementation for people living in disadvantaged areas. The proposed e-health system is also suitable for distributed computing, big data and NoSQL structures, thus allowing the immediate application of machine learning and AI algorithms to discover knowledge patterns from the overall population.  © 2013 IEEE.},
	author_keywords = {Artificial intelligence; e-health; elderly people; infectious diseases; microservice architecture; microservices; telemonitoring},
	keywords = {Cost effectiveness; Diagnosis; Diseases; Machine learning; Medical computing; Population statistics; Clinical diagnosis; Computational paradigm; Cost-effective implementations; High performance applications; Infectious disease; Knowledge patterns; System components; Telemonitoring systems; Computer architecture},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Gabor2020457,
	author = {Gabor, Thomas and Sedlmeier, Andreas and Phan, Thomy and Ritz, Fabian and Kiermeier, Marie and Belzner, Lenz and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner and Wieghardt, Jan and Zeller, Marc and Linnhoff-Popien, Claudia},
	title = {The scenario coevolution paradigm: adaptive quality assurance for adaptive systems},
	year = {2020},
	journal = {International Journal on Software Tools for Technology Transfer},
	volume = {22},
	number = {4},
	pages = {457 – 476},
	doi = {10.1007/s10009-020-00560-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081618983&doi=10.1007%2fs10009-020-00560-5&partnerID=40&md5=c713db31b550ed8562a95cf044d4eba2},
	abstract = {Systems are becoming increasingly more adaptive, using techniques like machine learning to enhance their behavior on their own rather than only through human developers programming them. We analyze the impact the advent of these new techniques has on the discipline of rigorous software engineering, especially on the issue of quality assurance. To this end, we provide a general description of the processes related to machine learning and embed them into a formal framework for the analysis of adaptivity, recognizing that to test an adaptive system a new approach to adaptive testing is necessary. We introduce scenario coevolution as a design pattern describing how system and test can work as antagonists in the process of software evolution. While the general pattern applies to large-scale processes (including human developers further augmenting the system), we show all techniques on a smaller-scale example of an agent navigating a simple smart factory. We point out new aspects in software engineering for adaptive systems that may be tackled naturally using scenario coevolution. This work is a substantially extended take on Gabor et al. (International symposium on leveraging applications of formal methods, Springer, pp 137–154, 2018). © 2020, The Author(s).},
	author_keywords = {Adaptation; Artificial intelligence; Coevolution; Machine learning; Quality assurance; Self-adaptive systems; Software engineering; Software evolution},
	keywords = {Adaptive systems; Artificial intelligence; Formal methods; Learning systems; Machine learning; Quality assurance; Software engineering; Software testing; Adaptation; Adaptive testing; Co-evolution; Formal framework; General description; General patterns; Self-adaptive system; Software Evolution; Software quality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Tahvili2020,
	author = {Tahvili, Sahar and Hatvani, Leo and Ramentol, Enislay and Pimentel, Rita and Afzal, Wasif and Herrera, Francisco},
	title = {A novel methodology to classify test cases using natural language processing and imbalanced learning},
	year = {2020},
	journal = {Engineering Applications of Artificial Intelligence},
	volume = {95},
	doi = {10.1016/j.engappai.2020.103878},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089416670&doi=10.1016%2fj.engappai.2020.103878&partnerID=40&md5=a4f298102ad1f065e676a33f25599c97},
	abstract = {Detecting the dependency between integration test cases plays a vital role in the area of software test optimization. Classifying test cases into two main classes – dependent and independent – can be employed for several test optimization purposes such as parallel test execution, test automation, test case selection and prioritization, and test suite reduction. This task can be seen as an imbalanced classification problem due to the test cases’ distribution. Often the number of dependent and independent test cases is uneven, which is related to the testing level, testing environment and complexity of the system under test. In this study, we propose a novel methodology that consists of two main steps. Firstly, by using natural language processing we analyze the test cases’ specifications and turn them into a numeric vector. Secondly, by using the obtained data vectors, we classify each test case into a dependent or an independent class. We carry out a supervised learning approach using different methods for handling imbalanced datasets. The feasibility and possible generalization of the proposed methodology is evaluated in two industrial projects at Bombardier Transportation, Sweden, which indicates promising results. © 2020 The Authors},
	author_keywords = {Artificial intelligence; Doc2Vec; IFROWANN; Imbalanced classification; Natural language processing; Optimization; Software testing},
	keywords = {Learning systems; Natural language processing systems; Testing; Bombardier Transportation; Imbalanced classification; Imbalanced Data-sets; Imbalanced Learning; NAtural language processing; Supervised learning approaches; Test case selection; Test suite reduction; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access, Green Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Daglarli202098491,
	author = {Daglarli, Evren},
	title = {Computational modeling of prefrontal cortex for meta-cognition of a humanoid robot},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {98491 – 98507},
	doi = {10.1109/ACCESS.2020.2998396},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086075046&doi=10.1109%2fACCESS.2020.2998396&partnerID=40&md5=27438cd2defe5a9ed81a37b725e6cddd},
	abstract = {For robot intelligence and human-robot interaction (HRI), complex decision-making, interpretation, and adaptive planning processes are great challenges. These require recursive task processing and meta-cognitive reasoning mechanism. Naturally, the human brain realizes these cognitive skills by prefrontal cortex which is a part of the neocortex. Previous studies about neurocognitive robotics would not meet these requirements. Thus, it is aimed at developing a brain-inspired robot control architecture that performs spatial-temporal and emotional reasoning. In this study, we present a novel solution that covers a computational model of the prefrontal cortex for humanoid robots. Computational mechanisms are mainly placed on the bio-physical plausible neural structures embodied in different dynamics. The main components of the system are composed of several computational modules including dorsolateral, ventrolateral, anterior, and medial prefrontal regions. Also, it is responsible for organizing the working memory. A reinforcement meta-learning based explainable artificial intelligence (xAI) procedure is applied to the working memory regions of the computational prefrontal cortex model. Experimental evaluation and verification tests are processed by the developed software framework embodied in the humanoid robot platform. The humanoid robots' perceptual states and cognitive processes including emotion, attention, and intention-based reasoning skills can be observed and controlled via the developed software. Several interaction scenarios are implemented to monitor and evaluate the model's performance. © 2013 IEEE.},
	author_keywords = {Artificial intelligence; brain modeling; cognitive robotics; human-robot interaction},
	keywords = {Anthropomorphic robots; Brain; Computation theory; Decision making; Intelligent robots; Reinforcement learning; Robot programming; Software testing; Verification; Computational model; Experimental evaluation; Human robot Interaction (HRI); Neural structures; Robot control architecture; Robot intelligences; Software frameworks; Verification tests; Human robot interaction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access, Gold Open Access}
}

@ARTICLE{Ghaemi2020,
	author = {Ghaemi, Amir and Arasteh, Bahman},
	title = {SFLA-based heuristic method to generate software structural test data},
	year = {2020},
	journal = {Journal of Software: Evolution and Process},
	volume = {32},
	number = {1},
	doi = {10.1002/smr.2228},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071277124&doi=10.1002%2fsmr.2228&partnerID=40&md5=442ef4301e0a7428ee0f0d39d67b314b},
	abstract = {Software testing is one of the significant stages in software development life cycle which is a costly and time-consuming task. Automatic tests data generation is one of the traditional techniques to reduce the cost and time spent in software testing. Different evolutionary algorithms have been proposed to generate test data which cover target paths in a software program. In this paper, shuffled frog leaping algorithm (SFLA) is proposed to generate structural test data. The proposed SFLA algorithm is characterized by high convergence speed and simple implementation. In the proposed SFLA, branch coverage is used as the fitness function to generate effective test data. For comparing the performance of the proposed SFLA with genetic algorithm (GA), particle swarm optimization (PSO), ant colony optimization (ACO), and artificial bee colony (ABC), seven benchmark programs were used. The results indicated that the proposed SFLA has an average of 99.99% for branch coverage, average 99.97% for success rate, and 2.03 for the average number of generation for covering all branches. © 2019 John Wiley & Sons, Ltd.},
	author_keywords = {automatic test-data generation; branch coverage; shuffled frog leaping algorithm; software testing},
	keywords = {Ant colony optimization; Artificial intelligence; Benchmarking; Genetic algorithms; Heuristic methods; Life cycle; Particle swarm optimization (PSO); Software design; Test facilities; Ant Colony Optimization (ACO); Artificial bee colonies (ABC); Automatic test data generation; Branch coverage; Shuffled frog leaping algorithm (SFLA); Software development life cycle; Time-consuming tasks; Traditional techniques; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access, Bronze Open Access}
}

@ARTICLE{Sowah2020,
	author = {Sowah, Robert A. and Bampoe-Addo, Adelaide A. and Armoo, Stephen K. and Saalia, Firibu K. and Gatsi, Francis and Sarkodie-Mensah, Baffour},
	title = {Design and Development of Diabetes Management System Using Machine Learning},
	year = {2020},
	journal = {International Journal of Telemedicine and Applications},
	volume = {2020},
	doi = {10.1155/2020/8870141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089017069&doi=10.1155%2f2020%2f8870141&partnerID=40&md5=ba97ec8e16af0b2f136ff89f1c79af4b},
	abstract = {This paper describes the design and implementation of a software system to improve the management of diabetes using a machine learning approach and to demonstrate and evaluate its effectiveness in controlling diabetes. The proposed approach for this management system handles the various factors that affect the health of people with diabetes by combining multiple artificial intelligence algorithms. The proposed framework factors the diabetes management problem into subgoals: building a Tensorflow neural network model for food classification; thus, it allows users to upload an image to determine if a meal is recommended for consumption; implementing K-Nearest Neighbour (KNN) algorithm to recommend meals; using cognitive sciences to build a diabetes question and answer chatbot; tracking user activity, user geolocation, and generating pdfs of logged blood sugar readings. The food recognition model was evaluated with cross-entropy metrics that support validation using Neural networks with a backpropagation algorithm. The model learned features of the images fed from local Ghanaian dishes with specific nutritional value and essence in managing diabetics and provided accurate image classification with given labels and corresponding accuracy. The model achieved specified goals by predicting with high accuracy, labels of new images. The food recognition and classification model achieved over 95% accuracy levels for specific calorie intakes. The performance of the meal recommender model and question and answer chatbot was tested with a designed cross-platform user-friendly interface using Cordova and Ionic Frameworks for software development for both mobile and web applications. The system recommended meals to meet the calorific needs of users successfully using KNN (with k=5) and answered questions asked in a human-like way. The implemented system would solve the problem of managing activity, dieting recommendations, and medication notification of diabetics.  © 2020 Robert A. Sowah et al.},
	keywords = {Application programs; Backpropagation; Nearest neighbor search; Neural networks; Nutrition; Software design; Software testing; User interfaces; Artificial intelligence algorithms; Classification models; Design and Development; Design and implementations; K nearest neighbours (k-NN); Machine learning approaches; Neural network model; User friendly interface; article; artificial intelligence; artificial neural network; back propagation; caloric intake; comparative effectiveness; diabetic patient; diagnostic test accuracy study; diet; entropy; glucose blood level; human; k nearest neighbor; nutritional value; psychology; software; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 61; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Ali2020,
	author = {Ali, Sikandar and Ullah, Niamat and Abrar, Muhammad Faisal and Yang, Zhongguo and Huang, Jiwei and Ali, Rahman},
	title = {Fuzzy Multicriteria Decision-Making Approach for Measuring the Possibility of Cloud Adoption for Software Testing},
	year = {2020},
	journal = {Scientific Programming},
	volume = {2020},
	doi = {10.1155/2020/6597316},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084971608&doi=10.1155%2f2020%2f6597316&partnerID=40&md5=329273113345db270907ca315b6bd403},
	abstract = {To reduce costs and improve organizational efficiency, the adoption of innovative services such as Cloud services is the current trend in today's highly competitive global business venture. The aim of the study is to guide the software development organization (SDO) for Cloud-based testing (CBT) adoption. To achieve the aim, this study first explores the determinants and predictors of Cloud adoption for software testing. Grounded on the collected data, this study designs a technology acceptance model using fuzzy multicriteria decision-making (FMCDM) approach. For the stated model development, this study identifies a list of predictors (main criteria) and factors (subcriteria) using systematic literature review (SLR). In the results of SLR, this study identifies seventy subcriteria also known as influential factors (IFs) from a sample of 136 papers. To provide a concise understanding of the facts, this study classifies the identified factors into ten predictors. To verify the SLR results and to rank the factors and predictors, an empirical survey was conducted with ninety-five experts from twenty different countries. The application value in the industrial field and academic achievement of the present study is the development of a general framework incorporating fuzzy set theory for improving MCDM models. The model can be applied to predict organizational Cloud adoption possibility taking various IFs and predictors as assessment criteria. The developed model can be divided into two main parts, ranking and rating. To measure the success or failure contribution of the individual IFs towards successful CBT adoption, the ranking part of the model will be used, while for a complete organizational assessment in order to identify the weak area for possible improvements, the assessment part of the model will be used. Collectively, it can be used as a decision support system to gauge SDO readiness towards successful CBT. © 2020 Sikandar Ali et al.},
	keywords = {Artificial intelligence; Decision making; Decision support systems; Fuzzy set theory; Software design; Academic achievements; Fuzzy multi criteria decision making (FMCDM); Fuzzy multi-criteria decision makings; Organizational assessment; Organizational efficiency; Software development organizations; Systematic literature review (SLR); Technology acceptance model; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access, Hybrid Gold Open Access}
}

@ARTICLE{Grigorescu20201,
	author = {Grigorescu, Sorin and Cocias, Tiberiu and Trasnea, Bogdan and Margheri, Andrea and Lombardi, Federico and Aniello, Leonardo},
	title = {Cloud2edge elastic ai framework for prototyping and deployment of Ai inference engines in autonomous vehicles},
	year = {2020},
	journal = {Sensors (Switzerland)},
	volume = {20},
	number = {19},
	pages = {1 – 21},
	doi = {10.3390/s20195450},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091439757&doi=10.3390%2fs20195450&partnerID=40&md5=b73b8d42ff965ce07d3fbc18f5f0ad26},
	abstract = {Self-driving cars and autonomous vehicles are revolutionizing the automotive sector, shaping the future of mobility altogether. Although the integration of novel technologies such as Artificial Intelligence (AI) and Cloud/Edge computing provides golden opportunities to improve autonomous driving applications, there is the need to modernize accordingly the whole prototyping and deployment cycle of AI components. This paper proposes a novel framework for developing so-called AI Inference Engines for autonomous driving applications based on deep learning modules, where training tasks are deployed elastically over both Cloud and Edge resources, with the purpose of reducing the required network bandwidth, as well as mitigating privacy issues. Based on our proposed data driven V-Model, we introduce a simple yet elegant solution for the AI components development cycle, where prototyping takes place in the cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment and evaluation on the target ECUs (Electronic Control Units) is performed as Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework is demonstrated using two real-world use-cases of AI inference engines for autonomous vehicles, that is environment perception and most probable path prediction. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
	author_keywords = {Artificial intelligence; Autonomous vehicles; Cloud computing; Deep learning; Edge computing; Self-driving cars},
	keywords = {Control systems; Deep learning; Engines; Software prototyping; Software testing; Automotive sector; Autonomous driving; Development cycle; Electronic control units; Environment perceptions; Hardware in the loops; Network bandwidth; Software in the loop(SIL); article; artificial intelligence; cloud computing; deep learning; perception; prediction; privacy; software; Autonomous vehicles},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access, Green Open Access}
}

@ARTICLE{Dubey2020,
	author = {Dubey, Rameshwar and Gunasekaran, Angappa and Childe, Stephen J. and Bryde, David J. and Giannakis, Mihalis and Foropon, Cyril and Roubaud, David and Hazen, Benjamin T.},
	title = {Big data analytics and artificial intelligence pathway to operational performance under the effects of entrepreneurial orientation and environmental dynamism: A study of manufacturing organisations},
	year = {2020},
	journal = {International Journal of Production Economics},
	volume = {226},
	doi = {10.1016/j.ijpe.2019.107599},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077155060&doi=10.1016%2fj.ijpe.2019.107599&partnerID=40&md5=5f3564e0c97fe3447db9606b3356e270},
	abstract = {The importance of big data analytics, artificial intelligence, and machine learning has been at the forefront of research for operations and supply chain management. Literature has reported the influence of big data analytics for improved operational performance, but there has been a paucity of research regarding the role of entrepreneurial orientation (EO) on the adoption of big data analytics. To address this gap, we draw on the dynamic capabilities view of firms and on contingency theory to develop and test a model that describes the role of EO on the adoption of big data analytics powered by artificial intelligence (BDA-AI) and operational performance (OP). We tested our research hypotheses using a survey of 256 responses gathered using a pre-tested questionnaire from manufacturing firms in India with the help of the National Association of Software and Services Companies (NASSCOM) and the Federation of Indian Chambers of Commerce and Industry (FICCI). The results from our analysis indicate that EO enables an organisation to exploit and further explore the BDA-AI capabilities to achieve superior OP. Further, our results provide empirical evidence based on data analysis that EO is strongly associated with higher order capabilities (such as BDA-AI) and OP under differential effects of environmental dynamism (ED). These findings extend the dynamic capability view and contingency theory to create better understanding of dynamic capabilities of the organisation while also providing theoretically grounded guidance to the managers to align their EO with their technological capabilities within their firms. © 2019},
	author_keywords = {Artificial intelligence; Big data analytics; Entrepreneurial orientation; Operational performance; PLS SEM; Supply chain management},
	keywords = {Advanced Analytics; Artificial intelligence; Big data; Enterprise resource management; Industrial research; Manufacture; Software testing; Supply chain management; Surveys; Differential effect; Dynamic capabilities; Entrepreneurial orientation; Environmental dynamisms; Manufacturing organisations; Operational performance; Operations and supply chain managements; Technological capability; Data Analytics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 544; All Open Access, Green Open Access}
}

@ARTICLE{Pirouz2020,
	author = {Pirouz, Behzad and Nejad, Hana Javadi and Violini, Galileo and Pirouz, Behrouz},
	title = {The role of artificial intelligence, MLR and statistical analysis in investigations about the correlation of swab tests and stress on health care systems by COVID-19},
	year = {2020},
	journal = {Information (Switzerland)},
	volume = {11},
	number = {9},
	doi = {10.3390/info11090454},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092319068&doi=10.3390%2finfo11090454&partnerID=40&md5=9f202d443f5481c2480a760e570c5c05},
	abstract = {The outbreak of the new Coronavirus (COVID-19) pandemic has prompted investigations on various aspects. This research aims to study the possible correlation between the numbers of swab tests and the trend of confirmed cases of infection, while paying particular attention to the sickness level. The study is carried out in relation to the Italian case, but the result is of more general importance, particularly for countries with limited ICU (intensive care units) availability. The statistical analysis showed that, by increasing the number of tests, the trend of home isolation cases was positive. However, the trend of mild cases admitted to hospitals, intensive case cases, and daily deaths were all negative. The result of the statistical analysis provided the basis for an AI study by ANN. In addition, the results were validated using a multivariate linear regression (MLR) approach. Our main result was to identify a significant statistical effect of a reduction of pressure on the health care system due to an increase in tests. The relevance of this result is not confined to the COVID-19 outbreak, because the high demand of hospitalizations and ICU treatments due to this pandemic has an indirect effect on the possibility of guaranteeing an adequate treatment for other high-fatality diseases, such as, e.g., cardiological and oncological ones. Our results show that swab testing may play a significant role in decreasing stress on the health system. Therefore, this case study is relevant, in particular, for plans to control the pandemic in countries with a limited capacity for admissions to ICU units. © 2020 by the authors.},
	author_keywords = {ANN; Artificial intelligence; COVID-19; Health systems; MLR; Statistical analysis; Swab},
	keywords = {Artificial intelligence; Diseases; Health care; Information dissemination; Intensive care units; Coronaviruses; Health systems; Health-care system; High demand; Indirect effects; Limited capacity; Multivariate linear regressions; Statistical effects; Statistical methods},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access, Gold Open Access}
}

@ARTICLE{Chen2020,
	author = {Chen, Lin-Kung and Chen, Yen-Hung and Chang, Shu-Fang and Chang, Shun-Chieh},
	title = {A long/short-term memory based automated testing model to quantitatively evaluate game design},
	year = {2020},
	journal = {Applied Sciences (Switzerland)},
	volume = {10},
	number = {19},
	doi = {10.3390/APP10196704},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092455243&doi=10.3390%2fAPP10196704&partnerID=40&md5=60d4fdbc7ae5530e6b570ab2245cc406},
	abstract = {The mobile casual game application lifespan is getting shorter. Acompany has to shorten the game testing procedure to avoid being squeezed out of the game market share. There is no sufficient testing indicator to objectively evaluate the operability of different game designs. Many automated testing methodologies are proposed, but they adopt rule-based approaches and cannot provide quantitative analysis to statistically evaluate gameplay experience. This study suggests applying "Learning Time" as a testing indicator and using the learning curve to identify the operability of different game designs. This study also proposes a Long/Short-Term Memory based automated testing model (called LSTM-Testing) to statistically testing game experience through end-to-end functionality (Input: game image; Output: game action) without any manual intervention. The experiment results demonstrate LSTM-Testing can provide quantitative testing data by using learning time as the control variable, game design as the independent variable, and time to complete game as the dependent variable. This study also demonstrates how LSTM-Testing evaluates the effectiveness of different gameplay learning strategies, e.g., reviewing the newest decisions, reviewing the correct decision, or reviewing the wrong decisions. The contributions of LSTM-Testing are (1) providing an objective and quantitative analytical game-testing framework, (2) reducing the labor cost of inefficient and subjective manual game testing, and (3) allowing game company boosts software development by focusing on game intellectual property and leaves game testing to artificial intelligence (AI). © 2020 by the authors.},
	author_keywords = {Artificial intelligence; Artificial neural networks; Quality management; Software measurement; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Sivamani2020173599,
	author = {Sivamani, Saraswathi and Chon, Sun Il and Choi, Do Yeon and Park, Ji Hwan},
	title = {Investigating and suggesting the evaluation dataset for image classification model},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {173599 – 173608},
	doi = {10.1109/ACCESS.2020.3024575},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102802945&doi=10.1109%2fACCESS.2020.3024575&partnerID=40&md5=fa4b19fef176505ca5be5ee75a1ad618},
	abstract = {Image processing systems are widespread with the digital transformation of artificial intelligence. Many researchers developed and tested several image classification models using machine learning and statistical techniques. Nevertheless, the current research seldom focuses on the quality assurance of these models. The existing methods lack to verify the quality assurance, with the lack of test cases to prepare the evaluation dataset to test the model, which can cause critical drawbacks in the nuclear field and defense system. In this article, we discuss and suggest the preparation of the evaluation dataset using improved test cases through Cause-Effect Graphing. The proposed method can generate the evaluation dataset with automated test cases through the quantification method, which consists of 1) image characteristic selection 2) creating the Cause-Effect graphing approach of the image with the feature, and 3) generate all possible test coverage. The testing is performed with the COCO dataset, which shows the declining prediction accuracy with the adjusted brightness and sharpness ranging between −75 to 75%, which indicates the negligence of the important characteristics in the existing test dataset. The experiment shows the prediction fails while sharpness is less than the 0%, and the brightness fails at −75% with less number of detection object between −50% and 75%. This indicates that characteristic changes affects the prediction accuracy and the number of detected objects in an image. Our approach proves the importance of the characteristic selection process for the overall image to generate a more efficient model and increase the accuracy of object detection. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author_keywords = {Cause-effect graphing; Image classification; Machine learning; Object detection; Quality assurance},
	keywords = {Artificial intelligence; Classification (of information); Forecasting; Luminance; Object detection; Quality assurance; Quality control; Statistical tests; Automated test; Classification models; Digital transformation; Image characteristics; Image processing system; Prediction accuracy; Quantification methods; Statistical techniques; Image classification},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access, Gold Open Access}
}

@ARTICLE{Wendell Bates202061806,
	author = {Wendell Bates, Ira and Karimoddini, Ali and Karimadini, Mohammad},
	title = {Learning a partially-known discrete event system},
	year = {2020},
	journal = {IEEE Access},
	volume = {8},
	pages = {61806 – 61816},
	doi = {10.1109/ACCESS.2020.2983074},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083460029&doi=10.1109%2fACCESS.2020.2983074&partnerID=40&md5=860534a4886d9725e7aa96179f4d0fff},
	abstract = {There are many cases in which our understanding of a system may be limited due to its complexity or lack of access into the entire system, leaving us with only partial system knowledge. This paper proposes a novel systematic active-learning method for realizing a partially-known Discrete Event System (DES). The proposed technique takes the available information about the system into account by tabularly capturing the known data from the system, and then, discovers the unknown part of the system via an active-learning procedure. For this purpose, a series of tables will be constructed to first infer the information about the system from the available data, and if unavailable, the developed algorithm collects the information through basic queries made to an oracle. It is proven that the developed technique returns a language-equivalent finite-state automaton model for the system under identification after a finite number of iterations. A real-world illustrative example is provided to explain the details of the proposed method. © 2013 IEEE.},
	author_keywords = {active-learning; automata theory; automotive industries; complex systems; Discrete event systems; manufacturing systems; partially-known systems; systems identification},
	keywords = {Artificial intelligence; Discrete event simulation; Query processing; Active Learning; Active learning methods; Entire system; Finite number; Partial systems; Real-world; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access, Gold Open Access}
}

@ARTICLE{Vázquez-Ingelmo20201803,
	author = {Vázquez-Ingelmo, Andrea and García-Peñalvo, Francisco José and Therón, Roberto and Amo Filvà, Daniel and Fonseca Escudero, David},
	title = {Connecting domain-specific features to source code: towards the automatization of dashboard generation},
	year = {2020},
	journal = {Cluster Computing},
	volume = {23},
	number = {3},
	pages = {1803 – 1816},
	doi = {10.1007/s10586-019-03012-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074801554&doi=10.1007%2fs10586-019-03012-1&partnerID=40&md5=7bd2bd0dfc7a5f5a227e6a45a0918da8},
	abstract = {Dashboards are useful tools for generating knowledge and support decision-making processes, but the extended use of technologies and the increasingly available data asks for user-friendly tools that allow any user profile to exploit their data. Building tailored dashboards for any potential user profile would involve several resources and long development times, taking into account that dashboards can be framed in very different contexts that should be studied during the design processes to provide practical tools. This situation leads to the necessity of searching for methodologies that could accelerate these processes. The software product line paradigm is one recurrent method that can decrease the time-to-market of products by reusing generic core assets that can be tuned or configured to meet specific requirements. However, although this paradigm can solve issues regarding development times, the configuration of the dashboard is still a complex challenge; users’ goals, datasets, and context must be thoroughly studied to obtain a dashboard that fulfills the users’ necessities and that fosters insight delivery. This paper outlines the benefits and a potential approach to automatically configuring information dashboards by leveraging domain commonalities and code templates. The main goal is to test the functionality of a workflow that can connect external algorithms, such as artificial intelligence algorithms, to infer dashboard features and feed a generator based on the software product line paradigm. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
	author_keywords = {Artificial intelligence; Automatic configuration; Domain engineering; Feature model; Information dashboards; Meta-model; SPL},
	keywords = {Artificial intelligence; Decision making; Automatic configuration; Domain engineering; Feature modeling; Information dashboards; Meta model; Software testing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access, Green Open Access}
}

@ARTICLE{Vejandla2020,
	author = {Vejandla, Kishore and Valluri, Sivaprasad and Vakamulla, V. Mani and Kumar, Abhinav},
	title = {A Tunable Energy Signal for Intensity Modulation and Direct Detection Systems: Theory, Simulations, and Experiments},
	year = {2020},
	journal = {IEEE Photonics Journal},
	volume = {12},
	number = {1},
	doi = {10.1109/JPHOT.2019.2958836},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078254092&doi=10.1109%2fJPHOT.2019.2958836&partnerID=40&md5=3393490de07fc333dbcfd47223b7a224},
	abstract = {In this paper, we propose a novel method to generate real signal for Visible Light Communication (VLC) systems without using traditional hermitian symmetry on the data symbols obtained using M-ary Pulse Amplitude Modulation (PAM) which is named as Auxiliary PAM (Ax-PAM). We mathematically analyse this method to generate a real signal with tunable energy using auxiliary symbols at the transmitter and the corresponding receiver. Simulation results for Bit Error Rate (BER) show better performance over conventional PAM Discrete Multi-Tone (PAM-DMT) and Asymmetrically Clipped Optical Orthogonal Frequency Division Multiplexing (ACO-OFDM) even under clipping distortion and also demonstrate the tunable energy for the proposed scheme. Furthermore, the proposed scheme is implemented on a VLC test bed designed using Universal Software defined Radio Peripheral (USRP). The experimental results for estimated Signal to Noise Ratio (SNR) and achieved BER for Ax-PAM outperforms PAM-DMT and ACO-OFDM. © 2020 IEEE.},
	author_keywords = {Asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM); bit error rate (BER); pulse amplitude modulated discrete multi-tone (PAM-DMT); visible light communication (VLC)},
	keywords = {Amplitude modulation; Artificial intelligence; Bit error rate; Light; Optical fiber communication; Orthogonal frequency division multiplexing; Signal receivers; Signal to noise ratio; Software radio; Software testing; Visible light communication; Clipping distortion; Discrete multi-tone; Hermitian symmetry; Intensity modulation and direct detections; Optical orthogonal frequency division multiplexing; Pulse amplitude modulations (PAM); Software-defined radios; Visible light communications (VLC); Pulse amplitude modulation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access, Gold Open Access}
}